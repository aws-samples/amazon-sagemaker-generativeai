{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ed967c-af24-48a1-b778-1a2e740f27a9",
   "metadata": {},
   "source": [
    "# Post-training Quantization (PTQ) using Amazon SageMaker AI ðŸš€\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2927f86-69b5-4d41-a6a3-fe07df8925c9",
   "metadata": {},
   "source": [
    "Quantization is a technique used to compress large language models by reducing the precision of their weights and activations, often from 16-bit or 32-bit floating-point numbers down to lower bit-width integers (like int8 or int4). This compression reduces model size, lowers memory bandwidth requirements, and speeds up inference on supported hardware â€” all while trying to maintain acceptable model accuracy.\n",
    "\n",
    "Post-Training Quantization (PTQ) applies quantization to a pretrained model without requiring any additional fine-tuning. Instead, it uses a small calibration dataset to estimate activation statistics and determine optimal quantization parameters. PTQ is especially useful when retraining is expensive or infeasible. In this notebook, weâ€™ll demonstrate how PTQ works and evaluate the impact on model size and inference performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc97cc32-859a-42b0-9dca-f2861d0472fd",
   "metadata": {},
   "source": [
    "## 01. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434b5687-cd68-42a2-b6e5-743604401416",
   "metadata": {},
   "source": [
    "Download the latest version of SageMaker Python SDK for up to date features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c1c69f-8ac1-4ebe-a819-687264d71634",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b33424-e142-497c-903a-c72259e9fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d4b7a-1dfe-4696-85b9-3e4d53128792",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2234e0-d21c-486a-ab3f-986c5072cf6b",
   "metadata": {},
   "source": [
    "## 02. Running Post-Training Quantization on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae52926d-3b5d-47ea-a1f1-6ab225d08918",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T20:27:06.681559Z",
     "iopub.status.busy": "2025-07-09T20:27:06.680727Z",
     "iopub.status.idle": "2025-07-09T20:27:06.686175Z",
     "shell.execute_reply": "2025-07-09T20:27:06.685700Z",
     "shell.execute_reply.started": "2025-07-09T20:27:06.681503Z"
    }
   },
   "source": [
    "---\n",
    "\n",
    "To quantize large language models at scale, we use Amazon SageMaker Training Jobs to execute a PTQ (Post-Training Quantization) script on a GPU-backed instance. While the name *Training Job* suggests model training, in this case, we are **not retraining or fine-tuning the model**. Instead, weâ€™re simply using the Training Job infrastructure to run our quantization workload efficiently on a high-performance GPU (e.g., `ml.g5.2xlarge`), leveraging parallelism and scalability built into the SageMaker platform.\n",
    "\n",
    "The script we run (`post_training_sagemaker_quantizer.py`) automates all steps of PTQ. It loads the model in full or half-precision, preprocesses a calibration dataset, and applies either GPTQ or AWQ quantization using the [`llm-compressor`](https://github.com/vllm-project/llm-compressor) library. This is a one-shot quantization process that computes activation statistics from a small number of input sequences and generates a compressed version of the model, reducing memory and compute footprint without needing any labeled data or training.\n",
    "\n",
    "Once the Training Job completes, the quantized model is automatically saved to Amazon S3. From there, it can be untarred and deployed behind a fully managed SageMaker Endpoint using a prebuilt inference container (like `lmi-dist` with `vLLM`). The following code snippet shows how to launch the Training Job, pass in hyperparameters like quantization scheme and number of calibration samples, and prepare your model for efficient, low-latency inference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51eb81b-2acf-44a5-ae84-0e5dfef7f6d9",
   "metadata": {},
   "source": [
    "Post training Quantization takes the folllowing arguments,\n",
    "\n",
    "```bash\n",
    "usage: post_training_sagemaker_quantizer.py [-h] --model-id MODEL_ID [--sequential-loading SEQUENTIAL_LOADING] --dataset-id DATASET_ID\n",
    "                                            [--dataset-split DATASET_SPLIT] [--dataset-seed DATASET_SEED] [--num-calibration-samples NUM_CALIBRATION_SAMPLES]\n",
    "                                            [--max-sequence-length MAX_SEQUENCE_LENGTH] [--vision-enabled] [--transformer-model-name TRANSFORMER_MODEL_NAME]\n",
    "                                            [--vision-sequential-targets VISION_SEQUENTIAL_TARGETS] [--algorithm {awq,gptq}] [--ignore-layers IGNORE_LAYERS]\n",
    "                                            [--include-targets INCLUDE_TARGETS] [--awq-quantization-scheme {W4A16_ASYM,W4A16}]\n",
    "                                            [--gptq-quantization-scheme {W4A16,W4A16_ASYM,W8A8,W8A16}] [--sm-model-dir SM_MODEL_DIR]\n",
    "\n",
    "Quantize a language model using AWQ\n",
    "\n",
    "options:\n",
    "  -h, --help            show this help message and exit\n",
    "  --model-id MODEL_ID   Hugging Face model ID\n",
    "  --sequential-loading SEQUENTIAL_LOADING\n",
    "                        If the quantization model size GPU set this param to true to run sequential loading to optimize on a single GPU\n",
    "  --dataset-id DATASET_ID\n",
    "                        Hugging Face dataset ID\n",
    "  --dataset-split DATASET_SPLIT\n",
    "                        Dataset split to use for calibration\n",
    "  --dataset-seed DATASET_SEED\n",
    "                        Deterministic dataset seed\n",
    "  --num-calibration-samples NUM_CALIBRATION_SAMPLES\n",
    "                        Number of samples for calibration, larger value <> better quantized model\n",
    "  --max-sequence-length MAX_SEQUENCE_LENGTH\n",
    "                        Maximum sequence length for tokenization\n",
    "  --vision-enabled      Weather to use images during quanitzation with vision models\n",
    "  --transformer-model-name TRANSFORMER_MODEL_NAME\n",
    "                        Need a dynamic transformer import mechanism for varying types\n",
    "  --vision-sequential-targets VISION_SEQUENTIAL_TARGETS\n",
    "                        Vision model sequential targets\n",
    "  --algorithm {awq,gptq}\n",
    "                        Quantization Algorithm to use\n",
    "  --ignore-layers IGNORE_LAYERS\n",
    "                        Ignore layers to quantize, comma separated\n",
    "  --include-targets INCLUDE_TARGETS\n",
    "                        Targets to quantize including, comma separated\n",
    "  --awq-quantization-scheme {W4A16_ASYM,W4A16}\n",
    "                        AWQ Param: Quantization scheme to use\n",
    "  --gptq-quantization-scheme {W4A16,W4A16_ASYM,W8A8,W8A16}\n",
    "                        GPTQ Param: Quantization scheme to use\n",
    "  --sm-model-dir SM_MODEL_DIR\n",
    "                        Directory to save quantized model\n",
    "```\n",
    "\n",
    "Use `--gptq-*` params to set runtime quantization GPTQ params and `--awq-*` to set runtime quantization AWQ params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81b3dd5-bc7b-4127-9a88-bd4b0dc72c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters which are passed to the training job - Example with AWQ\n",
    "hyperparameters = {\n",
    "    'model-id': 'meta-llama/Llama-3.1-8B-Instruct',\n",
    "    'dataset-id': 'HuggingFaceH4/ultrachat_200k',\n",
    "    'dataset-split': 'train_sft',\n",
    "    'dataset-seed': 42,\n",
    "    'algorithm': 'gptq',\n",
    "    'max-sequence-length': 2048,\n",
    "    'num-calibration-samples': 256,\n",
    "    'ignore-layers': 'lm_head',\n",
    "    'include-targets': 'Linear',\n",
    "    'gptq-quantization-scheme': 'W8A16',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6cc789-dbc9-43d5-986d-6f45a6be6aa3",
   "metadata": {},
   "source": [
    "If you're attempting to quantize a gated model like [meta-Llama](https://huggingface.co/meta-llama) model series, please provide `HF_TOKEN` in the environments to ensure the session is capable of pulling model weights from HF_Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32c0eb6-328f-4c71-a930-8c12337f8a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_estimator = PyTorch(\n",
    "    entry_point='post_training_sagemaker_quantizer.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type='ml.g6e.2xlarge', # Change the instance size based on Quota or choice\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    framework_version='2.4.0',\n",
    "    py_version='py311',\n",
    "    hyperparameters=hyperparameters,\n",
    "    environment={\"HF_TOKEN\": \"\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1ab10d-f3cd-4aa5-a19e-2e7aa7a7e039",
   "metadata": {},
   "source": [
    "ðŸš€ Go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979f25da-0717-4a7f-a75d-7007db43263f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quantization_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33492b19-f244-4141-9421-aabf9ffebc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Quantized model available under: {quantization_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d900648-e073-4412-9545-ae8b08c5eb80",
   "metadata": {},
   "source": [
    "## Download Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bae9f4-f0ea-48a8-bc9c-bfa2418e4fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "from sagemaker.s3 import S3Downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b4e11c-2178-44a7-a080-deb5db405bef",
   "metadata": {},
   "source": [
    "This is where the model.tar.gz will be pulled and saved locally\n",
    "\n",
    "âš ï¸ NOTE: if you're using a large model, ensure you have sufficient EBS storage size using `df -h`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c586a5-433f-44a8-b5b1-60f30ef61f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_download_basepath = os.path.join(os.getcwd(), \"quantized-model-tj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1636156e-4b86-4198-843b-a992eed33a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_model_from_s3(s3_uri: str, local_tar_path: str, extract_dir: str):\n",
    "    \"\"\"\n",
    "    Downloads a .tar.gz file from S3 using SageMaker's Downloader and extracts it.\n",
    "\n",
    "    Parameters:\n",
    "        s3_uri (str): Full S3 URI to the .tar.gz file (e.g., 's3://my-bucket/path/model.tar.gz').\n",
    "        local_tar_path (str): Local path where the .tar.gz will be saved.\n",
    "        extract_dir (str): Local directory to extract the tar.gz file to.\n",
    "    \"\"\"\n",
    "\n",
    "    file_name = os.path.basename(s3_uri)\n",
    "    \n",
    "    # Create extract directory if it doesn't exist\n",
    "    os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "    # Download from S3 using SageMaker Downloader\n",
    "    print(f\"Downloading {s3_uri} to {local_tar_path}\")\n",
    "    S3Downloader.download(s3_uri, local_tar_path)\n",
    "\n",
    "    # Extract tar.gz archive\n",
    "    tarball_path = os.path.join(local_tar_path, file_name)\n",
    "    print(f\"Extracting {tarball_path} to {extract_dir}\")\n",
    "    with tarfile.open(tarball_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=extract_dir)\n",
    "\n",
    "    print(\"Download and extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8506f04c-fcc6-49ae-a9d8-086cd104f9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_extract_model_from_s3(\n",
    "    s3_uri=quantization_estimator.model_data,\n",
    "    local_tar_path=model_download_basepath,\n",
    "    extract_dir=model_download_basepath\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26c217b-c25f-44e1-979e-555826230c2a",
   "metadata": {},
   "source": [
    "## Upload Quantized Model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3fed46-ca74-426f-b8ea-4005588fa72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4721d74-64c8-4837-b80b-3816b5003e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_quant_model_path = os.path.join(\n",
    "    model_download_basepath, \n",
    "    [model_path for model_path in os.listdir(model_download_basepath) if 'AWQ' in model_path or 'GPTQ' in model_path][0]\n",
    ")\n",
    "assert os.path.exists(local_quant_model_path), f\"model path does not exists: {local_quant_model_path}\"\n",
    "print(f\"reference local model path: {local_quant_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2385e12-b0bf-41ce-8713-34641acef0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_prefix = '-'.join(os.path.basename(local_quant_model_path).split('-')[4:]).replace('_', '-')\n",
    "print(f\"leveraging quant prefix: {quant_prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc63873e-c05d-40e9-81f8-1e60977d4771",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_upload_s3uri = os.path.join(\n",
    "    os.path.dirname(quantization_estimator.model_data), \n",
    "    os.path.basename(local_quant_model_path)\n",
    ")\n",
    "print(f\"s3 target dir to upload quantized model > {remote_upload_s3uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5b3c2f-22b5-45cf-a4e0-7f44a0dcb0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"uploading model from: {local_quant_model_path} to remote: {remote_upload_s3uri}\")\n",
    "S3Uploader.upload(\n",
    "    local_path=local_quant_model_path, \n",
    "    desired_s3_uri=remote_upload_s3uri\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc4e0e-ed5f-45ea-a10b-b291c42f9fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {remote_upload_s3uri}/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a7e070-01ae-410c-9c92-1ac3be4428f0",
   "metadata": {},
   "source": [
    "## Deploy Quantized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61925f99-e60b-4756-a2fd-bccec67ce961",
   "metadata": {},
   "source": [
    "---\n",
    "The lmi-dist (Large Model Inference - Distributed) container in Amazon SageMaker Hosting is purpose-built to serve large or optimized models efficiently using features like model partitioning, tensor parallelism, and inference optimization. It allows seamless deployment of models stored in Amazon S3 by specifying the S3 path as `HF_MODEL_ID` parameter during endpoint creation. This container is ideal for serving quantized modelsâ€”such as those compressed using GPTQ or AWQâ€”and supports efficient multi-GPU inference. To deploy your quantized model, simply upload the model artifacts (e.g., model.pt or model.safetensors) to an S3 bucket, then create a SageMaker model using the lmi-dist container and point to your S3 path. The container automatically loads the model, handles parallel execution, and exposes a performant inference endpoint ready for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da089c9f-f915-4d5a-a73e-f493ff257930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b9b44b-a206-4485-ac1d-61fc6898ba2a",
   "metadata": {},
   "source": [
    "All available images can be found here: https://github.com/aws/deep-learning-containers/blob/master/available_images.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191baf19-65dd-4c02-a66e-88b333849419",
   "metadata": {},
   "outputs": [],
   "source": [
    "prebaked_inference_image_uri = f\"763104351884.dkr.ecr.{sagemaker.Session().boto_session.region_name}.amazonaws.com/djl-inference:0.33.0-lmi15.0.0-cu128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cd9de3-10bb-4bbd-be74-666ab1b8ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"quantized-model-{quant_prefix}-{datetime.now().strftime('%y%m%d-%H%M%S')}\"\n",
    "endpoint_name = f\"{model_name}-ep\"\n",
    "print(f\"choosing model name > {model_name}\")\n",
    "print(f\"choosing endpoint name > {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa3738d-6084-4f74-aa84-c8542c7e2f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_model = sagemaker.Model(\n",
    "    image_uri=prebaked_inference_image_uri,\n",
    "    env={\n",
    "        \"HF_MODEL_ID\": f\"{remote_upload_s3uri}/\",\n",
    "        \"OPTION_MAX_MODEL_LEN\": \"12000\",\n",
    "        \"OPTION_GPU_MEMORY_UTILIZATION\": \"0.95\",\n",
    "        \"OPTION_ENABLE_STREAMING\": \"false\",\n",
    "        \"OPTION_ROLLING_BATCH\": \"auto\",\n",
    "        \"OPTION_MODEL_LOADING_TIMEOUT\": \"3600\",\n",
    "        \"OPTION_PAGED_ATTENTION\": \"false\",\n",
    "        \"OPTION_DTYPE\": \"fp16\",\n",
    "    },\n",
    "    role=role,\n",
    "    name=model_name,\n",
    "    sagemaker_session=sagemaker.Session()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e6188-eec9-4e10-86fc-7251ce7af60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_predictor = quant_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    container_startup_health_check_timeout=600,\n",
    "    wait=False\n",
    ")\n",
    "print(f\"Your Endpoint: {endpoint_name} is now deployed! ðŸš€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb286a7-9f3b-423a-b220-c7b0add2f9f9",
   "metadata": {},
   "source": [
    "## Inference with LiteLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc4867a-32ed-47d7-97ff-0b033647938f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -Uq litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a4ec64-957e-4376-a5fb-01e1e7a4676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from litellm import completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eea1cca-5fe5-4385-8b3f-63fccb584fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = completion(\n",
    "    model=f\"sagemaker/{endpoint_name}\", \n",
    "    messages=[\n",
    "        { \"content\": \"Hello\", \"role\": \"user\"}, \n",
    "        { \"content\": \"You are a helpful assistant that follows instructions\", \"role\": \"system\"}\n",
    "    ],\n",
    "    temperature=0.1,\n",
    "    max_tokens=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21405326-f1c6-470b-a9be-2181c90ef83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize_gpt_style(text):\n",
    "    return text.replace(\"ÄŠ\", \"\\n\").replace(\"Ä \", \" \")\n",
    "    \n",
    "print(detokenize_gpt_style(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafc1d07-8bcd-4e3e-8871-594b456829be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
