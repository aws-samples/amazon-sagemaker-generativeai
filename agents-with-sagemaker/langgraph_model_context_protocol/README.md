# ğŸ¦ Loan Underwriter: Agentic LLM Pipeline with MCP + LangGraph + SageMaker

This project implements an LLM-powered loan underwriting pipeline using:

- ğŸ¤– **LangGraph** for multi-step orchestration
- ğŸ›  **MCP servers** (Model Context Protocol) as LLM-powered tools
- ğŸ§  **Agent prompt modules** for clean, reusable prompt engineering
- ğŸ“¡ **Amazon SageMaker** for secure, scalable LLM hosting
- ğŸ” **LangSmith** for deep observability & tracing

---

## ğŸ’¡ Overview

This is a modular, agentic system that processes a loan application through 3 roles:

1. **Loan Officer** â Summarizes the application
2. **Credit Analyst** â Evaluates creditworthiness
3. **Risk Manager** â Makes a final approval/denial decision

Each role is:
- Represented by an **agent** (in `agents/`)
- Deployed via a dedicated **MCP server** (in `servers/`)
- Orchestrated with **LangGraph** (in `langgraph_flow/`)

---

## ğŸ§± Project Structure

```
Agents-FSI-MCP/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ loan_officer.py
â”‚   â”œâ”€â”€ credit_analyst.py
â”‚   â””â”€â”€ risk_manager.py
â”‚
â”œâ”€â”€ servers/
â”‚   â”œâ”€â”€ loan_parser/
â”‚   â”œâ”€â”€ credit_analyzer/
â”‚   â””â”€â”€ risk_assessor/
â”‚
â”œâ”€â”€ common/
â”‚   â””â”€â”€ sagemaker_client.py
â”‚
â”œâ”€â”€ langgraph_flow/
â”‚   â””â”€â”€ graph.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_pipeline.py
â”‚
â”œâ”€â”€ .env
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### 1. Install dependencies
```bash
pip install -r requirements.txt
pip install -U langchain-aws
```
Note: In case you get module not found error with Langchain
Run pip install -U langchain-aws

### 2. Deploy LLM Endpoint
python3 deploy_sm_endpoint.py

Copy the output of the endpoint name craeted after this job is successful for the next step to paste it in your.env file.

### 3. Set up `.env`file in the same root folder. Copy the code below.
Note: Langchain API key to access LangSmith is optional.

```env
AWS_REGION=YOUR REGION
SAGEMAKER_ENDPOINT=your-endpoint-name (created in Step2)
AWS_ACCESS_KEY_ID=your-key
AWS_SECRET_ACCESS_KEY=your-secret
LANGCHAIN_API_KEY=your-langsmith-key(optional)
LANGCHAIN_PROJECT=LoanUnderwriterFlow(optional)
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com(optional)
LANGCHAIN_TRACING_V2=true(optional)

LOAN_PARSER_URL=http://127.0.0.1:8002/process
CREDIT_ANALYZER_URL=http://127.0.0.1:8003/process
RISK_ASSESSOR_URL=http://127.0.0.1:8004/process
```

### 3. Start MCP servers
In case you are running this code in Amazon SageMaker JupyterLab. You would need to run the commands individually in three different terminals to start the servers.
Go to JupyterLab -> Click New Terminal -> Copy and paste uvicorn servers.loan_parser.main:app --port 8002
Go to JupyterLab -> Click New Terminal -> Copy and paste uvicorn servers.credit_analyzer.main:app --port 8003
Go to JupyterLab -> Click New Terminal -> Copy and paste uvicorn servers.risk_assessor.main:app --port 8004

Or you can use tmux( terminal multiplexer) to run all three servers if you have tmux installed
```bash
uvicorn servers.loan_parser.main:app --port 8002
uvicorn servers.credit_analyzer.main:app --port 8003
uvicorn servers.risk_assessor.main:app --port 8004
```

### 4. Run the pipeline
From JupyterLab-> Click new terminal-> copy and paste python scripts/run_pipeline.py
Note: make sure all the terminals are running.
```bash
python scripts/run_pipeline.py
```

Check results in:
- ğŸ§  Terminal (final output)
- ğŸ” [LangSmith](https://smith.langchain.com) for full trace (Optional)

---

## âœ¨ Key Concepts

### âœ… MCP Servers
Each server exposes a single `/process` endpoint that:
- Receives structured input
- Uses an agent to generate a role-specific prompt
- Sends the prompt to the SageMaker-hosted LLM
- Returns structured output

### âœ… Agents
Each agent is a prompt builder that encapsulates role-specific logic. They're imported inside the MCP server `utils.py` files.

### âœ… LangGraph
Handles orchestration across:
- `LoanParser` â `CreditAnalyzer` â `RiskAssessor`
- Passes structured state between nodes
- Tagged with `agent:*` + `mcp:*` in LangSmith

### âœ… LangSmith
Observability tool that traces each LLM call. Every prompt, response, and tag is recorded for debugging and analysis.

---

## ğŸ“¦ Possible Next Steps
- Docker Compose setup for all MCP servers
- Streamlit UI to submit applications
- Fine-tuned LLMs for better decision accuracy

---
