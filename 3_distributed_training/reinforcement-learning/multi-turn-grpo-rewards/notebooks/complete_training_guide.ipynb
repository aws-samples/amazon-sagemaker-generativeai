{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Turn RL Agent Training on SageMaker - Complete Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create new environment with all dependencies\n",
    "# conda create -n notebook-env python=3.11 -y\n",
    "# conda activate notebook-env\n",
    "\n",
    "# # Install exact versions from pyproject.toml\n",
    "# pip install torch==2.5.1\n",
    "# pip install transformers==4.49.0\n",
    "# pip install datasets==4.5.0\n",
    "# pip install accelerate deepspeed peft wandb rich liger-kernel vllm pyserini\n",
    "\n",
    "# # Install TRL from specific commit (as in pyproject.toml)\n",
    "# pip install git+https://github.com/huggingface/trl.git@fc4dae256d924dfbb906af9c2e817bc6fb7b590b\n",
    "\n",
    "\n",
    "## AWS/SageMaker\n",
    "# pip install boto3 sagemaker\n",
    "\n",
    "# # Install Jupyter\n",
    "# pip install jupyter ipykernel ipywidgets\n",
    "\n",
    "# # Register kernel\n",
    "# python -m ipykernel install --user --name=notebook-env --display-name=\"Python (notebook-env)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "print(\"Loading TriviaQA dataset...\")\n",
    "dataset = load_dataset(\"mandarjoshi/trivia_qa\", \"rc\", split=\"train\")\n",
    "dataset = dataset.select(range(1000))\n",
    "\n",
    "print(f\"\\nDataset size: {len(dataset)} examples\")\n",
    "print(f\"\\nDataset features: {list(dataset.features.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[0]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE QUESTION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Question: {sample['question']}\")\n",
    "print(f\"\\nAnswer: {sample['answer']['value']}\")\n",
    "print(f\"\\nAnswer Aliases: {sample['answer']['aliases'][:5]}...\")\n",
    "print(f\"\\nNormalized Aliases: {sample['answer']['normalized_aliases'][:5]}...\")\n",
    "print(f\"\\nTotal aliases: {len(sample['answer']['aliases'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MORE EXAMPLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(5):\n",
    "    ex = dataset[i]\n",
    "    print(f\"\\n{i+1}. Q: {ex['question']}\")\n",
    "    print(f\"   A: {ex['answer']['value']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding Multi-Turn Reasoning and Rewards\n",
    "\n",
    "### Important: Multi-Turn REASONING, Not Conversation\n",
    "\n",
    "This is **Multi-Turn Reasoning RL**, not multi-turn conversation:\n",
    "- The dataset only has: Question + Answer (no conversations)\n",
    "- The agent **learns** to break down answering into multiple reasoning steps\n",
    "- Each \"turn\" is an internal reasoning step, not a user-agent exchange\n",
    "\n",
    "**Example of agent's internal reasoning:**\n",
    "- **Turn 1**: \"I should search for this\" → `<search>capital of France</search>` → gets results\n",
    "- **Turn 2**: \"I found the answer\" → `<answer>Paris</answer>`\n",
    "\n",
    "The agent could also do:\n",
    "- **Turn 1**: Search broadly\n",
    "- **Turn 2**: Search more specifically based on Turn 1 results\n",
    "- **Turn 3**: Verify with another search\n",
    "- **Turn 4**: Provide final answer\n",
    "\n",
    "### What is a \"Turn\"?\n",
    "\n",
    "A **turn** is one reasoning step by the agent:\n",
    "- Agent decides on an action (search, calculate, answer)\n",
    "- Executes the action (if it's a tool)\n",
    "- Receives feedback\n",
    "- Gets rewarded for that step\n",
    "\n",
    "The agent **learns through RL** to:\n",
    "- Use tools effectively across multiple steps\n",
    "- Gather information incrementally\n",
    "- Build up to correct answers through reasoning\n",
    "\n",
    "**This is NOT given in the dataset** - the agent discovers optimal multi-turn strategies through training!\n",
    "\n",
    "### Two Types of Rewards:\n",
    "\n",
    "#### 1. Turn-Level Rewards (Intermediate)\n",
    "Rewards given **during each turn** when tools are used:\n",
    "- **Tool Execution Reward**: Did the tool call succeed? (+0.2 per successful tool, 0 if no tools)\n",
    "- **Answer Presence in Search**: Does the search result contain the answer? (+0.5 yes, 0 no)\n",
    "\n",
    "**Note**: Turns without tool use (like final answer) get 0 turn-level reward.\n",
    "\n",
    "#### 2. Outcome Rewards (Final)\n",
    "Rewards given **at the end** of all turns:\n",
    "- **Exact Match**: Does the final answer exactly match the ground truth? (+1.0)\n",
    "- **Answer Presence**: Is the answer present in the final response? (+0.5)\n",
    "- **Format Rewards**: Is the XML format correct? (varies by parser)\n",
    "\n",
    "### Total Reward Calculation:\n",
    "```\n",
    "Total Reward = Σ(Turn-Level Rewards) + Σ(Outcome Rewards)\n",
    "```\n",
    "\n",
    "**Example**: \n",
    "- Turn 1 (search): +0.7 (tool success 0.2 + answer found 0.5)\n",
    "- Turn 2 (answer): +0 (no tool used)\n",
    "- Outcome: +1.5+ (exact match 1.0 + presence 0.5 + format)\n",
    "- **Total: ~2.2+**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Reward Calculation Examples (Using Real Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTUAL REWARD FUNCTIONS from verifiers/rubrics/triviaqa_rubric.py\n",
    "# These are the EXACT functions used during training!\n",
    "\n",
    "def tool_execution_reward(trajectory):\n",
    "    \"\"\"Check if tools were executed successfully. Returns +0.2 per successful tool.\"\"\"\n",
    "    tool_attempts = 0\n",
    "    successful_executions = 0\n",
    "    \n",
    "    for i, turn in enumerate(trajectory):\n",
    "        if 'tool' in turn and turn['tool'] is not None:\n",
    "            tool_attempts += 1\n",
    "            if 'tool_result' in turn and not turn['tool_result'].startswith('Error:'):\n",
    "                successful_executions += 1\n",
    "    \n",
    "    if tool_attempts == 0:\n",
    "        return 0.0\n",
    "    return 0.2 * (successful_executions / tool_attempts)\n",
    "\n",
    "def exist_answer_in_search_results(search_result, answer_aliases):\n",
    "    \"\"\"Check if answer exists in search results. Returns +0.5 if found.\"\"\"\n",
    "    if search_result is None:\n",
    "        return 0.0\n",
    "    \n",
    "    search_lower = search_result.lower()\n",
    "    for alias in answer_aliases:\n",
    "        if str(alias).lower() in search_lower:\n",
    "            return 0.5\n",
    "    return 0.0\n",
    "\n",
    "def exist_answer_reward(final_answer, answer_aliases):\n",
    "    \"\"\"Check if answer exists in final response. Returns +0.5 if found.\"\"\"\n",
    "    if final_answer is None:\n",
    "        return 0.0\n",
    "    \n",
    "    answer_lower = str(final_answer).lower()\n",
    "    for alias in answer_aliases:\n",
    "        if str(alias).lower() in answer_lower:\n",
    "            return 0.5\n",
    "    return 0.0\n",
    "\n",
    "def exact_match_reward(final_answer, answer_aliases):\n",
    "    \"\"\"Check if answer exactly matches. Returns +1.0 if exact match.\"\"\"\n",
    "    if final_answer is None:\n",
    "        return 0.0\n",
    "    \n",
    "    answer_lower = str(final_answer).lower().strip()\n",
    "    for alias in answer_aliases:\n",
    "        if str(alias).lower().strip() == answer_lower:\n",
    "            return 1.0\n",
    "    return 0.0\n",
    "\n",
    "def format_reward(action):\n",
    "    \"\"\"Check if XML format is correct. Returns +0.1 for proper tags.\"\"\"\n",
    "    if '<answer>' in action and '</answer>' in action:\n",
    "        return 0.1\n",
    "    elif '<search>' in action and '</search>' in action:\n",
    "        return 0.1\n",
    "    return 0.0\n",
    "\n",
    "print(\"✓ Loaded ACTUAL reward functions from training code:\")\n",
    "print(\"  - tool_execution_reward: +0.2 per successful tool\")\n",
    "print(\"  - exist_answer_in_search_results: +0.5 if answer in search\")\n",
    "print(\"  - exist_answer_reward: +0.5 if answer in final response\")\n",
    "print(\"  - exact_match_reward: +1.0 if exact match\")\n",
    "print(\"  - format_reward: +0.1 for proper XML tags\")\n",
    "print(\"\\nThese are simplified versions of the actual functions for demonstration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a real example from the dataset\n",
    "example = dataset[0]\n",
    "question = example['question']\n",
    "answer_value = example['answer']['value']\n",
    "answer_aliases = example['answer']['normalized_aliases']\n",
    "\n",
    "print(\"Using real dataset example:\")\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer_value}\")\n",
    "print(f\"Aliases: {answer_aliases[:3]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a GOOD agent conversation for this question\n",
    "conversation = [\n",
    "    {\n",
    "        \"turn\": 1,\n",
    "        \"action\": f\"<search>{question}</search>\",\n",
    "        \"tool_result\": f\"According to Wikipedia, the answer is {answer_value}. {answer_value} is widely known...\",\n",
    "        \"tool_success\": True,\n",
    "    },\n",
    "    {\n",
    "        \"turn\": 2,\n",
    "        \"action\": f\"<answer>{answer_value}</answer>\",\n",
    "        \"final_answer\": answer_value,\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GOOD EXAMPLE CONVERSATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Ground Truth: {answer_value}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TURN-LEVEL REWARDS (Using Actual Functions)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "turn_rewards = []\n",
    "\n",
    "for turn_data in conversation:\n",
    "    turn_num = turn_data['turn']\n",
    "    print(f\"\\nTurn {turn_num}:\")\n",
    "    print(f\"  Action: {turn_data['action'][:80]}...\" if len(turn_data['action']) > 80 else f\"  Action: {turn_data['action']}\")\n",
    "    \n",
    "    rewards_this_turn = 0.0\n",
    "    \n",
    "    # Tool execution reward\n",
    "    if 'tool_success' in turn_data:\n",
    "        tool_reward = 0.2 if turn_data['tool_success'] else 0.0\n",
    "        rewards_this_turn += tool_reward\n",
    "        print(f\"  ✓ Tool Execution Reward: {tool_reward}\")\n",
    "    \n",
    "    # Answer in search results reward\n",
    "    if 'tool_result' in turn_data:\n",
    "        search_reward = exist_answer_in_search_results(turn_data['tool_result'], answer_aliases)\n",
    "        rewards_this_turn += search_reward\n",
    "        print(f\"  ✓ Answer in Search Results: {search_reward}\")\n",
    "    \n",
    "    # Format reward\n",
    "    fmt_reward = format_reward(turn_data['action'])\n",
    "    rewards_this_turn += fmt_reward\n",
    "    if fmt_reward > 0:\n",
    "        print(f\"  ✓ Format Reward: {fmt_reward}\")\n",
    "    \n",
    "    print(f\"  → Turn Total: {rewards_this_turn}\")\n",
    "    turn_rewards.append(rewards_this_turn)\n",
    "\n",
    "total_turn_rewards = sum(turn_rewards)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TOTAL TURN-LEVEL REWARDS: {total_turn_rewards}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OUTCOME REWARDS (Using Actual Functions)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "final_answer = conversation[-1].get('final_answer', '')\n",
    "print(f\"\\nFinal Answer: {final_answer}\")\n",
    "print(f\"Ground Truth: {answer_value}\")\n",
    "\n",
    "# Use actual reward functions\n",
    "exact_match = exact_match_reward(final_answer, answer_aliases)\n",
    "print(f\"\\n✓ Exact Match Reward: {exact_match}\")\n",
    "\n",
    "answer_present = exist_answer_reward(final_answer, answer_aliases)\n",
    "print(f\"✓ Answer Presence Reward: {answer_present}\")\n",
    "\n",
    "fmt_reward = format_reward(conversation[-1]['action'])\n",
    "print(f\"✓ Format Reward: {fmt_reward}\")\n",
    "\n",
    "total_outcome_rewards = exact_match + answer_present + fmt_reward\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TOTAL OUTCOME REWARDS: {total_outcome_rewards}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL REWARD CALCULATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTurn-Level Rewards:  {total_turn_rewards}\")\n",
    "print(f\"Outcome Rewards:     {total_outcome_rewards}\")\n",
    "print(f\"{'─'*40}\")\n",
    "total_reward = total_turn_rewards + total_outcome_rewards\n",
    "print(f\"TOTAL REWARD:        {total_reward}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"REWARD BREAKDOWN BY TURN\")\n",
    "print(\"=\" * 80)\n",
    "for i, reward in enumerate(turn_rewards, 1):\n",
    "    print(f\"Turn {i}: {reward}\")\n",
    "print(f\"Final (Outcome): {total_outcome_rewards}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Bad Example (Low Rewards) - Same Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a BAD agent conversation for the same question\n",
    "bad_conversation = [\n",
    "    {\n",
    "        \"turn\": 1,\n",
    "        \"action\": \"<search>random unrelated query</search>\",\n",
    "        \"tool_result\": \"Some completely irrelevant information that doesn't contain the answer...\",\n",
    "        \"tool_success\": True,\n",
    "    },\n",
    "    {\n",
    "        \"turn\": 2,\n",
    "        \"action\": \"Wrong Answer\",  # No XML tags, wrong answer\n",
    "        \"final_answer\": \"Wrong Answer\",\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BAD EXAMPLE - LOW REWARDS (Same Question)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Ground Truth: {answer_value}\")\n",
    "\n",
    "bad_turn_rewards = []\n",
    "\n",
    "print(\"\\nTurn 1:\")\n",
    "print(f\"  Action: {bad_conversation[0]['action']}\")\n",
    "tool_reward = 1.0  # Tool executed successfully\n",
    "search_reward = 0.0  # But answer not in results\n",
    "print(f\"  Tool Execution: {tool_reward}\")\n",
    "print(f\"  Answer in Search: {search_reward}\")\n",
    "bad_turn_rewards.append(tool_reward + search_reward)\n",
    "\n",
    "print(\"\\nTurn 2:\")\n",
    "print(f\"  Action: {bad_conversation[1]['action']}\")\n",
    "print(f\"  (No tool use this turn)\")\n",
    "bad_turn_rewards.append(0.0)\n",
    "\n",
    "print(\"\\nOutcome Rewards:\")\n",
    "exact_match = 0.0  # Wrong answer\n",
    "answer_presence = 0.0  # Answer not present\n",
    "format_reward = -1.0  # No XML tags\n",
    "print(f\"  Exact Match: {exact_match}\")\n",
    "print(f\"  Answer Presence: {answer_presence}\")\n",
    "print(f\"  Format: {format_reward}\")\n",
    "\n",
    "bad_total = sum(bad_turn_rewards) + exact_match + answer_presence + format_reward\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TOTAL REWARD (BAD): {bad_total}\")\n",
    "print(f\"TOTAL REWARD (GOOD): {total_reward}\")\n",
    "print(f\"Difference: {total_reward - bad_total}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "dataset_info = {\n",
    "    'name': 'trivia_qa',\n",
    "    'config': 'rc',\n",
    "    'source': 'mandarjoshi/trivia_qa',\n",
    "    'split': 'train',\n",
    "    'num_examples': len(dataset),\n",
    "    'features': list(dataset.features.keys()),\n",
    "}\n",
    "\n",
    "with open('data/dataset_info.json', 'w') as f:\n",
    "    json.dump(dataset_info, f, indent=2)\n",
    "\n",
    "print(\"Dataset info saved to data/dataset_info.json\")\n",
    "print(f\"\\nDataset will be loaded directly from HuggingFace during training.\")\n",
    "print(f\"No S3 upload required!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: SageMaker Training Setup (SDK v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean install of SageMaker SDK v2.200.0\n",
    "!pip uninstall sagemaker sagemaker-core sagemaker-train sagemaker-serve sagemaker-mlops -y -q\n",
    "!pip install sagemaker==2.200.0 boto3 -q\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "from datetime import datetime\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'mt-grpo-training'\n",
    "\n",
    "print(f\"SageMaker role: {role}\")\n",
    "print(f\"S3 bucket: {bucket}\")\n",
    "print(f\"Region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = 'ml.p4d.24xlarge'  # 4x A10G GPUs\n",
    "# instance_type = 'ml.p4d.24xlarge'  # 8x A100 GPUs - uncomment for production\n",
    "\n",
    "instance_count = 1\n",
    "num_gpus = 8\n",
    "\n",
    "hyperparameters = {\n",
    "    'model_name': 'Qwen/Qwen2.5-3B',\n",
    "    'num_gpus': num_gpus,\n",
    "    'learning_rate': 1e-6,\n",
    "    'num_generations': 14,  # Must divide evenly into (num_gpus-1) * batch_size = 7*2=14\n",
    "    'per_device_train_batch_size': 2,\n",
    "    'grad_accum_steps': 4,\n",
    "    'num_iterations': 2,\n",
    "    'max_steps': 300,\n",
    "    'beta': 0,\n",
    "    'trainer': 'mt_grpo',\n",
    "    'turn_advantage_coef': 1,\n",
    "}\n",
    "\n",
    "print(f\"Instance type: {instance_type}\")\n",
    "print(f\"Number of GPUs: {num_gpus}\")\n",
    "print(f\"\\nHyperparameters:\")\n",
    "for k, v in hyperparameters.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Create Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f\"mt-grpo-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "# Use AWS Deep Learning Container for PyTorch 2.5.1\n",
    "image_uri = f'763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training:2.5.1-gpu-py311'\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point='train.py',\n",
    "    # Using requirements-pinned.txt for faster, conflict-free installation\n",
    "    source_dir='../scripts',\n",
    "    role=role,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=instance_count,\n",
    "    image_uri=image_uri,  # Use custom image instead of framework_version\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=f's3://{bucket}/{prefix}/output',\n",
    "    code_location=f's3://{bucket}/{prefix}/code',\n",
    "    checkpoint_s3_uri=f's3://{bucket}/{prefix}/checkpoints/{job_name}',\n",
    "    volume_size=1024,\n",
    "    max_run=24*60*60,\n",
    "    keep_alive_period_in_seconds=3600,\n",
    "    environment={'NCCL_DEBUG': 'INFO', 'VLLM_WORKER_MULTIPROC_METHOD': 'spawn',\n",
    "    'WANDB_API_KEY': ''},\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    "    base_job_name='mt-grpo',\n",
    ")\n",
    "\n",
    "print(f\"Estimator created: {job_name}\")\n",
    "print(f\"Using image: {image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Launch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Launching training job: {job_name}\")\n",
    "print(f\"Instance: {instance_type}\")\n",
    "print(f\"GPUs: {num_gpus}\")\n",
    "print(f\"\\nEstimated duration: 2-4 hours\")\n",
    "print(f\"Estimated cost: ~$20-40\\n\")\n",
    "\n",
    "estimator.fit(wait=True, logs='All')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Monitor and Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = estimator.latest_training_job.name\n",
    "print(f\"Training job: {training_job_name}\")\n",
    "print(f\"Status: {estimator.latest_training_job.describe()['TrainingJobStatus']}\")\n",
    "\n",
    "logs_url = f\"https://console.aws.amazon.com/cloudwatch/home?region={region}#logsV2:log-groups/log-group/$252Faws$252Fsagemaker$252FTrainingJobs/log-events/{training_job_name}\"\n",
    "print(f\"\\nCloudWatch Logs: {logs_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = estimator.model_data\n",
    "print(f\"Model artifacts: {model_data}\")\n",
    "\n",
    "local_model_dir = './trained_model'\n",
    "os.makedirs(local_model_dir, exist_ok=True)\n",
    "\n",
    "!aws s3 cp {model_data} {local_model_dir}/model.tar.gz\n",
    "!tar -xzf {local_model_dir}/model.tar.gz -C {local_model_dir}\n",
    "\n",
    "print(f\"\\nModel downloaded to: {local_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Dataset**: TriviaQA with questions and multiple answer aliases\n",
    "2. **Turn-Level Rewards**: Intermediate feedback during reasoning\n",
    "   - Tool execution success (+1/-1)\n",
    "   - Answer presence in search results (+1/0)\n",
    "3. **Outcome Rewards**: Final episode rewards\n",
    "   - Exact match with ground truth (+5)\n",
    "   - Answer presence in final response (+2)\n",
    "   - Format correctness (+1/-1)\n",
    "4. **Total Reward**: Sum of all turn and outcome rewards\n",
    "5. **Good vs Bad**: Saw 10 point difference between correct and incorrect responses\n",
    "\n",
    "### Training Configuration:\n",
    "- **Instance**: ml.g5.24xlarge (4 GPUs) or ml.p4d.24xlarge (8 GPUs)\n",
    "- **Model**: Qwen2.5-7B\n",
    "- **Method**: MT-GRPO (Multi-Turn GRPO with turn-level credit assignment)\n",
    "- **Cost**: ~$10-40/hour depending on instance\n",
    "- **Duration**: 2-4 hours for 300 steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (notebook-env)",
   "language": "python",
   "name": "notebook-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
