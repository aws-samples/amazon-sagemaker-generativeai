project_name: distil-logits

models:
  student_model_name_or_path: Qwen/Qwen3-0.6B
  teacher_model_name_or_path: Qwen/Qwen3-1.7B

model_config:
  use_flash_attention: false

dataset:
  dataset_name_or_path: /opt/ml/input/data/dataset/dataset.json
  dataset_split: train
  seed: 42

tokenizer:
  chat_template: '{% for message in messages %}{% if loop.first and messages[0][''role'']
    != ''system'' %}{{ ''<|im_start|>system

    You are a helpful assistant.<|im_end|>

    '' }}{% endif %}{{''<|im_start|>'' + message[''role''] + ''

    '' + message[''content''] + ''<|im_end|>'' + ''

    ''}}{% endfor %}{% if add_generation_prompt %}{{ ''<|im_start|>assistant

    '' }}{% endif %}'
  max_length: 4096

training:
  bf16: true
  fp16: false
  gradient_accumulation_steps: 1
  hub_model_id: ''
  learning_rate: 2.0e-05
  logging_steps: 1
  lr_scheduler_type: cosine
  num_train_epochs: 1
  output_dir: '/opt/ml/model/Qwen3-1.7b-to-Qwen3-0.6b'
  per_device_train_batch_size: 2
  push_to_hub: false
  resume_from_checkpoint: null
  save_steps: 100000000
  warmup_ratio: 0.1
  weight_decay: 0.05
  report_to: mlflow

distillation:
  alpha: 0.5
  temperature: 2.0

hub_location:
  hub_location: ''