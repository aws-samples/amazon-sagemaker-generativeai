{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d98e7379-6879-415c-856f-3ace919026d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Summarization on Custom Dataset with SageMaker Jumpstart and [LangChain](https://python.langchain.com/en/latest/index.html) Library\n",
    "\n",
    "Reference: https://github.com/gkamradt/langchain-tutorials/tree/main/data_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d4f450-2126-416f-a6a1-b3adfc14a921",
   "metadata": {},
   "source": [
    "\n",
    " There are two main types of methods for summarizing text: abstractive and extractive.\n",
    "\n",
    "Abstractive summarization generates a new shorter summary in its own words based on understanding the meaning and concepts of the original text. It analyzes the text using advanced natural language techniques to grasp the key ideas and then expresses those ideas in a summarized form using different words and phrases. This is similar to how humans summarize by reading something and then explaining the main points in their own words.\n",
    "\n",
    "Extractive summarization works by selecting the most important sentences, phrases or words from the original text to construct a summary. It calculates the weight or importance of each part of the text using algorithms and then chooses the parts with the highest weights to put into the summary. This pulls summarizes by extracting key elements from the text itself rather than interpreting the meaning.\n",
    "\n",
    "So in short, abstractive summarization rewrites the key ideas in new words while extractive summarization selects the most salient parts of the existing text. Both aim to distill the essence and most significant information from the original document into a condensed summary.\n",
    "\n",
    "We're going to run through 3 methods for summarization that start with basic prompting to summarizing large documents using `map_reduce` method. These aren't the only options, feel free to modify it based on your use case. \n",
    "\n",
    "**3 Levels Of Summarization:**\n",
    "1. **Summarize a couple sentences** - Basic Prompt\n",
    "2. **Summarize a couple paragraphs** - Prompt Templates\n",
    "3. **Summarize a large document with multiple pages** - Map Reduce\n",
    "\n",
    "In this notebook we will demonstrate how to use a **Falcon 7b Instruct** model for text summarization using a library of documents as a reference.\n",
    "\n",
    "**This notebook serves a template such that you can easily replace the example dataset by your own to build a custom text summarization application. Let's install some dependencies that will be required and initialize some basic variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def9c9e3-ab82-4be3-bfdf-8e1ca9bf3e98",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade sagemaker\n",
    "!pip install langchain\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install ipywidgets --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bcd837-3614-4a0d-a35b-f93a5131b2a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Note\n",
    "You must Restart Kernel here for the installations to take effect. After restarting kernel, run the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a95503-078d-4571-9939-6fa420e73466",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "print(f\"Region is {aws_region}, Role is {aws_role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e371d3e0-9666-4ade-a901-f508c2110d50",
   "metadata": {},
   "source": [
    "###  Get the deployed model instance\n",
    "Replace the variable `endpoint_name` with the name of the endpoint you deployed for the Falcon 7V model. You can retrieve that by navigating on the left side through Home -> Deployments -> Endpoints. Select the ednpoint that starts with \"hf-llm-falcon-7b-instruct-bf16-\" and replace its value with the variable `endpoint_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d57e89-8748-453e-bf64-ed9721288b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the existing endpoint name\n",
    "endpoint_name = \"<endpoint name>\"  # Replace with your endpoint name\n",
    "\n",
    "# Create a SageMaker predictor object\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "CONTENT_TYPE = 'application/json'\n",
    "\n",
    "predictor.endpoint_name "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317ff31d-d676-45c3-90f1-7c483dbf2e32",
   "metadata": {},
   "source": [
    "## Summarize a few sentences \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c922e936-9da3-4b7d-b56d-ac3c92af2e55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Given the following text, provide a coincise and complete summary.\n",
    "\n",
    "Text:\n",
    "Philosophy (from Greek: φιλοσοφία, philosophia, 'love of wisdom') \\\n",
    "is the systematized study of general and fundamental questions, \\\n",
    "such as those about existence, reason, knowledge, values, mind, and language. \\\n",
    "Some sources claim the term was coined by Pythagoras (c. 570 – c. 495 BCE), \\\n",
    "although this theory is disputed by some. Philosophical methods include questioning, \\\n",
    "critical discussion, rational argument, and systematic presentation.\n",
    "\n",
    "Summary:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9116c5b5-9dcd-459c-902d-9ebd06b4e6fb",
   "metadata": {},
   "source": [
    "In order to use our model endpoint with LangChain we wrap up endpoints for LLM into `langchain_community.llms.sagemaker_endpoint.SagemakerEndpoint` which is LangChain's built in support for SageMaker endpoints. \n",
    "\n",
    "Replace the 'InferenceComponentName' value with the name of the model you deployed for the Falcon 7b model. You can retrieve that by navigating on the left side through Home -> Deployments -> Endpoints. Select the model name that starts with \"huggingface-llm-falcon-7b-instruct-bf16-\" and replace its value with the place holder `<model name>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58acef7-fda1-4a57-9dd7-f4777e4a67d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from langchain_community.llms import SagemakerEndpoint\n",
    "from langchain_community.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt,  \"parameters\": model_kwargs})\n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "sm_llm=SagemakerEndpoint(\n",
    "        endpoint_name=endpoint_name, \n",
    "        region_name=aws_region,\n",
    "        endpoint_kwargs={\"InferenceComponentName\":'<model name>'},\n",
    "        model_kwargs={\"do_sample\": True,\n",
    "                                    \"top_p\": 0.9,\n",
    "                                    \"temperature\": 0.5,\n",
    "                                    \"max_new_tokens\":  100,\n",
    "                                    \"stop\": [\"<|endoftext|>\", \"</s>\"]},\n",
    "        content_handler=content_handler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e94fec1-88fb-4abf-8297-fafcafcd514f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_tokens = sm_llm.get_num_tokens(prompt)\n",
    "print (f\"Our prompt has {num_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ebd258-5276-4d81-b907-ae7e66fd315a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = sm_llm(prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3281803b-7889-416c-afbd-dc8cfebda57b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Given the following text, write a 1 line summary.\n",
    "\n",
    "Text:\n",
    "Philosophy (from Greek: φιλοσοφία, philosophia, 'love of wisdom') \\\n",
    "is the systematized study of general and fundamental questions, \\\n",
    "such as those about existence, reason, knowledge, values, mind, and language. \\\n",
    "Some sources claim the term was coined by Pythagoras (c. 570 – c. 495 BCE), \\\n",
    "although this theory is disputed by some. Philosophical methods include questioning, \\\n",
    "critical discussion, rational argument, and systematic presentation.\n",
    "\n",
    "Summary:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1e7bec-cf0d-45dc-88e0-03f381b496e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = sm_llm(prompt)\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ec95dd-4c5b-4db6-8a5c-ce077e9c9e39",
   "metadata": {},
   "source": [
    "##  Summarize a couple paragraphs -  Prompt Templates\n",
    "---\n",
    "\n",
    "Prompt templates are a great way to dynamically place text within your prompts. They are like [python f-strings](https://realpython.com/python-f-strings/) but specialized for working with language models.\n",
    "\n",
    "We're going to look at 2 short Paul Graham essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cd99d5-4fc5-478a-aac7-0b4212059973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"chromadb/paul_graham_essay\", trust_remote_code=True)\n",
    "essay1 = dataset['data'][0]['document']\n",
    "essay2 = dataset['data'][1]['document']\n",
    "\n",
    "essays=[essay1, essay2]\n",
    "for essay in essays:\n",
    "    print(essay)\n",
    "    print(\"===============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a80929-324f-44fb-8a7f-0b253863d8f9",
   "metadata": {},
   "source": [
    "Next let's create a prompt template which will hold our instructions and a placeholder for the essay. In this example we only want a 1 sentence summary to come back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c327a-0daf-4c76-94db-82c9a30b5e28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Given the following text, write a short summary.\n",
    "\n",
    "Text: {essay}\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"essay\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f3007a",
   "metadata": {},
   "source": [
    "Replace the 'InferenceComponentName' value with the name of the model you deployed for the Falcon 7b model. You can retrieve that by navigating on the left side through Home -> Deployments -> Endpoints. Select the model name that starts with \"huggingface-llm-falcon-7b-instruct-bf16-\" and replace its value with the place holder `<model name>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418ab48-ad3e-445f-83ae-f0ac4bf1169a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_llm=SagemakerEndpoint(\n",
    "        endpoint_name=endpoint_name, \n",
    "        region_name=aws_region,\n",
    "        endpoint_kwargs={\"InferenceComponentName\":'<model name>'},\n",
    "        model_kwargs={\"do_sample\": True,\n",
    "                                    \"top_p\": 0.9,\n",
    "                                    \"temperature\": 0.8,\n",
    "                                    \"max_new_tokens\":  200,\n",
    "                                    \"stop\": [\"<|endoftext|>\", \"</s>\"]},\n",
    "        content_handler=content_handler\n",
    "    )\n",
    "\n",
    "for essay in essays:\n",
    "    summary_prompt = prompt.format(essay=essay)\n",
    "    \n",
    "    num_tokens = sm_llm.get_num_tokens(summary_prompt)\n",
    "    print (f\"--> This prompt + essay has {num_tokens} tokens\")\n",
    "    \n",
    "    summary = sm_llm(summary_prompt)\n",
    "    \n",
    "    print (f\"Summary: {summary.strip()}\")\n",
    "    print (\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b6a8c9-c9ce-4178-9474-095ed6ff6879",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Summarize large text  from multiple pages of a document - MapReduce\n",
    "---\n",
    "\n",
    "If you have multiple pages you'd like to summarize, you'll likely hve large amounts of text and will likely run into a token limit. Token limits won't always be a problem, but it is good to know how to handle them if you run into the issue.\n",
    "\n",
    "The chain type \"Map Reduce\" is a method that helps with this. You first generate a summary of smaller chunks (that fit within the token limit) and then you get a summary of the summaries.\n",
    "\n",
    "Check out [this video](https://www.youtube.com/watch?v=f9_BWhCI4Zo) for more information on how chain types work. We will use articles from the PubMed dataset available via HuggingFace `datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2404fbe2-5894-4429-b987-274cacbe855e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ccdv/pubmed-summarization\", trust_remote_code=True)\n",
    "essay = dataset['train'][0]['article']\n",
    "print(essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf80a98-b756-4a14-885f-25a52afc9811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b991fc-b213-4d7f-a9c4-2347b13d8872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_llm.get_num_tokens(essay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b6fa0-d43d-47bf-9b31-efe1d58ed95e",
   "metadata": {},
   "source": [
    "That's too many, let's split our text up into chunks so they fit into the prompt limit. I'm going a chunk size of 2,000 characters. \n",
    "\n",
    "> You can think of tokens as pieces of words used for natural language processing. For English text, **1 token is approximately 4 characters** or 0.75 words. As a point of reference, the collected works of Shakespeare are about 900,000 words or 1.2M tokens.\n",
    "\n",
    "This means the number of tokens we should expect is 2,000 / 4 = ~500 token chunks. But this will vary, each body of text/code will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4782136-d2bd-41c6-bf9d-78b5efb3e0f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\"], chunk_size=2000, chunk_overlap=500)\n",
    "\n",
    "docs = text_splitter.create_documents([essay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3186a918-89ea-478a-b958-46aae447bf1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_docs = len(docs)\n",
    "\n",
    "num_tokens_first_doc = sm_llm.get_num_tokens(docs[0].page_content)\n",
    "\n",
    "print (f\"Now we have {num_docs} documents and the first one has {num_tokens_first_doc} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d06706-a5b6-43e6-bd79-b77f7b159693",
   "metadata": {},
   "source": [
    "Great, assuming that number of tokens is consistent in the other docs we should be good to go. Let's use LangChain's [load_summarize_chain](https://python.langchain.com/en/latest/use_cases/summarization.html) method, we will use `refine` chain type for summarization. We first need to initialize our chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2072ddf5-4fdf-4df5-9ac5-503c06578c04",
   "metadata": {},
   "source": [
    "Our document is pretty large and has 19 chunks, so lets pick the first few chunks and try to summarize them using LangChain's load_summarize_chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95ff5b9-7339-4052-9e36-6ac2ee5383c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_chain = load_summarize_chain(llm=sm_llm, chain_type='map_reduce',\n",
    "                                     verbose=True # Set verbose=True if you want to see the prompts being used\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807e202-ab77-4e5e-bfe6-e9244e3d2c7a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = summary_chain.run(docs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acc08a7-d9d7-4dc9-9abe-46e80a950820",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(output.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a35f4a-2e49-41ca-96bb-d7909ccc847c",
   "metadata": {},
   "source": [
    "---\n",
    "This summary is a great start, but since we took partial text our resulting summary isn't great and is left incomplete. This can be solved with a bit of prompt engineering but ideally we would like to summarize the whole document. So, lets modify to summarize the entire document and get only the key points as the final summary.\n",
    "\n",
    "In order to do this we will use custom prompts (like we did above) to instruct the model on what we need. But this time, instead of using just 5 chunks of the given document, we will use all chunks of the documents and use a MapReduce Summary chain from LangChain and our Falcon model hosted in SageMaker.\n",
    "\n",
    "We will Summarize the document using LangChain MapReduce summary chain\n",
    "\n",
    "- We will first generate summaries of the smaller chunks (map)\n",
    "- Then we will generate a narrative using the generated summaries (reduce)\n",
    "- Then we will use the shortened narrative to generate final key themes, summary of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce45dbd-72bd-4adc-957f-8b69d20987da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "llm =SagemakerEndpoint(\n",
    "        endpoint_name=endpoint_name, \n",
    "        region_name=aws_region,\n",
    "        endpoint_kwargs={\"InferenceComponentName\":'<model name>'},\n",
    "        model_kwargs={\"do_sample\": True,\n",
    "                                    \"top_p\": 0.9,\n",
    "                                    \"temperature\": 0.8,\n",
    "                                    \"max_new_tokens\":  100,\n",
    "                                    \"stop\": [\"<|endoftext|>\", \"</s>\"]},\n",
    "        content_handler=content_handler\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc30af-9a69-44a9-83b7-f4c3ac538fd7",
   "metadata": {},
   "source": [
    "Let's define the Map chain that will generate summaries of each of the 30 chunks. In this case, you can see that it is just a regular LLMChain with a simple summary prompt. This is because we simply want to run summary on each of the indovidual chunks of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59da55b6-f062-4d31-8065-a3a403cef931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Map\n",
    "map_template = \"\"\"Given the following text, write a short summary.\n",
    "\n",
    "Text: {docs}\n",
    "Summary: \"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5714e10-6c35-4bb5-91bb-1d295520eba3",
   "metadata": {},
   "source": [
    "We then define the reduce chain. The purpose of this chain is to take all the generated summaries (by the map chain) and generate a single final summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985bf702-17c8-4890-b5a5-87ec20e2619c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reduce\n",
    "reduce_template = \"\"\"The following is set of summaries. Take these and distill it into a final, consolidated summary of the main themes. \n",
    "\n",
    "Text: {doc_summaries}\n",
    "Summary: \"\"\"\n",
    "\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391ce736-cfab-4dd6-94af-5fb56147b075",
   "metadata": {},
   "source": [
    "We then define a chain that combines all the generated summaries from the Map chain, subsequently pass it to the Reduce chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b959606-0d1f-44d8-a6a7-d401c1ce6abf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a0e9d-8526-426d-8452-cf9d131616f0",
   "metadata": {},
   "source": [
    "Finally we define the overall MapReduceDocumentsChain. This chain takes care of executing all the chains we have defined so far, passing the output(s) from one to the other to  generate the final summary. If you want to be able to see each of the steps as they execute, you can pass `verbose = True` in the `map_chain` and the `reduce_chain` initializations above. For this exercise we kept it default to False, but feel free to change it and execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffd55ca-6338-488a-9b03-215eeb5a9852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522c1dec-1592-4a27-abad-27cde3bbc1b4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we have already split our document into chunks previously so we will use it now\n",
    "print(map_reduce_chain.run(docs))"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
