{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 - Building an LLM-powered Natural Language to SQL Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Generative AI and Large Language Models (LLMs) have recently gained a lot of popularity due to their advanced natural language processing and understanding abilities. In 2018, the state of the art NLP task accuracy was set by BERT-large with its 340M parameters and novel transformers architecture. After just a few years, state-of-the-art model size has grown by more than 500x with models such as OpenAI‚Äôs 175 billion parameter GPT-3 and the similarly sized open source Falcon 180B by TII, raising the bar on NLP accuracy. The surge in parameter count is caused by the straightforward and empirically proven correlation between model size and performance that bigger often means better. As model repositories like HuggingFace, that provide convenient access to models and NLP tasks such as classification and text/code generation, continue to improve, users are increasingly opting for these large models. Nonetheless, the deployment of these models can present challenges due to their substantial size. \n",
    "\n",
    "In our era of data driven enterprises, a lot of data is stored in structured databases. SQL (Structured Query Language) has long been the language of data manipulation, enabling professionals to extract valuable insights from vast databases. However, for many, learning SQL has a steep learning curve and thus remains intimidating. \n",
    "With the rise of LLMs, we can now leverage their code generation capabilities to interact with these database using natural language, having SQL only as an intermediate step. \n",
    "This allows us to democratise data to non-technical people, build chat-bots on enterprise data, and also assist SQL experts like data scientists with their tasks. \n",
    "\n",
    "In this Lab we will explore how to build a Generative AI powered application to query structured data from Amazon Relational Database Service using natural language on AWS.\n",
    "\n",
    "We will create an RDS Database instance backed by the Amazon Aurora PostgreSQL engine and populate it with synthetic data. Furthermore, we will deploy the [Code Llama](https://ai.meta.com/blog/code-llama-large-language-model-coding/) (7B) model by [Meta](https://ai.meta.com/) using Amazon SageMaker Jumpstart. \n",
    "Our database schema of synthetic data looks as follows:\n",
    "\n",
    "![database schema](./fe/schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background and details\n",
    "\n",
    "We have two primary types of knowledge for LLMs:\n",
    "\n",
    "* Parametric knowledge: refers to everything the LLM learned during training and acts as a frozen snapshot of the world for the LLM.\n",
    "* Source knowledge: covers any information fed into the LLM via the input prompt.\n",
    "\n",
    "When infusing knowledge into Generative AI powered applications, we need to decide which of these types to target. In previous labs, we have elevated the parametric knowledge through fine tuning. Fine-tuning can be a resource intensive task and many practitioners opt for targeting the source knowledge. Luckily, Meta's Code Llama, comes as a fine-tuned version of Llama-2, for code-secific tasks. This Lab is about using prompt engineering techniques to leverage the code capabilities of our model to query our specific data.  \n",
    "The application we will be building in a step-by-step process will be a LLM powered dashboard for you to query synthetic data of a made up software company. You will be able to query the database through a frontend and receive information, specific to the underlying data. \n",
    "\n",
    "\n",
    "### Workflow\n",
    "\n",
    "The workflow of our demo application looks as following. We feed the (1) user's question in natural language to our model hosted on an SageMaker Endpoint using some prompt engineering techniques, which we dive deeper into in this Notebook. (2) The LLM converts the question in natural language to a SQL query, which is (3) directly excecuted on our database. Our application is designed in a way to (4) return the results of the query to the user directly, alongside with (5) an answer to the question in Natural Language.\n",
    "\n",
    "![workflow](./img/workflow.png)\n",
    "\n",
    "### Architecture\n",
    "The mentioned architecture is implemented using the three main services: \n",
    "* AWS Elastic Beanstalk\n",
    "* Amazon SageMaker\n",
    "* Amazon Relational Database Service (RDS)\n",
    "\n",
    "The architecture diagram shows how these services interact for our application.\n",
    "\n",
    "![architecture](./img/arch_codellama.png)\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Prerequisites\n",
    "To run this Lab, you need \n",
    "* a computer with a web browser\n",
    "* an AWS account with access to `ml.g5.2xlarge` instances\n",
    "* Access to the Code Llama model by Meta AI through SageMaker Jumpstart\n",
    "\n",
    "#### Recommended Background \n",
    "It will be easier for you to run this workshop if you have:\n",
    "\n",
    "* Experience with Large Language Models (LLMs) and prompt engineering techniques\n",
    "* Familiarity with Python or other similar programming languages\n",
    "* Familiarity with SQL and relational databases\n",
    "* Experience with Jupyter notebooks\n",
    "* Beginners level knowledge and experience with SageMaker Hosting/Inference.\n",
    "* Beginners level knowledge and experience with Large Language Models\n",
    "\n",
    "\n",
    "#### Target audience\n",
    "Data Scientists, ML Engineering, ML Infrastructure, MLOps Engineers, Technical Leaders. Intended for customers working with large Generative AI models including Language, Computer vision and Multi-modal use-cases. Customers using Elastic Beanstalk for hosting or experience with SageMaker.\n",
    "\n",
    "Level of expertise - 400\n",
    "\n",
    "#### Time to complete\n",
    "Approximately 1 hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import of required dependencies\n",
    "\n",
    "For this lab, we will be needing the following packages:\n",
    "\n",
    "- `sagemaker`: SageMaker SDK for interacting with Amazon SageMaker.\n",
    "- `boto3`: The boto3 library is the AWS SDK for python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the notebook environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you are using this notebook in an Amazon SageMaker Notebook Instance for the cell below to work. Here we will retrieve the region specific ARN for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin with building the application, we need to setup the notebook environment properly. This includes:\n",
    "* retrieval of the chosen region for later usage\n",
    "* retrieval of the SageMaker Session object for later usage, to manage interactions with the Amazon SageMaker APIs and any other AWS services needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve region\n",
    "region = boto3.Session().region_name\n",
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve SageMaker Session\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model to an endpoint for real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Code Llama](https://ai.meta.com/blog/code-llama-large-language-model-coding/) is a model released by Meta built on top of Llama 2 that is designed to help programmers write high quality, well-documented code more efficiently. It supports major programming languages like Python, C++, Java, and can understand natural language instructions. Code Llama comes in three variants for different applications - a general model, a Python specialized model, and one focused on instruction-following. All variants are available in three sizes - 7B, 13B and 34B parameters - to cover a wide range of use cases. Here we leverage the SQL generation capabilities of the foundational Code Llama 7B model, to strike the right balance between exceptional quality and affordability.\n",
    "\n",
    "\n",
    "In this section we will be deploying the Code Llama model to a SageMaker endpoint for real time inference. Let us first define an endpoint name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = \"code-llama-7b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While creating an endpoint, you can choose the instance type to run the model on. If you don't specify it, it runs on a default instance type, which is an `ml.g5.2xlarge` for Code Llama 7b. We follow this [blog post](https://aws.amazon.com/blogs/machine-learning/code-llama-code-generation-models-from-meta-are-now-available-via-amazon-sagemaker-jumpstart/) for deployment and make use of the default instance.\n",
    "\n",
    "If you want to understand how real-time inference with Amazon SageMaker works, see [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html).\n",
    "\n",
    "Keep in mind, if your model is already deployed to an endpoint of the same name, or an endpoint configuration of the same name is already created, then this should fail! Deployment can take a while. For us it took up to 10 minutes to deploy the `Code Llama 7B` model to a `ml.g5.2xlarge` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "model = JumpStartModel(model_id=\"meta-textgeneration-llama-codellama-7b\")\n",
    "predictor = model.deploy(\n",
    "    endpoint_name=endpoint_name\n",
    "    # instance_type=\"ml.g5.2xlarge\", # uncomment to use another instance\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model should now be deployed. Let us now try interacting with the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact with the model (prompting) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While interacting with our model, there is some things we must think about. First of all the model has arguments, also known as parameters, which let you control the models responses.\n",
    "The main thing we need to think about here however, is the input, that we send into out model, which is called the prompt. Think of the model as a sentence completing algorithm, where it simply completes our prompt.\n",
    "First we try the [zero shot prompting](https://www.promptingguide.ai/techniques/zeroshot) method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-Shot prompting\n",
    "\n",
    "We ask the model to create an SQL statement given our instruction in natural language as you can see below. This should generate us a correct SQL statement and your output should look similar to this: \n",
    "```SQL \n",
    "SELECT * FROM employees;\n",
    "``` \n",
    "Next to our prompt, we also need to feed in hyperparameters. The hyperparameters are: \n",
    "* `max_new_tokens`, define the maximum number of tokens to generate in this call.\n",
    "* `temperature`, which let's you control how deterministic a model's responses are. Setting temperature to 1.0 samples directly from the model distribution. Lower (higher) values increase the chance of sampling higher (lower) probability tokens. A value of 0 essentially disables sampling and results in greedy decoding, where the most likely token is chosen at every step. To find out more about temperature you can have a look [here](https://www.promptingguide.ai/introduction/settings.en). We choose relatively low temperature, as we want deterministic SQL results.\n",
    "* `top_p` is also a sampling technique, with which you can control how deterministic the models generation is. A lower value keeps the answers factual, while higher values make the responses more diverse. Intuitively, instead of sampling only from the most likely _k_ words, this method samples from the smallest possible set of words, whose summed up probability exeeds `p`. This is also known as nucleus sampling.\n",
    "\n",
    "Furthermore, you must explicitly accept the EULA to deploy the model via SageMaker SDK. Note that by default, `accept_eula` is set to `false`. You need to set `accept_eula=true` to invoke the endpoint successfully. By doing so, you accept the user license agreement and acceptable use policy. You can also [download](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) the license agreement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruction = \"\"\"\n",
    "Create an executable SQL statement from instruction:\n",
    "\n",
    "Instruction:\n",
    "Who are the employees of the company?\n",
    "\n",
    "SQL Query:\n",
    "\"\"\"\n",
    "\n",
    "payload = {\n",
    "   \"inputs\": instruction,\n",
    "   \"parameters\": {\"max_new_tokens\": 10, \"temperature\": 0.2, \"top_p\": 0.9}\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes=\"accept_eula=true\")\n",
    "print(response[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the zero-shot method works well for simple queries, it would fail for more complex ones. Here we ask it to list all the SDEs of the company. SDE is an [ambigous](https://en.wikipedia.org/wiki/SDE) term used for Software Engineers or to be precise Software Development Engineers. \n",
    "\n",
    " We expect our model to understand, that by `SDEs`, we mean employees, who have the designation `Software Engineer` but without context, the model has too little information to create a correct SQL query and might interpret the term SDE as something else. Also, due to lack of context, it might make up table and column names for the database. For some context, this is how our `employees` table looks like:\n",
    "\n",
    "| employee_id\t|   first_name  |\tlast_name\t|   designation             |   project_id  |   email                   |   manager_id  |\n",
    "| ----------- | ----------- | ----------- | ----------- | ----------- | ----------- |----------- |\n",
    "| 91021111      |\tMax\t        |   Mustermann\t|   Software Engineer\t    |   283921\t    |   mustermann@company.com  |\t48234012|\n",
    "| 91021151      |\tNed\t        |   Stark       |   Chief Technology Officer|\t- |   nstark@company.com\t    |   -   |\n",
    "| 10921212      |\tDolly       |\tMeinhard\t|   Project Manager         |\t131032      |\tdmein@company.com       |\t91021111|\n",
    "| 48234012      |\tJosey   \t|   Rojas\t    |   Product Manager        \t|   283921\t    |   jroj@company.com        |\t91021111|\n",
    "| 43204121      |\tFidel       |   Wind\t    |   Software Engineer\t    |   311092      |\tfwind@company.com\t    |   1823012 |\n",
    "| 1823012   \t|   Peter       |\tKabel\t    |   Software Engineer\t    |   131032      |\tpkabel@company.com      |\t13220812|\n",
    "| 13220812      |\tVelma       |\tCabello     |\tData Scientist          |\t933012      |\tvcabello@company.com    |\t91021111|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruction = \"\"\"\n",
    "Create an executable SQL statement from instruction:\n",
    "\n",
    "Instruction:\n",
    "Who are the SDEs?\n",
    "\n",
    "SQL Query:\n",
    "\"\"\"\n",
    "\n",
    "payload = {\n",
    "   \"inputs\": instruction,\n",
    "   \"parameters\": {\"max_new_tokens\": 15, \"temperature\": 0.2, \"top_p\": 0.9}\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes=\"accept_eula=true\")\n",
    "print(response[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle problems, which we mentioned earlier, lets provide more context to our model and try a [few-shot](https://www.promptingguide.ai/techniques/fewshot) approach. We will split this into multiple steps:\n",
    "1. curate our prompt template using LangChains [`PromptTemplate`](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/) module\n",
    "2. feed in our database context \n",
    "3. add in some example queries for our few-shot approach. \n",
    "\n",
    "[LangChain](https://www.langchain.com/) is a framework built to simplify the creation of applications using Large Language Models (LLMs). For our application we use it throughout our prompting process. We leverage its integration with SageMaker Endpoints and Relational Databases to connect directly with our services. For more information on what it offers, take a look at the [documentation](https://python.langchain.com/docs/get_started/introduction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "TEMPLATE = \"\"\"Given an input question, create a syntactically correct {dialect} query to run.\n",
    "Use the following format:\n",
    "\n",
    "Question: \"Question here\"\n",
    "SQLQuery:\n",
    "\"SQL Query to run\"\n",
    "\n",
    "Only use the following tables:\n",
    "\n",
    "{table_info}.\n",
    "\n",
    "Some examples of SQL queries that correspond to questions are:\n",
    "\n",
    "{few_shot_examples}\n",
    "\n",
    "Question: {input}\n",
    "SQLQuery:\n",
    "\"\"\"\n",
    "\n",
    "CUSTOM_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"input\", \"few_shot_examples\", \"table_info\", \"dialect\"], template=TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table Context\n",
    "Now we will add some information about our database tables into the prompt. For this we add information from all tables following the best practices specified in [Rajkumar et. al., 2022](https://arxiv.org/abs/2204.00498), in which they found the `CREATE TABLE` table definition syntax to give promising results. Additionally we add in the top 4 rows of each table, in the same way as done in the paper.\n",
    "\n",
    "Here we define our table information `table_info` as a variable and pass it as an argument into our prompt in a later cell.\n",
    "<a id='db_context_cell'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define table info\n",
    "\n",
    "table_info = \"\"\"\n",
    "\n",
    "CREATE TABLE employees (\n",
    "\temployee_id INTEGER NOT NULL, \n",
    "\tfirst_name VARCHAR(50) NOT NULL, \n",
    "\tlast_name VARCHAR(50) NOT NULL, \n",
    "\tdesignation VARCHAR(50) NOT NULL, \n",
    "\tproject_id INTEGER, \n",
    "\temail VARCHAR(255) NOT NULL, \n",
    "\tmanager_id INTEGER, \n",
    "\tCONSTRAINT employees_pkey PRIMARY KEY (employee_id), \n",
    "\tCONSTRAINT employees_manager_id_fkey FOREIGN KEY(manager_id) REFERENCES employees (employee_id), \n",
    "\tCONSTRAINT employees_project_id_fkey FOREIGN KEY(project_id) REFERENCES projects (project_id), \n",
    "\tCONSTRAINT employees_email_key UNIQUE (email)\n",
    ")\n",
    "\n",
    "/*\n",
    "4 rows from employees table:\n",
    "employee_id\tfirst_name\tlast_name\tdesignation\tproject_id\temail\tmanager_id\n",
    "91021111\tMax\tMustermann\tSoftware Engineer\t283921\tmustermann@company.com\t48234012\n",
    "91021151\tNed\tStark\tChief Technology Officer\tNone\tnstark@company.com\tNone\n",
    "10921212\tDolly\tMeinhard\tProject Manager\t131032\tdmein@company.com\t91021111\n",
    "48234012\tJosey\tRojas\tProduct Manager\t283921\tjroj@company.com\t91021111\n",
    "*/\n",
    "\n",
    "\n",
    "CREATE TABLE projects (\n",
    "\tproject_id INTEGER NOT NULL, \n",
    "\tproject_name VARCHAR(255) NOT NULL, \n",
    "\tcutomer VARCHAR(25), \n",
    "\tCONSTRAINT projects_pkey PRIMARY KEY (project_id)\n",
    ")\n",
    "\n",
    "/*\n",
    "4 rows from projects table:\n",
    "project_id\tproject_name\tcutomer\n",
    "283921\tRestaurant Management App\tThe Mozzarella Fellas\n",
    "131032\tGarden Planner\tFlamingo Gardens\n",
    "933012\tMusic generator\tElvisAI\n",
    "311092\tWeather forecasting system\tFlamingo Gardens\n",
    "*/\n",
    "\n",
    "\n",
    "CREATE TABLE timelog (\n",
    "\tentry_id INTEGER NOT NULL, \n",
    "\temployee INTEGER, \n",
    "\tworking_day DATE, \n",
    "\tentered_hours INTEGER, \n",
    "\tCONSTRAINT timelog_pkey PRIMARY KEY (entry_id), \n",
    "\tCONSTRAINT timelog_employee_fkey FOREIGN KEY(employee) REFERENCES employees (employee_id)\n",
    ")\n",
    "\n",
    "/*\n",
    "4 rows from timelog table:\n",
    "entry_id\temployee\tworking_day\tentered_hours\n",
    "10000002\t91021111\t2022-01-03\t3\n",
    "10000003\t91021151\t2022-01-03\t4\n",
    "10000004\t10921212\t2022-01-03\t7\n",
    "10000005\t48234012\t2022-01-03\t4\n",
    "*/\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-Shot prompting\n",
    "Now we define some examples for our model, which we will use in our prompt as well. Here we show, in which style and format we want the answer through examples. We try to cover as many different query types as possible to let the model generalize as well as possible. We limit ourselves to 6 samples, because we don't want to bias our model towards the sample queries when running inference. The number of examples that should be chosen is heaviliy dependent on the size and complexity of the data and should be chosen on a case to case basis. For more complex databases it might make sense to store multiple few-shot examples using a vector database and use only a few of them dynamically for each query. More information and an example on this approach can be found [here](https://python.langchain.com/docs/use_cases/qa_structured/sql#including-dynamic-few-shot-examples) and for more information on Retrieval Augmented Generation (RAG) see [here](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define few shot examples\n",
    "few_shot_examples = \"\"\"\n",
    "\n",
    "Question: Who worked the most hours in 2022?\n",
    "SQL Query:\n",
    "SELECT e.first_name, e.last_name, SUM(t.entered_hours) AS total_hours_worked\n",
    "FROM employees e\n",
    "JOIN timelog t ON e.employee_id = t.employee_id\n",
    "WHERE EXTRACT(YEAR FROM t.working_day) = 2022\n",
    "GROUP BY e.employee_id, e.first_name, e.last_name\n",
    "ORDER BY total_hours_worked DESC\n",
    "LIMIT 1;\n",
    "\n",
    "##\n",
    "\n",
    "Question: How many Software Engineers does the company have?\n",
    "SQL Query:\n",
    "SELECT COUNT(*) from employees\n",
    "WHERE designation='Software Engineer';\n",
    "\n",
    "##\n",
    "\n",
    "Question: How many hours did Velma work in July 2022?\n",
    "SQL Query:\n",
    "SELECT SUM(t.entered_hours) AS total_hours_worked\n",
    "FROM employees e\n",
    "JOIN timelog t ON e.employee_id = t.employee\n",
    "WHERE e.first_name = 'Velma'\n",
    "  AND EXTRACT(YEAR FROM t.working_day) = 2022\n",
    "  AND EXTRACT(MONTH FROM t.working_day) = 7;\n",
    "\n",
    "##\n",
    "\n",
    "Question: Who is working on the Music generator project?\n",
    "SQL Query:\n",
    "SELECT * FROM employees\n",
    "WHERE project_id=(\n",
    "SELECT project_id FROM projects\n",
    "WHERE project_name = 'Music generator'\n",
    ");\n",
    "\n",
    "##\n",
    "\n",
    "Question: Who works under Max?\n",
    "SQL Query:\n",
    "SELECT * FROM employees\n",
    "WHERE manager_id=(\n",
    "SELECT employee_id FROM employees\n",
    "WHERE first_name = 'Max');\n",
    "\n",
    "##\n",
    "\n",
    "Question: Who worked the least hours in April 2022?\n",
    "SQL Query:\n",
    "SELECT e.first_name, e.last_name, SUM(t.entered_hours) AS total_hours_worked\n",
    "FROM employees e\n",
    "JOIN timelog t ON e.employee_id = t.employee\n",
    "WHERE EXTRACT(YEAR FROM t.working_day) = 2022\n",
    "  AND EXTRACT(MONTH FROM t.working_day) = 4\n",
    "GROUP BY e.employee_id, e.first_name, e.last_name\n",
    "ORDER BY total_hours_worked\n",
    "LIMIT 1;\n",
    "\n",
    "##\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out with the same prompt as before. After running this cell, we can see that these results match [our database schema](#schema) more and could be executed on the database directly. \n",
    "As a reminder, our prompt from earlier looked like this:\n",
    "```python\n",
    "TEMPLATE = \"\"\"Given an input question, create a syntactically correct {dialect} query to run.\n",
    "Use the following format:\n",
    "\n",
    "Question: \"Question here\"\n",
    "SQLQuery:\n",
    "\"SQL Query to run\"\n",
    "\n",
    "Only use the following tables:\n",
    "\n",
    "{table_info}.\n",
    "\n",
    "Some examples of SQL queries that correspond to questions are:\n",
    "\n",
    "{few_shot_examples}\n",
    "\n",
    "Question: {input}\n",
    "SQLQuery:\n",
    "\"\"\"\n",
    "\n",
    "CUSTOM_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"input\", \"few_shot_examples\", \"table_info\", \"dialect\"], template=TEMPLATE\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input=\"Who are the SDEs?\"\n",
    "\n",
    "instruction = CUSTOM_PROMPT.format(\n",
    "    input=input,\n",
    "    table_info=table_info,\n",
    "    dialect=\"PostgreSQL\",\n",
    "    few_shot_examples=few_shot_examples\n",
    ")\n",
    "\n",
    "payload = {\n",
    "   \"inputs\": instruction,\n",
    "   \"parameters\": {\"max_new_tokens\": 20, \"temperature\": 0.2, \"top_p\": 0.9}\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes=\"accept_eula=true\")\n",
    "print(response[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now seen and validated that this works, as a next step we want to make this more scalable and manageable. Let's try to automate this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the database with Langchain\n",
    "\n",
    "Now that we have our model deployed, let us automate the prompting process, eliminating the necessity of hardcoding the table information. For this let's create a database. To follow best practice and simplicity, we will create a database locally next to this notebook to try out the prompting process instead of connecting to an RDS instance. Later, when we create the frontend and deploy the architecture, we will interact with our database cluster on RDS directly. Let's create a database locally and populate it with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!chmod +x setup_db_debian.sh && ./setup_db_debian.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the connection works properly, by executing an SQL statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!psql -d \"companydb\" -c \"SELECT * FROM employees;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now connect to the database using LangChains [SQLDatabase]() module. Here we have the option to specify which tables to include and how many sample rows we want in our table information. Now running this cell should leave you with exacly the same table info as [before](#3-interact-with-the-model-prompting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import SQLDatabase\n",
    "\n",
    "RDS_PORT = 5432\n",
    "RDS_DB_NAME = \"companydb\" \n",
    "RDS_URI = f\"postgresql+psycopg2://:@:{RDS_PORT}/{RDS_DB_NAME}\"\n",
    "   \n",
    "db = SQLDatabase.from_uri(RDS_URI,\n",
    "                           include_tables=[\"employees\", \"projects\", \"timelog\"],\n",
    "                           sample_rows_in_table_info=4)\n",
    "print(db.table_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us check if the connection to our database through the LangChain module works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check if the connection works.\n",
    "db.run(\"SELECT * from employees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate the prompting process using Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a database connection in place using Langchain, we will automate the prompting process and avoid having to hardcode the table info like we did before, but rather fetch it dynamically. First we define a ContentHandler class to encode and decode the communication with the model, as it has been shown [here](https://python.langchain.com/docs/integrations/llms/sagemaker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json \n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint\n",
    "from langchain.chains import create_sql_query_chain\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "        input_str = json.dumps(\n",
    "            {\"inputs\" : prompt,\n",
    "            \"parameters\" : {**model_kwargs}})\n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"generated_text\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wrap the communication with the model with LangChain as well and recieve it as a python object using LangChains `SagemakerEndpoint` class. This will make the communication with our SageMaker endpoint easier. As arguments we have to feed it our endpoint_name, region, the content handler, which we just defined and the aformentioned hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wrap the communication with the model with Langchain as well\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "parameters = {\"max_new_tokens\": 100, \"temperature\": 0.2, \"top_p\": 0.9}\n",
    "\n",
    "llm_codellama = SagemakerEndpoint(\n",
    "    endpoint_name=endpoint_name,\n",
    "    region_name=region,\n",
    "    model_kwargs=parameters,\n",
    "    endpoint_kwargs={\"CustomAttributes\": 'accept_eula=true'},\n",
    "    content_handler=content_handler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's run our query end to end using LangChain's `create_sql_query_chain` and format our answer the way we need it. For more information on `create_sql_query_chain` you can have a look at the [documentation](https://api.python.langchain.com/en/latest/chains/langchain.chains.sql_database.query.create_sql_query_chain.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input = \"Who are the SDEs?\"\n",
    "prompt = CUSTOM_PROMPT.format(\n",
    "    input=input,\n",
    "    table_info=db.table_info,\n",
    "    dialect=\"PostgreSQL\",\n",
    "    few_shot_examples=few_shot_examples\n",
    ")\n",
    "\n",
    "chain = create_sql_query_chain(llm_codellama, db) \n",
    "result = chain.invoke({\"question\": prompt})\n",
    "\n",
    "# We cut off everything after our SQL statement and add a semicolon if it wasn't already there.\n",
    "result = result.split(\";\")[0] + \";\"\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run this query now directly against our database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this code directly on the db\n",
    "import os\n",
    "os.system(f'psql -d companydb -c \"{result}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Frontend and deploy architecture\n",
    "\n",
    "Now that we have our model deployed on an endpoint for real-time inference, and learned how to query a Postgresql Database using Natural Language, let's deploy the rest of the architecture. For this we use AWS CloudFormation, which lets you model, provision, and manage AWS and third-party resources by treating infrastructure as code. For more information on Cloudformation and its features, have a look [here](https://aws.amazon.com/cloudformation/).\n",
    "\n",
    "<span style=\"color:red\"> *‚ö†Ô∏èWarning: We give Elastic Beanstalk full SageMaker access, which is over permissive and should not be used in production*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# define bucket and zip file\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket() # Set default S3 bucket\n",
    "zip_file = \"myapp.zip\"\n",
    "rds_master_username = \"dbadmin\"\n",
    "rds_master_password = \"12345678\" # change password here\n",
    "\n",
    "# deploy the architecture\n",
    "!chmod +x deploy_cf.sh\n",
    "os.system(f\"./deploy_cf.sh {bucket} {zip_file} {rds_master_username} {rds_master_password}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us wait for the Cloudformation stack to finish deploying, before we continue interacting with it. This should take about 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws cloudformation wait stack-create-complete --stack-name text2sql-app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add our database credentials to AWS Systems Manager Parameter Store, so our application can access our database securely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_client = boto3.client(\"rds\")\n",
    "for cluster in rds_client.describe_db_clusters()['DBClusters']:\n",
    "    if 'DatabaseName' in cluster.keys() and cluster['DatabaseName'] == 'CompanyDatabase':\n",
    "        rds_endpoint = cluster['Endpoint'] # get RDS endpoint\n",
    "\n",
    "assert rds_endpoint is not None\n",
    "\n",
    "# Upload Database credentials to Parameterstore\n",
    "ssm_client = boto3.client(\"ssm\")\n",
    "ssm_client.put_parameter(\n",
    "    Name=\"text2sql_db_endpoint\",\n",
    "    Type=\"SecureString\",\n",
    "    Value=rds_endpoint,\n",
    "    Overwrite=True\n",
    ")\n",
    "ssm_client.put_parameter(\n",
    "    Name=\"text2sql_db_user\",\n",
    "    Type=\"SecureString\",\n",
    "    Value=rds_master_username,\n",
    "    Overwrite=True\n",
    "\n",
    ")\n",
    "ssm_client.put_parameter(\n",
    "    Name=\"text2sql_db_password\",\n",
    "    Type=\"SecureString\",\n",
    "    Value=rds_master_password,\n",
    "    Overwrite=True\n",
    ")\n",
    "\n",
    "# Print the IP address of the deployed demo!\n",
    "cf_client = boto3.client('cloudformation')\n",
    "stackname = 'text2sql-app'\n",
    "\n",
    "response = cf_client.describe_stacks(StackName=stackname)\n",
    "outputs = response[\"Stacks\"][0][\"Outputs\"]\n",
    "for output in outputs:\n",
    "    keyName = output[\"OutputKey\"]\n",
    "    if keyName == \"StreamlitURL\":\n",
    "        print(f\"Paste the following IP address in your browser to view your application: {output['OutputValue']} ü§ñ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Congratulations! You have successfully built and deployed a Natural Language to SQL application on AWS! üéâ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teardown\n",
    "\n",
    "To avoid being charged, let us delete the resources we created throughout this notebook. This includes the SageMaker Endpoint, SageMaker Endpoint Configuration, Parameters and the model itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the endpoint and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Cloudformation stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws cloudformation delete-stack --stack-name text2sql-app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Delete Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws ssm delete-parameters --names \"text2sql_db_endpoint\" \"text2sql_db_user\" \"text2sql_db_password\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we saw a simple workflow on how to build a Natural Language to SQL demo leveraging the code generation capabilities of LLMs using Amazon SageMaker Jumpstart. Later we deployed the entire containerized application through AWS Elastic Beanstalk and built a frontend around it using the open source library Streamlit. \n",
    "We have the following learnings:\n",
    "* We deployed the Code Llama 7B model using SageMaker Jumpstart\n",
    "* We learned to interact with our model using the SageMaker SDK\n",
    "* We used LangChains `PromptTemplate` module to define our prompt template, which includes table information and few shot examples.\n",
    "* We explored LangChains `create_sql_query_chain` module, to generate an SQL query\n",
    "* We connected all these components including the execution of the query on our database.\n",
    "\n",
    "You can adapt this notebook and also the application to your data by connecting it to your database and using few-shot examples specific to your data. This notebook contains static few-shot examples. For a larger corpus of data, it might make sense to consider [including dynamic few-shot examples](https://python.langchain.com/docs/use_cases/qa_structured/sql#including-dynamic-few-shot-examples). Furthermore, the table information can also be dynamic for every question, eliminating the need of including all the tables. For this, you can take a look at [Retrieval Augmented Generation (RAG)](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html)\n",
    "\n",
    "While this notebook contains a simple workflow on how to build a Natural Language to SQL demo, for a deeper dive, please refer to [this](https://arxiv.org/pdf/2308.15363.pdf) benchmark evaluation on this use case, including Supervised Fine Tuning for Text-to-SQL. If you want to explore how to build applications around our use case and operationalize everything, see [here](https://aws.amazon.com/blogs/machine-learning/fmops-llmops-operationalize-generative-ai-and-differences-with-mlops/).\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
