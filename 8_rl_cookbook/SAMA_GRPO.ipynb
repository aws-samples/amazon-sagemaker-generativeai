{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAMA RL: GRPO Training Guide\n",
    "\n",
    "**A complete guide to training language models with Group Relative Policy Optimization (GRPO)**\n",
    "\n",
    "## What You'll Learn\n",
    "- How to create reward functions that guide model behavior\n",
    "- How to train models with GRPO on SageMaker\n",
    "- How to deploy and run inference on trained models\n",
    "- How to monitor and optimize training jobs\n",
    "\n",
    "## Overview\n",
    "Training an AI model with reinforcement learning:\n",
    "- **Model generates response** → AI creates text output\n",
    "- **Reward function scores response** → Function evaluates quality  \n",
    "- **Model learns from rewards** → AI improves based on scores\n",
    "- **GRPO** → The training algorithm that optimizes the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Setup & Imports](#setup)\n",
    "2. [Understanding Reward Functions](#rewards)\n",
    "3. [GRPO Training](#training)\n",
    "4. [Model Deployment](#deployment)\n",
    "5. [Inference](#inference)\n",
    "6. [Advanced Usage](#advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SAMA RL\n",
    "# pip install -e .\n",
    "\n",
    "# Core SAMA RL imports\n",
    "from sama_rl import GRPO, create_inference_model\n",
    "\n",
    "# Standard libraries\n",
    "from typing import List\n",
    "import time\n",
    "\n",
    "print(\"SAMA RL imported successfully\")\n",
    "print(\"Ready to start training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Reward Functions\n",
    "\n",
    "**Goal**: Teach the model what \"good\" responses look like\n",
    "\n",
    "### Anatomy of a Reward Function\n",
    "```python\n",
    "def my_reward_function(completions: List[str], **kwargs) -> List[float]:\n",
    "    # completions = list of model responses\n",
    "    # kwargs = extra info (tokenizer, etc.)\n",
    "    # returns = list of reward scores\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length-Based Reward Function\n",
    "\n",
    "**Goal**: Train model to write responses of a specific length  \n",
    "**Use Cases**: Summaries, tweets, product descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_length_reward(target_length: int = 400):\n",
    "    \"\"\"\n",
    "    Creates reward function targeting specific length\n",
    "    \n",
    "    How it works:\n",
    "    - Counts tokens in response\n",
    "    - Gives higher reward for responses closer to target\n",
    "    - Uses quadratic penalty for distance from target\n",
    "    \"\"\"\n",
    "    def length_reward(completions: List[str], **kwargs) -> List[float]:\n",
    "        tokenizer = kwargs.get('tokenizer')\n",
    "        rewards = []\n",
    "        \n",
    "        for completion in completions:\n",
    "            # Count tokens (or words as fallback)\n",
    "            if tokenizer:\n",
    "                num_tokens = len(tokenizer.encode(completion, add_special_tokens=False))\n",
    "            else:\n",
    "                num_tokens = len(completion.split())\n",
    "            \n",
    "            # Reward: closer to target = higher score\n",
    "            distance = abs(num_tokens - target_length)\n",
    "            reward = -(distance ** 2) / 1000\n",
    "            rewards.append(reward)\n",
    "        \n",
    "        return rewards\n",
    "    \n",
    "    return length_reward\n",
    "\n",
    "# Create length reward targeting 400 tokens\n",
    "length_400_reward = create_length_reward(target_length=400)\n",
    "\n",
    "print(\"Length reward function created\")\n",
    "print(\"Target: 400 tokens\")\n",
    "print(\"Penalty increases quadratically with distance from target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment-Based Reward Function\n",
    "\n",
    "**Goal**: Train model to write positive, helpful responses  \n",
    "**Use Cases**: Customer service, educational content, friendly chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentiment_reward(positive_weight: float = 1.0, negative_weight: float = -0.5):\n",
    "    \"\"\"\n",
    "    Creates reward function based on sentiment\n",
    "    \n",
    "    How it works:\n",
    "    - Counts positive words (good, great, helpful, etc.)\n",
    "    - Counts negative words (bad, terrible, awful, etc.)\n",
    "    - Rewards positive sentiment, penalizes negative\n",
    "    \"\"\"\n",
    "    positive_words = ['good', 'great', 'excellent', 'amazing', 'helpful', 'useful', 'clear']\n",
    "    negative_words = ['bad', 'terrible', 'awful', 'horrible', 'useless', 'wrong', 'confusing']\n",
    "    \n",
    "    def sentiment_reward(completions: List[str], **kwargs) -> List[float]:\n",
    "        rewards = []\n",
    "        \n",
    "        for completion in completions:\n",
    "            text_lower = completion.lower()\n",
    "            \n",
    "            # Count sentiment words\n",
    "            positive_count = sum(1 for word in positive_words if word in text_lower)\n",
    "            negative_count = sum(1 for word in negative_words if word in text_lower)\n",
    "            \n",
    "            # Calculate sentiment score\n",
    "            reward = (positive_count * positive_weight) + (negative_count * negative_weight)\n",
    "            rewards.append(reward)\n",
    "        \n",
    "        return rewards\n",
    "    \n",
    "    return sentiment_reward\n",
    "\n",
    "# Create sentiment reward favoring positive language\n",
    "positive_sentiment_reward = create_sentiment_reward(positive_weight=1.0, negative_weight=-0.5)\n",
    "\n",
    "print(\"Sentiment reward function created\")\n",
    "print(\"Rewards: good, great, helpful, excellent\")\n",
    "print(\"Penalizes: bad, terrible, awful, horrible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Your Reward Functions\n",
    "\n",
    "Let's see how our reward functions work on sample text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test completions with different characteristics\n",
    "test_completions = [\n",
    "    \"Short response.\",  # Short\n",
    "    \"This is a much longer response that contains detailed information and explanations that should score higher on length-based rewards.\",  # Long\n",
    "    \"This response is great and excellent, providing wonderful insights that are very helpful.\",  # Positive\n",
    "    \"This is a bad and terrible response that is awful and provides useless information.\",  # Negative\n",
    "    \"This is a medium-length response with neutral tone and factual content.\"  # Neutral, medium\n",
    "]\n",
    "\n",
    "print(\"Testing reward functions on sample completions:\")\n",
    "print(f\"Number of test completions: {len(test_completions)}\")\n",
    "\n",
    "# Test length rewards\n",
    "length_rewards = length_400_reward(test_completions)\n",
    "print(\"\\nLength Rewards (target: 400 tokens):\")\n",
    "for i, (completion, reward) in enumerate(zip(test_completions, length_rewards), 1):\n",
    "    word_count = len(completion.split())\n",
    "    print(f\"{i}. Reward: {reward:6.2f} | Words: {word_count:3d} | {completion[:50]}...\")\n",
    "\n",
    "# Test sentiment rewards\n",
    "sentiment_rewards = positive_sentiment_reward(test_completions)\n",
    "print(\"\\nSentiment Rewards:\")\n",
    "for i, (completion, reward) in enumerate(zip(test_completions, sentiment_rewards), 1):\n",
    "    sentiment = \"Positive\" if reward > 0 else \"Negative\" if reward < 0 else \"Neutral\"\n",
    "    print(f\"{i}. Reward: {reward:6.2f} | {sentiment} | {completion[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GRPO Training\n",
    "\n",
    "**Goal**: Train your model with reward functions on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GRPO trainer with length-based reward\n",
    "trainer = GRPO(\n",
    "    yaml_file=\"sama_rl/recipes/GRPO/qwen2-0.5b-grpo-config.yaml\",\n",
    "    reward_functions=[length_400_reward],  # Use our length reward\n",
    "    max_steps=10,  # Small number for testing\n",
    "    wandb_api_key=\"your_wandb_key_here\"  # Replace with your key\n",
    ")\n",
    "\n",
    "print(\"GRPO Trainer Created\")\n",
    "print(f\"Model: {trainer.config.model['name']}\")\n",
    "print(f\"Dataset: {trainer.config.data['dataset_name']}\")\n",
    "print(f\"Max steps: {trainer.config.training['max_steps']}\")\n",
    "\n",
    "# Get training job info\n",
    "model_name = trainer.config.model['name'].split('/')[-1].lower().replace('-', '')\n",
    "timestamp = int(time.time())\n",
    "job_name = f\"sama-grpo-{model_name}-{timestamp}\"\n",
    "print(f\"Training job will be named: {job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training\n",
    "\n",
    "**Warning: This will launch a real SageMaker training job and incur costs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to start actual training\n",
    "# trainer.train()\n",
    "\n",
    "print(\"Training is commented out to prevent accidental costs\")\n",
    "print(\"\\nTo start training:\")\n",
    "print(\"  1. Uncomment the line above\")\n",
    "print(\"  2. Add your real W&B API key\")\n",
    "print(\"  3. Run the cell\")\n",
    "print(\"\\nExpected cost: ~$8-12 for 800 steps on ml.g4dn.2xlarge\")\n",
    "print(\"Expected time: ~30-45 minutes\")\n",
    "\n",
    "print(\"\\nMonitor training:\")\n",
    "print(\"  • SageMaker Console: Training jobs\")\n",
    "print(\"  • W&B Dashboard: Real-time metrics\")\n",
    "print(\"  • CloudWatch: Detailed logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Deployment\n",
    "\n",
    "**Goal**: Deploy your trained model to a SageMaker endpoint for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy from Existing Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing training job and deploy\n",
    "trainer = GRPO(training_job_name=\"sama-grpo-qwen205binstruct-1234567890\")  # Replace with your job name\n",
    "\n",
    "# Deploy with auto-selected instance (based on model size)\n",
    "endpoint_name = trainer.deploy()\n",
    "\n",
    "print(f\"Model deployed to endpoint: {endpoint_name}\")\n",
    "print(\"Deployment complete - ready for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy with Custom Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy with specific instance type\n",
    "# endpoint_name = trainer.deploy(instance_type=\"ml.g5.2xlarge\")\n",
    "\n",
    "print(\"Available GPU instances for deployment:\")\n",
    "print(\"• ml.g5.xlarge - Small models (0.5B-1B) - ~$1.00/hour\")\n",
    "print(\"• ml.g5.2xlarge - Medium models (1B-3B) - ~$1.50/hour\")\n",
    "print(\"• ml.g5.4xlarge - Large models (7B+) - ~$2.50/hour\")\n",
    "print(\"• ml.g5.12xlarge - Very large models (13B+) - ~$7.00/hour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference\n",
    "\n",
    "**Goal**: Run inference on your deployed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference model from deployed endpoint\n",
    "model = create_inference_model(endpoint_name)\n",
    "\n",
    "# Single inference\n",
    "completion = model.generate(\n",
    "    prompt=\"What is machine learning?\",\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Prompt: What is machine learning?\")\n",
    "print(f\"Completion: {completion}\")\n",
    "print(f\"Token count: {model.get_token_count(completion)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple prompts\n",
    "test_prompts = [\n",
    "    \"Explain artificial intelligence in simple terms.\",\n",
    "    \"What are the benefits of renewable energy?\",\n",
    "    \"Describe the process of photosynthesis.\"\n",
    "]\n",
    "\n",
    "print(\"Running batch inference:\")\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    completion = model.generate(\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        stop_on_repetition=True\n",
    "    )\n",
    "    tokens = model.get_token_count(completion)\n",
    "    \n",
    "    print(f\"\\nPrompt {i}: {prompt}\")\n",
    "    print(f\"Completion ({tokens} tokens): {completion[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different temperature settings\n",
    "prompt = \"Write a short story about a robot.\"\n",
    "\n",
    "temperatures = [0.0, 0.7, 1.2]\n",
    "for temp in temperatures:\n",
    "    completion = model.generate(\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=100,\n",
    "        temperature=temp,\n",
    "        stop_on_repetition=True\n",
    "    )\n",
    "    \n",
    "    creativity = \"Deterministic\" if temp == 0.0 else \"Balanced\" if temp < 1.0 else \"Creative\"\n",
    "    print(f\"\\nTemperature {temp} ({creativity}):\")\n",
    "    print(f\"{completion[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Reward Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer with multiple reward functions\n",
    "multi_trainer = GRPO(\n",
    "    yaml_file=\"sama_rl/recipes/GRPO/qwen2-0.5b-grpo-config.yaml\",\n",
    "    reward_functions=[\n",
    "        length_400_reward,           # Target 400 tokens\n",
    "        positive_sentiment_reward    # Positive language\n",
    "    ],\n",
    "    max_steps=10,\n",
    "    wandb_api_key=\"your_key_here\"\n",
    ")\n",
    "\n",
    "print(\"Multi-reward trainer created\")\n",
    "print(\"Reward 1: Length (400 tokens)\")\n",
    "print(\"Reward 2: Positive sentiment\")\n",
    "print(\"Model will optimize for both objectives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Overrides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override configuration parameters at runtime\n",
    "advanced_trainer = GRPO(\n",
    "    yaml_file=\"sama_rl/recipes/GRPO/qwen2-0.5b-grpo-config.yaml\",\n",
    "    reward_functions=[length_400_reward],\n",
    "    \n",
    "    # Override training parameters\n",
    "    max_steps=1000,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    \n",
    "    # Override SageMaker settings\n",
    "    instance_type=\"ml.g4dn.4xlarge\",\n",
    "    \n",
    "    # Override W&B settings\n",
    "    wandb_api_key=\"your_key\"\n",
    ")\n",
    "\n",
    "print(\"Configuration Overrides Applied:\")\n",
    "print(f\"Max steps: {advanced_trainer.config.training['max_steps']}\")\n",
    "print(f\"Learning rate: {advanced_trainer.config.training['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You've Learned\n",
    "- Create custom reward functions for any objective\n",
    "- Train models with GRPO on SageMaker\n",
    "- Deploy models to endpoints for inference\n",
    "- Run inference with various parameters\n",
    "- Override configurations for different use cases\n",
    "\n",
    "### Complete Workflow\n",
    "1. **Define reward function** → `create_length_reward(400)`\n",
    "2. **Configure training** → `GRPO(yaml_file, reward_functions)`\n",
    "3. **Train model** → `trainer.train()`\n",
    "4. **Deploy model** → `trainer.deploy()`\n",
    "5. **Run inference** → `model.generate(prompt)`\n",
    "\n",
    "### Best Practices\n",
    "- Test with small max_steps first (10-50)\n",
    "- Use appropriate GPU instances for deployment\n",
    "- Monitor costs with max_run limits\n",
    "- Use stop_on_repetition for cleaner outputs\n",
    "- Start with smaller models and scale up\n",
    "\n",
    "You now have the tools to train language models with reinforcement learning using SAMA RL."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
