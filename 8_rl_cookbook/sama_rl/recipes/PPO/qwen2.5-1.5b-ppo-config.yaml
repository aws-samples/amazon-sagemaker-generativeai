model:
  name: "Qwen/Qwen2.5-1.5B-Instruct"
  trust_remote_code: true


data:
  dataset_name: "stanfordnlp/imdb"
  train_split: "train[:5000]"
  test_split: "test[:100]"
  input_min_length: 2
  input_max_length: 8

training:
  max_steps: 1200
  learning_rate: 1.0e-5
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 4
  warmup_steps: 120
  weight_decay: 0.005
  
  # Memory optimization for 1.5B model
  fp16: true
  gradient_checkpointing: true
  dataloader_num_workers: 0
  remove_unused_columns: false
  
  # Evaluation and logging
  eval_strategy: "steps"
  eval_steps: 150
  logging_steps: 25
  save_strategy: "steps"
  save_steps: 300
  do_eval: true

ppo:
  # PPO-specific parameters
  ppo_epochs: 4
  mini_batch_size: 1
  cliprange: 0.2
  cliprange_value: 0.2
  vf_coef: 0.1
  
  # Generation parameters
  max_new_tokens: 20
  min_new_tokens: 4
  temperature: 0.6
  top_p: 0.9
  top_k: 40
  do_sample: true
  pad_token_id: 151643  # Qwen2.5 EOS token
  
  # KL penalty
  kl_penalty: "kl"
  target_kl: 0.1
  init_kl_coef: 0.2

reward:
  model_name: "lvwerra/distilbert-imdb"
  reward_type: "sentiment_positive"
  batch_size: 8

wandb:
  project: "qwen2.5-ppo-experiments"
  run_name: "qwen2.5-1.5b-sentiment-ppo"
  tags: ["qwen2.5", "ppo", "sentiment", "1.5b"]
  
output:
  dir: "/opt/ml/model"
  save_total_limit: 2

sagemaker:
  instance_type: "ml.g4dn.8xlarge"
  instance_count: 1
  max_run: 5400
  keep_alive_period: 2700
