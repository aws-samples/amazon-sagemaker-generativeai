model:
  name: "Qwen/Qwen2-0.5B-Instruct"
  trust_remote_code: true

data:
  dataset_name: "trl-lib/tldr"
  train_split: "train[:8000]"
  test_split: "test[:200]"

training:
  max_steps: 800
  learning_rate: 5e-5
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 2
  warmup_steps: 80
  weight_decay: 0.01
  
  # Qwen-optimized settings
  fp16: true
  gradient_checkpointing: true
  dataloader_num_workers: 2
  remove_unused_columns: false
  
  # Evaluation and logging
  eval_strategy: "steps"
  eval_steps: 100
  logging_steps: 20
  save_strategy: "steps"
  save_steps: 200
  do_eval: true

grpo:
  num_generations: 2
  max_completion_length: 768
  max_prompt_length: 256
  
  # Qwen generation parameters
  temperature: 0.7
  top_p: 0.8
  top_k: 50
  repetition_penalty: 1.05
  do_sample: true
  
  # GRPO-specific
  beta: 0.1
  label_smoothing: 0.0

reward:
  target_length: 400
  reward_type: "length_quality_hybrid"

wandb:
  project: "qwen2-grpo-experiments"
  run_name: "qwen2-0.5b-tldr-summarization"
  tags: ["qwen2", "grpo", "summarization", "0.5b"]
  
output:
  dir: "/opt/ml/model"
  save_total_limit: 3

sagemaker:
  instance_type: "ml.p4d.24xlarge"
  instance_count: 1
  max_run: 3600
  keep_alive_period: 1800
