{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "auto",
   "metadata": {},
   "source": [
    "# üîß Model Training for Phishing Detection\n",
    "\n",
    "**Purpose**: Fine-tune Qwen2.5-1.5B for sequence classification on phishing detection.\n",
    "\n",
    "This notebook:\n",
    "- Creates training script with RSLoRA + MLflow\n",
    "- Configures SageMaker PyTorch ModelTrainer\n",
    "- Launches training job on ml.g5.xlarge\n",
    "- Retrieves trained model from S3\n",
    "\n",
    "## Prerequisites\n",
    "- **Run `01_data_processing.ipynb` first**\n",
    "- MLflow app created in SageMaker\n",
    "- Budget: ~$1.50-$2.00 (60-75 mins on ml.g5.xlarge)\n",
    "\n",
    "## Next Steps\n",
    "After training completes ‚Üí `03_model_deployment.ipynb`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a651295",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353f89c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq \"sagemaker==2.253.1\" \"sagemaker-mlflow==0.2.0\" mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b515d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from utils import get_mlflow_app_arn, find_latest_training_job, download_and_extract_model, upload_directory_to_s3, cleanup_local_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b5ee3d",
   "metadata": {},
   "source": [
    "## 2. Load Variables from Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aedac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r train_s3_uri\n",
    "%store -r val_s3_uri\n",
    "%store -r test_s3_uri\n",
    "%store -r NUM_LABELS\n",
    "%store -r LABEL_NAMES\n",
    "%store -r region\n",
    "%store -r role\n",
    "%store -r sagemaker_session_bucket\n",
    "\n",
    "# Verify\n",
    "try:\n",
    "    print(\"‚úÖ Variables loaded:\")\n",
    "    print(f\"  Train: {train_s3_uri}\")\n",
    "    print(f\"  Val: {val_s3_uri}\")\n",
    "    print(f\"  Test: {test_s3_uri}\")\n",
    "except NameError:\n",
    "    print(\"‚ùå Run 01_data_processing.ipynb first!\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a666bea",
   "metadata": {},
   "source": [
    "## 3. SageMaker Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfe6eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session(boto3.Session(region_name=region))\n",
    "\n",
    "print(f\"SageMaker role: {role}\")\n",
    "print(f\"SageMaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"Region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ba5f50",
   "metadata": {},
   "source": [
    "## 4. MLflow Configuration\n",
    "\n",
    "Auto-detect MLflow app. If none found, create one in SageMaker Console ‚Üí MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef5e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_APP_ARN = get_mlflow_app_arn(region)\n",
    "print(f\"‚úÖ MLflow ARN: {MLFLOW_APP_ARN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af73206",
   "metadata": {},
   "source": [
    "## 5. Create Training Code Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e1b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('sagemaker_code', exist_ok=True)\n",
    "print(\"‚úÖ Created sagemaker_code/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f21616",
   "metadata": {},
   "source": [
    "### 5.1 Requirements File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-dir",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('sagemaker_code', exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Created sagemaker_code/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aac9006-60dc-456c-9017-70f1fb10504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sagemaker_code/requirements.txt\n",
    "transformers==4.55.0\n",
    "torch>=2.1.0\n",
    "accelerate==1.10.0\n",
    "peft==0.17.0\n",
    "datasets==4.0.0\n",
    "scikit-learn==1.7.1\n",
    "mlflow\n",
    "sagemaker-mlflow==0.2.0\n",
    "sentencepiece==0.2.0\n",
    "safetensors>=0.6.2\n",
    "evaluate==0.4.5\n",
    "psutil\n",
    "nvidia-ml-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f885f0af",
   "metadata": {},
   "source": [
    "### 5.2 Training Script\n",
    "\n",
    "This training script demonstrates:\n",
    "- **Sequence classification** for binary phishing detection\n",
    "- **RSLoRA** (rank-stabilized LoRA) for efficient fine-tuning\n",
    "- **MLflow integration** for metric tracking\n",
    "- **Security-focused metrics** (precision, recall, F1, FPR, FNR)\n",
    "\n",
    "Read through this code to understand how the model is configured and trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd15946-45d7-40ac-a96b-d5c160411835",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sagemaker_code/train.py\n",
    "\"\"\"\n",
    "SageMaker Training Script for Qwen2.5-1.5B Phishing Detection\n",
    "Binary Classification: Safe (0) vs Phishing (1)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import sagemaker\n",
    "import mlflow\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Model parameters\n",
    "    parser.add_argument(\"--model_id\", type=str, default=\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "    parser.add_argument(\"--num_labels\", type=int, default=2)\n",
    "    parser.add_argument(\"--max_length\", type=int, default=512)\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1)\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=8)\n",
    "    parser.add_argument(\"--eval_batch_size\", type=int, default=8)\n",
    "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=4)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=2e-4)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0.01)\n",
    "    parser.add_argument(\"--warmup_ratio\", type=float, default=0.03)\n",
    "    \n",
    "    # LoRA parameters\n",
    "    parser.add_argument(\"--lora_r\", type=int, default=16)\n",
    "    parser.add_argument(\"--lora_alpha\", type=int, default=32)\n",
    "    parser.add_argument(\"--lora_dropout\", type=float, default=0.05)\n",
    "    parser.add_argument(\"--use_rslora\", action=\"store_true\")\n",
    "    parser.add_argument(\"--use_dora\", action=\"store_true\")\n",
    "    \n",
    "    # SageMaker specific\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\", \"/opt/ml/model\"))\n",
    "    parser.add_argument(\"--train_dir\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAINING\"))\n",
    "    parser.add_argument(\"--validation_dir\", type=str, default=os.environ.get(\"SM_CHANNEL_VALIDATION\"))\n",
    "    parser.add_argument(\"--test_dir\", type=str, default=os.environ.get(\"SM_CHANNEL_TEST\"))\n",
    "    parser.add_argument(\"--output_data_dir\", type=str, default=os.environ.get(\"SM_OUTPUT_DATA_DIR\", \"/opt/ml/output/data\"))\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def load_datasets(args):\n",
    "    \"\"\"Load train, validation, and test datasets from JSONL files.\"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    for split_name, split_dir in [\n",
    "        ('train', args.train_dir),\n",
    "        ('validation', args.validation_dir),\n",
    "        ('test', args.test_dir)\n",
    "    ]:\n",
    "        files = [os.path.join(split_dir, f) for f in os.listdir(split_dir) if f.endswith('.jsonl')]\n",
    "        datasets[split_name] = load_dataset('json', data_files=files, split='train')\n",
    "    \n",
    "    print(f\"Loaded datasets:\")\n",
    "    print(f\"  Train: {len(datasets['train'])} samples\")\n",
    "    print(f\"  Validation: {len(datasets['validation'])} samples\")\n",
    "    print(f\"  Test: {len(datasets['test'])} samples\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "\n",
    "def setup_model_and_tokenizer(args):\n",
    "    \"\"\"Initialize model and tokenizer with LoRA configuration.\"\"\"\n",
    "    print(f\"\\nSetting up model: {args.model_id}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.model_id,\n",
    "        add_prefix_space=True,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    # Qwen2.5 uses <|endoftext|> as pad token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        args.model_id,\n",
    "        num_labels=args.num_labels,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "    \n",
    "    # LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=args.lora_r,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_CLS,   # Configuring the task as sequence classifier\n",
    "        inference_mode=False,\n",
    "        use_rslora=args.use_rslora,\n",
    "        use_dora=args.use_dora,\n",
    "    )\n",
    "    \n",
    "    # Enable gradient checkpointing and apply LoRA\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def preprocess_function(examples, tokenizer, max_length):\n",
    "    \"\"\"Tokenize text inputs.\"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False,\n",
    "    )\n",
    "    tokenized['labels'] = examples['label']\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Calculate security-focused metrics for phishing detection.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'balanced_accuracy': balanced_accuracy_score(labels, predictions),\n",
    "        'precision': precision_score(labels, predictions, average='binary', pos_label=1),\n",
    "        'recall': recall_score(labels, predictions, average='binary', pos_label=1),\n",
    "        'f1': f1_score(labels, predictions, average='binary', pos_label=1),\n",
    "        'f1_macro': f1_score(labels, predictions, average='macro'),\n",
    "        'f1_weighted': f1_score(labels, predictions, average='weighted'),\n",
    "    }\n",
    "\n",
    "\n",
    "def print_training_summary(train_result):\n",
    "    \"\"\"Print training completion summary.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING COMPLETED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Training time: {train_result.metrics['train_runtime']:.2f}s\")\n",
    "    print(f\"Training samples/second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print(f\"Final training loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "\n",
    "\n",
    "def evaluate_and_save_results(trainer, tokenized_test_dataset, args):\n",
    "    \"\"\"Evaluate model on test set, calculate metrics, and save results.\"\"\"\n",
    "    print(\"\\n7. Evaluating on test set...\")\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = trainer.predict(tokenized_test_dataset)\n",
    "    pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "    true_labels = predictions.label_ids\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_results = {\n",
    "        'eval_accuracy': float(accuracy_score(true_labels, pred_labels)),\n",
    "        'eval_balanced_accuracy': float(balanced_accuracy_score(true_labels, pred_labels)),\n",
    "        'eval_precision': float(precision_score(true_labels, pred_labels, average='binary', pos_label=1)),\n",
    "        'eval_recall': float(recall_score(true_labels, pred_labels, average='binary', pos_label=1)),\n",
    "        'eval_f1': float(f1_score(true_labels, pred_labels, average='binary', pos_label=1)),\n",
    "        'eval_f1_macro': float(f1_score(true_labels, pred_labels, average='macro')),\n",
    "        'eval_f1_weighted': float(f1_score(true_labels, pred_labels, average='weighted')),\n",
    "    }\n",
    "    \n",
    "    # Calculate FPR and FNR using confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(true_labels, pred_labels).ravel()\n",
    "    test_results['false_positive_rate'] = float(fp / (fp + tn))\n",
    "    test_results['false_negative_rate'] = float(fn / (fn + tp))\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nüîí Security Metrics (Test Set):\")\n",
    "    print(f\"  Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "    print(f\"  Balanced Accuracy: {test_results['eval_balanced_accuracy']:.4f}\")\n",
    "    print(f\"  Precision (Dangerous): {test_results['eval_precision']:.4f}\")\n",
    "    print(f\"  Recall (Dangerous): {test_results['eval_recall']:.4f}\")\n",
    "    print(f\"  F1 Score (Dangerous): {test_results['eval_f1']:.4f}\")\n",
    "    print(f\"  F1 (Macro): {test_results['eval_f1_macro']:.4f}\")\n",
    "    print(f\"  F1 (Weighted): {test_results['eval_f1_weighted']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  Error Analysis:\")\n",
    "    print(f\"  False Positive Rate: {test_results['false_positive_rate']:.4f} (Safe flagged as Dangerous)\")\n",
    "    print(f\"  False Negative Rate: {test_results['false_negative_rate']:.4f} (Dangerous missed)\")\n",
    "    \n",
    "    # Log to MLflow\n",
    "    mlflow.log_metrics(test_results)\n",
    "    \n",
    "    # Save results\n",
    "    with open(os.path.join(args.output_data_dir, 'test_results.json'), 'w') as f:\n",
    "        json.dump(test_results, f, indent=2)\n",
    "    \n",
    "    # Save classification report\n",
    "    report = classification_report(\n",
    "        true_labels,\n",
    "        pred_labels,\n",
    "        target_names=['Safe', 'Phishing'],\n",
    "        digits=4\n",
    "    )\n",
    "    \n",
    "    with open(os.path.join(args.output_data_dir, 'classification_report.txt'), 'w') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(f\"\\nüìä Classification Report:\\n{report}\")\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "\n",
    "def save_models(model, tokenizer, model_dir):\n",
    "    \"\"\"Save merged model only.\"\"\"\n",
    "    print(\"\\n8. Saving merged model...\")\n",
    "    \n",
    "    try:\n",
    "        merged_model = model.merge_and_unload()\n",
    "        merged_model.save_pretrained(model_dir, safe_serialization=True)\n",
    "        tokenizer.save_pretrained(model_dir)\n",
    "        \n",
    "        print(f\"‚úÖ Merged model saved to: {model_dir}\")\n",
    "        print(f\"   Files: model.safetensors, config.json, tokenizer files\")\n",
    "        \n",
    "        return model_dir\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not merge adapters: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"LLAMA-3.2-1B PHISHING DETECTION - SAGEMAKER TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Task: Binary Classification (Safe vs Dangerous)\")\n",
    "\n",
    "    # Start MLflow run\n",
    "    run_name = sagemaker.utils.name_from_base(f\"phishing-{args.model_id.split('/')[-1]}-lr{args.learning_rate}-r{args.lora_r}\")\n",
    "    mlflow.start_run(run_name=run_name)\n",
    "    \n",
    "    # Load datasets\n",
    "    print(\"\\n1. Loading datasets...\")\n",
    "    datasets = load_datasets(args)\n",
    "    \n",
    "    # Setup model and tokenizer\n",
    "    print(\"\\n2. Setting up model and tokenizer...\")\n",
    "    model, tokenizer = setup_model_and_tokenizer(args)\n",
    "    \n",
    "    # Tokenize datasets (removes 'text' column, keeps only tokenized inputs and 'label')\n",
    "    print(\"\\n3. Tokenizing datasets...\")\n",
    "    tokenized_datasets = {\n",
    "        split: datasets[split].map(\n",
    "            lambda x: preprocess_function(x, tokenizer, args.max_length),\n",
    "            batched=True,\n",
    "            remove_columns=['text'],\n",
    "            desc=f\"Tokenizing {split}\"\n",
    "        )\n",
    "        for split in datasets.keys()\n",
    "    }\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "    \n",
    "    # Training arguments\n",
    "    print(\"\\n4. Setting up training arguments...\")\n",
    "    training_args = TrainingArguments(\n",
    "        # Output\n",
    "        output_dir=args.model_dir,\n",
    "        \n",
    "        # Training schedule\n",
    "        num_train_epochs=args.epochs,\n",
    "        per_device_train_batch_size=args.train_batch_size,\n",
    "        per_device_eval_batch_size=args.eval_batch_size,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        \n",
    "        # Optimization\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.weight_decay,\n",
    "        optim=\"adamw_torch_fused\",\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.999,\n",
    "        \n",
    "        # Learning rate schedule\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=args.warmup_ratio,\n",
    "        \n",
    "        # Evaluation\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_f1\",\n",
    "        \n",
    "        # Logging\n",
    "        logging_dir=f\"{args.model_dir}/logs\",\n",
    "        logging_steps=10,\n",
    "        logging_first_step=True,\n",
    "        \n",
    "        # Performance\n",
    "        fp16=False,\n",
    "        bf16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        \n",
    "        # Reproducibility\n",
    "        seed=42,\n",
    "        data_seed=42,\n",
    "    )\n",
    "    \n",
    "    # Create Trainer\n",
    "    print(\"\\n5. Creating Trainer...\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets['train'],\n",
    "        eval_dataset=tokenized_datasets['validation'],\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\n6. Starting training...\")\n",
    "    print(\"=\"*80)\n",
    "    train_result = trainer.train()\n",
    "    print_training_summary(train_result)\n",
    "    \n",
    "    # Evaluate and save results\n",
    "    test_results = evaluate_and_save_results(trainer, tokenized_datasets['test'], args)\n",
    "    \n",
    "    # Save models\n",
    "    merged_model_dir = save_models(model, tokenizer, args.model_dir)\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\nüìÅ Model Artifacts:\")\n",
    "    print(f\"   Merged model: {args.model_dir}/\")\n",
    "\n",
    "    mlflow.end_run()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéâ TRAINING JOB COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "launch-script",
   "metadata": {},
   "source": [
    "### 5.3 Launch Script\n",
    "\n",
    "Bash script to install dependencies and run training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "write-launch-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sagemaker_code/launch_train.sh\n",
    "#!/bin/bash\n",
    "set -e\n",
    "\n",
    "echo \"Installing dependencies...\"\n",
    "pip install -q -r requirements.txt\n",
    "\n",
    "echo \"Starting phishing detection training...\"\n",
    "python train.py \\\n",
    "    --model_id \"Qwen/Qwen2.5-1.5B-Instruct\" \\\n",
    "    --num_labels 2 \\\n",
    "    --max_length 512 \\\n",
    "    --epochs 1 \\\n",
    "    --train_batch_size 8 \\\n",
    "    --eval_batch_size 8 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lora_r 16 \\\n",
    "    --lora_alpha 32 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --use_rslora \\\n",
    "    \"$@\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chmod-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make script executable\n",
    "!chmod +x sagemaker_code/launch_train.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78751436",
   "metadata": {},
   "source": [
    "## 6. Configure Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.modules.configs import Compute, OutputDataConfig, SourceCode, StoppingCondition, InputData\n",
    "from sagemaker.modules.train import ModelTrainer\n",
    "\n",
    "# Configuration\n",
    "MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "job_name = sagemaker.utils.name_from_base(\"qwen2p5-1p5b-phishing-detection\")\n",
    "training_instance_type = \"ml.g5.xlarge\"\n",
    "training_instance_count = 1\n",
    "\n",
    "print(f\"üîí Training Job Configuration:\")\n",
    "print(f\"  Job name: {job_name}\")\n",
    "print(f\"  Instance: {training_instance_type}\")\n",
    "print(f\"  Model: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acbd91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow environment\n",
    "training_env = {\n",
    "    \"MLFLOW_EXPERIMENT_NAME\": f\"{job_name}-exp\",\n",
    "    \"MLFLOW_TAGS\": json.dumps({\n",
    "        \"source.job\": \"sm-training-jobs\",\n",
    "        \"source.type\": \"phishing-detection\",\n",
    "        \"model\": \"qwen2.5-1.5b\",\n",
    "    }),\n",
    "    \"MLFLOW_TRACKING_URI\": MLFLOW_APP_ARN,\n",
    "    \"MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING\": \"true\",\n",
    "}\n",
    "\n",
    "print(\"‚úÖ MLflow environment configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc94adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PyTorch training container\n",
    "pytorch_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    version=\"2.7.1\",\n",
    "    instance_type=training_instance_type,\n",
    "    image_scope=\"training\",\n",
    ")\n",
    "\n",
    "print(f\"Using image: {pytorch_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9646593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure ModelTrainer\n",
    "source_code = SourceCode(\n",
    "    source_dir=\"./sagemaker_code\",\n",
    "    command=\"bash launch_train.sh\",\n",
    ")\n",
    "\n",
    "compute_configs = Compute(\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=training_instance_count,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    volume_size_in_gb=100,\n",
    ")\n",
    "\n",
    "output_path = f\"s3://{sess.default_bucket()}/phishing-detection/models/{job_name}\"\n",
    "\n",
    "model_trainer = ModelTrainer(\n",
    "    training_image=pytorch_image_uri,\n",
    "    source_code=source_code,\n",
    "    base_job_name=job_name,\n",
    "    compute=compute_configs,\n",
    "    stopping_condition=StoppingCondition(max_runtime_in_seconds=7200),\n",
    "    output_data_config=OutputDataConfig(s3_output_path=output_path),\n",
    "    role=role,\n",
    "    environment=training_env,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ ModelTrainer configured\")\n",
    "print(f\"Output: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08b4225",
   "metadata": {},
   "source": [
    "## 7. Launch Training Job\n",
    "\n",
    "This will start a remote training job. Expected results:\n",
    "- **Accuracy**: ~99%\n",
    "- **F1 Score**: ~99%\n",
    "- **Training time**: 60-75 minutes\n",
    "- **Cost**: \\$2.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3d59d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Launching SageMaker Training Job...\")\n",
    "print(f\"\\nMonitor at: https://console.aws.amazon.com/sagemaker/home?region={region}#/jobs\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "model_trainer.train(\n",
    "    input_data_config=[\n",
    "        InputData(channel_name=\"training\", data_source=train_s3_uri),\n",
    "        InputData(channel_name=\"validation\", data_source=val_s3_uri),\n",
    "        InputData(channel_name=\"test\", data_source=test_s3_uri),\n",
    "    ],\n",
    "    wait=False,  # Set to True to wait for completion\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training job submitted!\")\n",
    "print(\"\\nüí° Next: Monitor in SageMaker Console and view metrics in MLflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab60497",
   "metadata": {},
   "source": [
    "## 8. Monitor Training Progress\n",
    "\n",
    "### View Training Metrics in MLflow\n",
    "\n",
    "While training runs, you can monitor progress in real-time:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3558da9-167e-4131-af22-27c99812f9ab",
   "metadata": {},
   "source": [
    "**MLflow Experiments Dashboard**\n",
    "- Navigate to SageMaker Console ‚Üí MLflow\n",
    "- View list of experiments and runs\n",
    "- See training job metadata and status\n",
    "\n",
    "![List of runs](./images/mlflow-view01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb699326-3c4c-442a-a4c0-96428b7a41b3",
   "metadata": {},
   "source": [
    "**MLflow Run Details**\n",
    "- Click on your training run\n",
    "- View loss curves over training steps\n",
    "- See evaluation metrics (accuracy, F1, precision, recall)\n",
    "- Monitor system metrics (GPU utilization, memory)\n",
    "\n",
    "![Run details](./images/mlflow-view02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f21c9b-1be5-466a-8d92-6889253c1b52",
   "metadata": {},
   "source": [
    "**MLflow Parameters**\n",
    "- View hyperparameters tab\n",
    "- See LoRA configuration (rank, alpha, dropout)\n",
    "- Check training arguments (learning rate, batch size, epochs\n",
    "\n",
    "![Hyperparameters](./images/mlflow-view03.png)\n",
    "\n",
    "Training will take approximately 60-75 minutes. Wait for the job to complete before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748ad39a",
   "metadata": {},
   "source": [
    "## 9. Retrieve Training Results\n",
    "\n",
    "After training completes, retrieve and extract the model artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19a1d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "bucket = sess.default_bucket()\n",
    "base_prefix = f\"phishing-detection/models/{job_name}\"\n",
    "\n",
    "training_job_name, training_job_prefix = find_latest_training_job(\n",
    "    s3_client,\n",
    "    bucket,\n",
    "    base_prefix\n",
    ")\n",
    "\n",
    "print(f\"\\nLatest training job: {training_job_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eacdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract model\n",
    "model_tar_key = f\"{training_job_prefix}/output/model.tar.gz\"\n",
    "\n",
    "extract_dir = download_and_extract_model(\n",
    "    s3_client,\n",
    "    bucket,\n",
    "    model_tar_key\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Model extracted to: {extract_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4be94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload uncompressed model back to S3\n",
    "s3_model_prefix = f\"{training_job_prefix}/uncompressed_model\"\n",
    "\n",
    "upload_directory_to_s3(\n",
    "    s3_client,\n",
    "    extract_dir,\n",
    "    bucket,\n",
    "    s3_model_prefix\n",
    ")\n",
    "\n",
    "model_s3_uri = f\"s3://{bucket}/{s3_model_prefix}/\"\n",
    "print(f\"\\nModel available at: {model_s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2bd41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup local files\n",
    "cleanup_local_files(\"model.tar.gz\", extract_dir)\n",
    "\n",
    "print(\"\\n‚úÖ Cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adab2107",
   "metadata": {},
   "source": [
    "## 10. Store Variables for Deployment\n",
    "\n",
    "Save model location for the deployment notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3857b0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store model_s3_uri\n",
    "%store training_job_name\n",
    "%store MLFLOW_APP_ARN\n",
    "\n",
    "mlflow_experiment_name = f\"{job_name}-exp\"\n",
    "%store mlflow_experiment_name\n",
    "\n",
    "print(\"\\n‚úÖ Variables stored:\")\n",
    "print(f\"  Model S3 URI: {model_s3_uri}\")\n",
    "print(f\"  Training job: {training_job_name}\")\n",
    "print(f\"  MLflow experiment: {mlflow_experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7f6177",
   "metadata": {},
   "source": [
    "## ‚úÖ Training Complete!\n",
    "\n",
    "### What We Accomplished:\n",
    "1. ‚úÖ Created training script with RSLoRA + MLflow\n",
    "2. ‚úÖ Configured SageMaker ModelTrainer\n",
    "3. ‚úÖ Launched training job on ml.g5.xlarge\n",
    "4. ‚úÖ Retrieved and extracted trained model\n",
    "5. ‚úÖ Stored model path for deployment\n",
    "\n",
    "### Training Results:\n",
    "- View complete metrics in MLflow dashboard\n",
    "- Expected accuracy: ~99%\n",
    "- Model artifacts saved to S3\n",
    "\n",
    "### Next Steps:\n",
    "**Proceed to `03_model_deployment.ipynb`** to deploy the model as a real-time endpoint.\n",
    "\n",
    "---\n",
    "\n",
    "**Training Time**: ~60-75 minutes  \n",
    "**Training Cost**: ~$1.50-$2.00"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
