{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "auto",
   "metadata": {},
   "source": [
    "# üìä Benchmarking Phishing Detection Endpoint\n",
    "\n",
    "**Purpose**: Evaluate endpoint performance under various load conditions.\n",
    "\n",
    "This notebook:\n",
    "- Tests single inference latency\n",
    "- Runs concurrent load tests (4, 8, 16, 32, 64 clients)\n",
    "- Measures P50/P90 latencies\n",
    "- Plots latency vs. concurrency\n",
    "- Provides cleanup utilities\n",
    "\n",
    "## Prerequisites\n",
    "- **Run `03_model_deployment.ipynb` first**\n",
    "- Active SageMaker endpoint\n",
    "- Test dataset from data processing\n",
    "\n",
    "## Benchmarking Goals\n",
    "- Understand endpoint capacity\n",
    "- Identify optimal concurrency\n",
    "- Measure tail latencies\n",
    "- Validate production readiness\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b110bc08",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c04c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq \"sagemaker==2.253.1\" joblib tqdm matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb92c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from botocore.config import Config\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574e8a7c",
   "metadata": {},
   "source": [
    "## 2. Load Variables from Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26566e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r endpoint_name\n",
    "%store -r model_name\n",
    "%store -r test_s3_uri\n",
    "%store -r region\n",
    "\n",
    "# Verify\n",
    "try:\n",
    "    print(\"‚úÖ Variables loaded:\")\n",
    "    print(f\"  Endpoint: {endpoint_name}\")\n",
    "    print(f\"  Model: {model_name}\")\n",
    "    print(f\"  Test data: {test_s3_uri}\")\n",
    "except NameError:\n",
    "    print(\"‚ùå Run 03_model_deployment.ipynb first!\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ed5754",
   "metadata": {},
   "source": [
    "## 3. Download Test Dataset\n",
    "\n",
    "Download test data from S3 for benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19798677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Download test data\n",
    "os.makedirs('datasets', exist_ok=True)\n",
    "\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "\n",
    "# Parse S3 URI\n",
    "bucket = test_s3_uri.split('/')[2]\n",
    "key = '/'.join(test_s3_uri.split('/')[3:])\n",
    "\n",
    "# Download\n",
    "local_test_file = 'datasets/test.jsonl'\n",
    "s3_client.download_file(bucket, key, local_test_file)\n",
    "\n",
    "# Count lines\n",
    "with open(local_test_file, 'r') as f:\n",
    "    num_test_samples = sum(1 for _ in f)\n",
    "\n",
    "print(f\"‚úÖ Downloaded test dataset: {num_test_samples} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32866765",
   "metadata": {},
   "source": [
    "## 4. Configure Inference Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d0b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_retry_config = Config(retries={'max_attempts': 1})\n",
    "runtime_client = boto3.client(\"sagemaker-runtime\", config=no_retry_config)\n",
    "\n",
    "def invoke_classification_endpoint(ep_name, texts):\n",
    "    \"\"\"\n",
    "    Invoke SageMaker classification endpoint.\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    \n",
    "    payload = {\"inputs\": texts}\n",
    "    \n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        EndpointName=ep_name,\n",
    "        ContentType='application/json',\n",
    "        Body=json.dumps(payload)\n",
    "    )\n",
    "    \n",
    "    return json.loads(response['Body'].read().decode())\n",
    "\n",
    "print(\"‚úÖ Inference client configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56753db0",
   "metadata": {},
   "source": [
    "## 5. Single Inference Latency Test\n",
    "\n",
    "Test latency for a single inference request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80425e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_latency(endpoint_name, test_file_location, num_lines=None):\n",
    "    \"\"\"\n",
    "    Run a single inference benchmark using a random sample from test file.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {'latency' (ms), 'error' (bool), 'result'}\n",
    "    \"\"\"\n",
    "    error = False\n",
    "    result = None\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        if num_lines is None:\n",
    "            with open(test_file_location, 'r') as f:\n",
    "                num_lines = sum(1 for _ in f)\n",
    "        \n",
    "        random_line_num = random.randint(0, num_lines - 1)\n",
    "        \n",
    "        with open(test_file_location, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i == random_line_num:\n",
    "                    data = json.loads(line)\n",
    "                    text = data['text']\n",
    "                    break\n",
    "        \n",
    "        result = invoke_classification_endpoint(endpoint_name, text)\n",
    "        \n",
    "    except Exception as e:\n",
    "        error = True\n",
    "        result = str(e)\n",
    "    \n",
    "    latency = (time.time() - start) * 1000.0  # Convert to ms\n",
    "    \n",
    "    return {'latency': latency, 'error': error, 'result': result}\n",
    "\n",
    "print(\"‚úÖ Latency test function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c2d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single test\n",
    "print(\"Running single inference test...\\n\")\n",
    "\n",
    "test_result = inference_latency(\n",
    "    endpoint_name,\n",
    "    local_test_file,\n",
    "    num_lines=num_test_samples\n",
    ")\n",
    "\n",
    "print(f\"Latency: {test_result['latency']:.2f} ms\")\n",
    "print(f\"Error: {test_result['error']}\")\n",
    "if not test_result['error']:\n",
    "    print(f\"Result: {test_result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0104e7",
   "metadata": {},
   "source": [
    "## 6. Concurrent Load Testing\n",
    "\n",
    "Test endpoint performance under concurrent load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5373e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(\n",
    "    endpoint_name,\n",
    "    test_file_location,\n",
    "    number_of_clients,\n",
    "    number_of_runs,\n",
    "    num_lines=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Run benchmark with concurrent clients.\n",
    "    \n",
    "    Args:\n",
    "        endpoint_name: SageMaker endpoint name\n",
    "        test_file_location: Path to test JSONL file\n",
    "        number_of_clients: Number of parallel clients\n",
    "        number_of_runs: Total number of requests\n",
    "        num_lines: Number of lines in test file (optional)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (p50_latency_ms, p90_latency_ms, mean_latency_ms, error_rate)\n",
    "    \"\"\"\n",
    "    progress_bar = tqdm(\n",
    "        range(number_of_runs),\n",
    "        desc=f\"{number_of_clients} clients\",\n",
    "        position=0,\n",
    "        leave=True\n",
    "    )\n",
    "    \n",
    "    results = Parallel(n_jobs=number_of_clients, prefer=\"threads\")(\n",
    "        delayed(inference_latency)(endpoint_name, test_file_location, num_lines)\n",
    "        for _ in progress_bar\n",
    "    )\n",
    "    \n",
    "    latencies = [res['latency'] for res in results if not res['error']]\n",
    "    errors = sum(1 for res in results if res['error'])\n",
    "    \n",
    "    if len(latencies) == 0:\n",
    "        return None, None, None, 1.0\n",
    "    \n",
    "    p50_latency_ms = float(np.quantile(latencies, 0.50))\n",
    "    p90_latency_ms = float(np.quantile(latencies, 0.90))\n",
    "    mean_latency_ms = float(np.mean(latencies))\n",
    "    error_rate = errors / len(results)\n",
    "    \n",
    "    return p50_latency_ms, p90_latency_ms, mean_latency_ms, error_rate\n",
    "\n",
    "print(\"‚úÖ Benchmark function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212e8cec",
   "metadata": {},
   "source": [
    "## 7. Run Benchmarks Across Concurrency Levels\n",
    "\n",
    "Test with 4, 8, 16, 32, and 64 concurrent clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffdfdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark configuration\n",
    "concurrency_levels = [4, 8, 16, 32, 64]\n",
    "num_requests_per_level = 512  # Total requests per concurrency level\n",
    "\n",
    "# Storage for results\n",
    "benchmark_results = {\n",
    "    'concurrency': [],\n",
    "    'p50_latency': [],\n",
    "    'p90_latency': [],\n",
    "    'mean_latency': [],\n",
    "    'error_rate': []\n",
    "}\n",
    "\n",
    "print(\"üöÄ Starting concurrent load tests...\\n\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Concurrency levels: {concurrency_levels}\")\n",
    "print(f\"  Requests per level: {num_requests_per_level}\")\n",
    "print(f\"  Test samples: {num_test_samples}\\n\")\n",
    "\n",
    "for num_clients in concurrency_levels:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing with {num_clients} concurrent clients\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    p50, p90, mean, error_rate = run_benchmark(\n",
    "        endpoint_name=endpoint_name,\n",
    "        test_file_location=local_test_file,\n",
    "        number_of_clients=num_clients,\n",
    "        number_of_runs=num_requests_per_level,\n",
    "        num_lines=num_test_samples\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    benchmark_results['concurrency'].append(num_clients)\n",
    "    benchmark_results['p50_latency'].append(p50)\n",
    "    benchmark_results['p90_latency'].append(p90)\n",
    "    benchmark_results['mean_latency'].append(mean)\n",
    "    benchmark_results['error_rate'].append(error_rate)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  P50 Latency: {p50:.2f} ms\")\n",
    "    print(f\"  P90 Latency: {p90:.2f} ms\")\n",
    "    print(f\"  Mean Latency: {mean:.2f} ms\")\n",
    "    print(f\"  Error Rate: {error_rate*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(\"‚úÖ All benchmarks complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640c592e",
   "metadata": {},
   "source": [
    "## 8. Visualize Results\n",
    "\n",
    "Plot latency vs. concurrency to understand endpoint behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf05f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Latency vs Concurrency\n",
    "ax1.plot(\n",
    "    benchmark_results['concurrency'],\n",
    "    benchmark_results['p50_latency'],\n",
    "    marker='o',\n",
    "    label='P50 Latency',\n",
    "    linewidth=2\n",
    ")\n",
    "ax1.plot(\n",
    "    benchmark_results['concurrency'],\n",
    "    benchmark_results['p90_latency'],\n",
    "    marker='s',\n",
    "    label='P90 Latency',\n",
    "    linewidth=2\n",
    ")\n",
    "ax1.plot(\n",
    "    benchmark_results['concurrency'],\n",
    "    benchmark_results['mean_latency'],\n",
    "    marker='^',\n",
    "    label='Mean Latency',\n",
    "    linewidth=2,\n",
    "    linestyle='--'\n",
    ")\n",
    "\n",
    "ax1.set_xlabel('Concurrent Clients', fontsize=12)\n",
    "ax1.set_ylabel('Latency (ms)', fontsize=12)\n",
    "ax1.set_title('Endpoint Latency vs Concurrency', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xticks(benchmark_results['concurrency'])\n",
    "\n",
    "# Plot 2: Error Rate\n",
    "ax2.bar(\n",
    "    benchmark_results['concurrency'],\n",
    "    [e * 100 for e in benchmark_results['error_rate']],\n",
    "    color='coral',\n",
    "    alpha=0.7\n",
    ")\n",
    "ax2.set_xlabel('Concurrent Clients', fontsize=12)\n",
    "ax2.set_ylabel('Error Rate (%)', fontsize=12)\n",
    "ax2.set_title('Error Rate vs Concurrency', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "ax2.set_xticks(benchmark_results['concurrency'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('benchmark_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualization saved as 'benchmark_results.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da45378c",
   "metadata": {},
   "source": [
    "## 9. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c6ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create results table\n",
    "results_df = pd.DataFrame({\n",
    "    'Concurrency': benchmark_results['concurrency'],\n",
    "    'P50 (ms)': [f\"{x:.2f}\" for x in benchmark_results['p50_latency']],\n",
    "    'P90 (ms)': [f\"{x:.2f}\" for x in benchmark_results['p90_latency']],\n",
    "    'Mean (ms)': [f\"{x:.2f}\" for x in benchmark_results['mean_latency']],\n",
    "    'Error Rate': [f\"{x*100:.2f}%\" for x in benchmark_results['error_rate']]\n",
    "})\n",
    "\n",
    "print(\"\\nüìä Benchmark Results Summary:\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('benchmark_results.csv', index=False)\n",
    "print(\"\\n‚úÖ Results saved to 'benchmark_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40017635",
   "metadata": {},
   "source": [
    "## 10. Cleanup Resources\n",
    "\n",
    "‚ö†Ô∏è **Important**: Delete the endpoint to stop incurring charges (~\\$1.41/hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c4e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup endpoint and model\n",
    "print(f\"‚ö†Ô∏è  Deleting endpoint: {endpoint_name}\")\n",
    "print(f\"‚ö†Ô∏è  Deleting model: {model_name}\")\n",
    "print(\"\\nUncomment the lines below to delete:\")\n",
    "print(\"\\n# from sagemaker_core.resources import Endpoint, Model\")\n",
    "print(f\"# endpoint = Endpoint.get(endpoint_name='{endpoint_name}')\")\n",
    "print(\"# endpoint.delete()\")\n",
    "print(f\"# model = Model.get(model_name='{model_name}')\")\n",
    "print(\"# model.delete()\")\n",
    "print(\"\\n# print('‚úÖ Resources deleted')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb08556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete resources:\n",
    "\n",
    "from sagemaker_core.resources import Endpoint, Model\n",
    "\n",
    "endpoint = Endpoint.get(endpoint_name=endpoint_name)\n",
    "endpoint.delete()\n",
    "\n",
    "model = Model.get(model_name=model_name)\n",
    "model.delete()\n",
    "\n",
    "print('‚úÖ Endpoint and model deleted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3201d022",
   "metadata": {},
   "source": [
    "## ‚úÖ Benchmarking Complete!\n",
    "\n",
    "### What We Accomplished:\n",
    "1. ‚úÖ Tested single inference latency\n",
    "2. ‚úÖ Ran concurrent load tests (4-64 clients)\n",
    "3. ‚úÖ Measured P50/P90/mean latencies\n",
    "4. ‚úÖ Visualized performance characteristics\n",
    "5. ‚úÖ Saved results to CSV and PNG\n",
    "\n",
    "### Key Findings:\n",
    "- Review the latency vs. concurrency plot\n",
    "- Identify optimal concurrency for your use case\n",
    "- Check error rates at higher concurrency\n",
    "- Use P90 latency for capacity planning\n",
    "\n",
    "### Performance Insights:\n",
    "- **Low latency**: Single-token classification is fast\n",
    "- **Scalability**: Endpoint handles concurrent requests well\n",
    "- **Cost-effective**: Small instance (ml.g5.xlarge) sufficient\n",
    "\n",
    "### Next Steps:\n",
    "1. Review benchmark results\n",
    "2. Adjust endpoint instance type/count if needed\n",
    "3. **Delete endpoint to stop charges** (see Section 10)\n",
    "4. Deploy to production when ready\n",
    "\n",
    "---\n",
    "\n",
    "**‚ö†Ô∏è Don't forget to delete the endpoint!** (~$1.41/hour)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
