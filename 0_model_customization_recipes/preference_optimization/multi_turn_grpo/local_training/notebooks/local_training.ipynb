{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Local P5 Multi-Turn GRPO Training\n",
    "\n",
    "This notebook runs Multi-Turn GRPO training locally on your P5 EC2 instance.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "- **GPU 7**: vLLM server for fast inference during rollouts\n",
    "- **GPUs 0-6**: Distributed training with DeepSpeed ZeRO-3\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Java 21 installed (for Pyserini)\n",
    "2. 8 GPUs available\n",
    "3. Docker container with PyTorch 2.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q -r requirements_local.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Verify Java Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set Java environment\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-21-openjdk-amd64'\n",
    "os.environ['PATH'] = f\"{os.environ['JAVA_HOME']}/bin:{os.environ['PATH']}\"\n",
    "\n",
    "# Verify Java\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi --list-gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Pre-download Pyserini Index (Optional but Recommended)\n",
    "\n",
    "This downloads the 10GB Wikipedia index used for tool calling. It's better to do this once upfront."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "print(\"Downloading Pyserini Wikipedia index (10GB)...\")\n",
    "searcher = LuceneSearcher.from_prebuilt_index('wikipedia-kilt-doc')\n",
    "print(\"âœ“ Pyserini index ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Configure Training\n",
    "\n",
    "Choose your model and training configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available configs\n",
    "!ls -1 hf_recipes/Qwen/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your configuration\n",
    "CONFIG_FILE = \"hf_recipes/Qwen/Qwen3-0.6B--mt-grpo.yaml\"\n",
    "NUM_GPUS = 8  # Total GPUs (7 for training, 1 for vLLM)\n",
    "VLLM_PORT = 8000\n",
    "\n",
    "print(f\"Configuration: {CONFIG_FILE}\")\n",
    "print(f\"Total GPUs: {NUM_GPUS}\")\n",
    "print(f\"Training GPUs: 0-{NUM_GPUS-2}\")\n",
    "print(f\"vLLM GPU: {NUM_GPUS-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: View Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(CONFIG_FILE, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Model: {config.get('model_name_or_path', 'N/A')}\")\n",
    "print(f\"  Max steps: {config.get('max_steps', 'N/A')}\")\n",
    "print(f\"  Learning rate: {config.get('learning_rate', 'N/A')}\")\n",
    "print(f\"  Batch size: {config.get('per_device_train_batch_size', 'N/A')}\")\n",
    "print(f\"  Num generations: {config.get('num_generations', 'N/A')}\")\n",
    "print(f\"  Max env steps: {config.get('max_env_steps', 'N/A')}\")\n",
    "print(f\"  Turn advantage coef: {config.get('turn_advantage_coef', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Launch Training\n",
    "\n",
    "### Option A: Run in Notebook (Blocking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will block until training completes\n",
    "!bash local_mt_grpo_train.sh --config {CONFIG_FILE} --num_process {NUM_GPUS} --vllm_port {VLLM_PORT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Run in Background (Non-blocking)\n",
    "\n",
    "Run this in a terminal instead:\n",
    "\n",
    "```bash\n",
    "cd /app/mt-grpo/local_training\n",
    "nohup bash local_mt_grpo_train.sh \\\n",
    "  --config hf_recipes/Qwen/Qwen3-0.6B--mt-grpo.yaml \\\n",
    "  --num_process 8 \\\n",
    "  --vllm_port 8000 \\\n",
    "  > training.log 2>&1 &\n",
    "\n",
    "# Monitor progress\n",
    "tail -f training.log\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Monitor Training\n",
    "\n",
    "### Check vLLM Server Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "try:\n",
    "    response = requests.get(f\"http://localhost:{VLLM_PORT}/health\", timeout=5)\n",
    "    print(f\"âœ“ vLLM server is running (status: {response.status_code})\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— vLLM server not reachable: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View vLLM Server Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -50 vllm_server.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor GPU Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Training Logs (if running in background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you ran training in background\n",
    "!tail -100 training.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Check Training Output\n",
    "\n",
    "Training checkpoints and logs will be saved according to your config file's `output_dir` setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List output directories\n",
    "import glob\n",
    "\n",
    "output_dirs = glob.glob(\"outputs/*\")\n",
    "if output_dirs:\n",
    "    print(\"Training outputs:\")\n",
    "    for d in sorted(output_dirs):\n",
    "        print(f\"  {d}\")\n",
    "else:\n",
    "    print(\"No output directories found yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "1. **Java not found**: Install OpenJDK 21\n",
    "   ```bash\n",
    "   apt-get update && apt-get install -y openjdk-21-jdk\n",
    "   ```\n",
    "\n",
    "2. **vLLM server fails to start**: Check GPU availability and port conflicts\n",
    "   ```bash\n",
    "   lsof -i :8000  # Check if port is in use\n",
    "   ```\n",
    "\n",
    "3. **Out of memory**: Reduce batch size or num_generations in config\n",
    "\n",
    "4. **Import errors**: Ensure all dependencies are installed with correct versions\n",
    "   ```bash\n",
    "   pip list | grep -E \"vllm|trl|transformers\"\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Stop vLLM server if it's still running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and kill vLLM process\n",
    "!pkill -f \"vllm.entrypoints.openai.api_server\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
