{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ebc4cc-9abf-4d4e-92b4-1719a2d87f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "import json\n",
    "import torch\n",
    "from math import ceil\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Optional\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from financial_tools_complex import run_tool\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c385d83-14e6-479d-817e-e72bce71be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \\\n",
    "\"\"\"You are a financial planning assistant with tools for portfolio allocation, mortgage affordability, tax optimization, retirement readiness, debt payoff strategies, insurance needs, education funding, and currency exchange. Analyze user requests and call the appropriate tool with all required parameters extracted from their query. Return concise answers with key metrics. Do not ask for clarification - use reasonable defaults if needed.\n",
    "\n",
    "You are provided with function signatures within <tools> </tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: \n",
    "\n",
    "<tools> \n",
    "[\n",
    "{\"name\": \"calculate_portfolio_allocation\", \"description\": \"Calculate optimal portfolio allocation based on risk tolerance, age, time horizon, and liquidity needs.\", \"parameters\": {\"total_investment\": {\"type\": \"float\"}, \"risk_tolerance\": {\"type\": \"str\", \"enum\": [\"conservative\", \"moderate_conservative\", \"moderate\", \"moderate_aggressive\", \"aggressive\"]}, \"time_horizon_years\": {\"type\": \"int\", \"range\": \"1-50\"}, \"current_age\": {\"type\": \"int\", \"range\": \"18-100\"}, \"retirement_age\": {\"type\": \"int\"}, \"income_need_percentage\": {\"type\": \"float\", \"range\": \"0.0-0.10\"}, \"existing_allocations\": {\"type\": \"Dict[str, float]\"}, \"esg_preference\": {\"type\": \"bool\"}, \"tax_loss_harvesting\": {\"type\": \"bool\"}, \"rebalancing_frequency\": {\"type\": \"str\", \"enum\": [\"monthly\", \"quarterly\", \"semi_annual\", \"annual\", \"threshold_based\"]}, \"inflation_protection\": {\"type\": \"bool\"}, \"liquidity_requirement\": {\"type\": \"str\", \"enum\": [\"immediate\", \"short_term\", \"medium_term\", \"long_term\"]}}}, \n",
    "{\"name\": \"calculate_mortgage_affordability\", \"description\": \"Calculate max home price and monthly payment based on income, debts, credit score, and loan type.\", \"parameters\": {\"annual_income\": {\"type\": \"float\"}, \"monthly_debts\": {\"type\": \"float\"}, \"down_payment\": {\"type\": \"float\"}, \"credit_score\": {\"type\": \"int\", \"range\": \"300-850\"}, \"loan_type\": {\"type\": \"str\", \"enum\": [\"conventional\", \"fha\", \"va\", \"jumbo\"]}, \"property_state\": {\"type\": \"str\"}, \"property_tax_rate\": {\"type\": \"float\", \"range\": \"0.003-0.025\"}, \"hoa_fees\": {\"type\": \"float\"}, \"homeowners_insurance\": {\"type\": \"float\"}, \"pmi_required\": {\"type\": \"bool\"}, \"interest_rate_override\": {\"type\": \"Optional[float]\"}, \"loan_term_years\": {\"type\": \"int\", \"enum\": [15, 20, 30]}}}, \n",
    "{\"name\": \"optimize_tax_strategy\", \"description\": \"Calculate tax liability with optimization suggestions based on income, deductions, and investments.\", \"parameters\": {\"gross_income\": {\"type\": \"float\"}, \"filing_status\": {\"type\": \"str\", \"enum\": [\"single\", \"married\", \"head\"]}, \"state\": {\"type\": \"str\"}, \"retirement_contributions\": {\"type\": \"Dict[str, float]\"}, \"capital_gains_short\": {\"type\": \"float\"}, \"capital_gains_long\": {\"type\": \"float\"}, \"dividend_income_qualified\": {\"type\": \"float\"}, \"dividend_income_ordinary\": {\"type\": \"float\"}, \"itemized_deductions\": {\"type\": \"float\"}, \"dependents\": {\"type\": \"int\", \"range\": \"0-10\"}, \"self_employment_income\": {\"type\": \"float\"}, \"rental_income\": {\"type\": \"float\"}}}, {\"name\": \"calculate_retirement_readiness\", \"description\": \"Assess retirement readiness by projecting savings vs income needs.\", \"parameters\": {\"current_age\": {\"type\": \"int\", \"range\": \"18-80\"}, \"retirement_age\": {\"type\": \"int\", \"max\": 75}, \"current_savings\": {\"type\": \"float\"}, \"annual_contribution\": {\"type\": \"float\"}, \"employer_match_percent\": {\"type\": \"float\", \"range\": \"0.0-1.0\"}, \"expected_return\": {\"type\": \"float\", \"range\": \"0.03-0.12\"}, \"inflation_rate\": {\"type\": \"float\", \"range\": \"0.01-0.05\"}, \"desired_retirement_income\": {\"type\": \"float\"}, \"social_security_estimate\": {\"type\": \"float\"}, \"pension_income\": {\"type\": \"float\"}, \"healthcare_cost_annual\": {\"type\": \"float\"}, \"life_expectancy\": {\"type\": \"int\"}}}, \n",
    "{\"name\": \"analyze_debt_payoff_strategy\", \"description\": \"Analyze optimal debt payoff using avalanche, snowball, or hybrid strategies.\", \"parameters\": {\"debts\": {\"type\": \"List[Dict]\", \"schema\": {\"balance\": \"float\", \"rate\": \"float\", \"minimum\": \"float\", \"type\": \"str\"}}, \"monthly_payment_budget\": {\"type\": \"float\"}, \"strategy\": {\"type\": \"str\", \"enum\": [\"avalanche\", \"snowball\", \"hybrid\"]}, \"extra_payment_allocation\": {\"type\": \"str\", \"enum\": [\"single_focus\", \"proportional\", \"high_interest_only\"]}, \"interest_rate_threshold\": {\"type\": \"float\", \"range\": \"0.05-0.25\"}, \"consolidation_available\": {\"type\": \"bool\"}, \"consolidation_rate\": {\"type\": \"float\", \"range\": \"0.04-0.15\"}, \"balance_transfer_fee\": {\"type\": \"float\", \"range\": \"0.0-0.05\"}, \"credit_score_impact_weight\": {\"type\": \"float\", \"range\": \"0.0-1.0\"}}}, \n",
    "{\"name\": \"calculate_insurance_needs\", \"description\": \"Calculate life, disability, and LTC insurance needs based on income and dependents.\", \"parameters\": {\"age\": {\"type\": \"int\", \"range\": \"18-80\"}, \"annual_income\": {\"type\": \"float\"}, \"dependents\": {\"type\": \"int\", \"range\": \"0-10\"}, \"mortgage_balance\": {\"type\": \"float\"}, \"other_debts\": {\"type\": \"float\"}, \"existing_coverage\": {\"type\": \"Dict[str, float]\", \"keys\": [\"life\", \"disability\", \"ltc\"]}, \"health_status\": {\"type\": \"str\", \"enum\": [\"excellent\", \"good\", \"fair\", \"poor\"]}, \"occupation_risk\": {\"type\": \"str\", \"enum\": [\"low\", \"medium\", \"high\"]}, \"years_until_retirement\": {\"type\": \"int\", \"range\": \"1-40\"}, \"spouse_income\": {\"type\": \"float\"}, \"college_funding_need\": {\"type\": \"float\"}, \"final_expenses\": {\"type\": \"float\"}}}, \n",
    "{\"name\": \"calculate_education_funding\", \"description\": \"Calculate college funding needs and savings gap.\", \"parameters\": {\"child_current_age\": {\"type\": \"int\", \"range\": \"0-17\"}, \"college_start_age\": {\"type\": \"int\"}, \"years_of_college\": {\"type\": \"int\", \"range\": \"2-6\"}, \"current_annual_cost\": {\"type\": \"float\"}, \"education_inflation_rate\": {\"type\": \"float\", \"range\": \"0.03-0.08\"}, \"current_savings\": {\"type\": \"float\"}, \"monthly_contribution\": {\"type\": \"float\"}, \"expected_return\": {\"type\": \"float\", \"range\": \"0.04-0.10\"}, \"financial_aid_expected\": {\"type\": \"float\"}, \"student_contribution_percent\": {\"type\": \"float\", \"range\": \"0.0-0.50\"}, \"state_residency\": {\"type\": \"str\"}, \"school_type\": {\"type\": \"str\", \"enum\": [\"public_instate\", \"public_outstate\", \"private\", \"community\"]}}}, \n",
    "{\"name\": \"calculate_currency_exchange_arbitrage\", \"description\": \"Calculate optimal currency exchange with fees, hedging, and arbitrage options.\", \"parameters\": {\"base_currency\": {\"type\": \"str\"}, \"target_currency\": {\"type\": \"str\"}, \"amount\": {\"type\": \"float\"}, \"exchange_method\": {\"type\": \"str\", \"enum\": [\"bank\", \"forex_broker\", \"crypto_bridge\", \"wire\"]}, \"transfer_fee_percent\": {\"type\": \"float\", \"range\": \"0.0-0.05\"}, \"transfer_fee_fixed\": {\"type\": \"float\"}, \"intermediate_currency\": {\"type\": \"Optional[str]\"}, \"spot_rate_override\": {\"type\": \"Optional[float]\"}, \"forward_contract_months\": {\"type\": \"int\", \"range\": \"0-24\"}, \"hedging_strategy\": {\"type\": \"str\", \"enum\": [\"none\", \"forward\", \"option\", \"collar\"]}, \"tax_reporting_required\": {\"type\": \"bool\"}}}\n",
    "] \n",
    "</tools> \n",
    "\n",
    "Use the following pydantic model JSON schema for each tool call you will make: {\"properties\": {\"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}, \"name\": {\"title\": \"Name\", \"type\": \"string\"}}, \"required\": [\"arguments\", \"name\"], \"title\": \"FunctionCall\", \"type\": \"object\"} For each function call return a json object with function name and arguments within <tool_call> </tool_call> XML tags as follows:\n",
    "<tool_call>\n",
    "{\"name\": <function-name>, \"arguments\": <args-dict>}\n",
    "</tool_call>\n",
    "\n",
    "You are a financial planning assistant. Analyze user requests and call the appropriate tool with all required parameters. Return concise answers. Use reasonable defaults if needed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617b5486-bb5e-4745-a075-978868fcd0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('json', data_files=\"financial_val_v4.jsonl\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebfe2f1-04c1-4cbb-bd4b-bca89286c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_samples = []\n",
    "for row in tqdm(dataset, total=len(dataset)):\n",
    "    valid_samples.append(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\"content\": system_prompt, \"role\": \"system\"},\n",
    "                {\"content\": row[\"prompt\"][1][\"content\"], \"role\": \"user\"},\n",
    "            ],\n",
    "            \"gt_answer\": row[\"answer\"],\n",
    "            \"gt_tool\": row[\"ground_truth\"]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40fa4a7-2a1f-4a94-a2b5-f95c703303e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile once (not inside the loop)\n",
    "TOOL_CALL_RE = re.compile(r\"<tool_call>\\s*(.*?)\\s*</tool_call>\", re.DOTALL)\n",
    "\n",
    "# Qwen-style think end token id (</think>) you were using\n",
    "THINK_END_TOKEN_ID = 151668\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "\n",
    "def _safe_get_prompt(messages: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"Best-effort: return user prompt content if present.\"\"\"\n",
    "    if len(messages) > 1 and isinstance(messages[1], dict):\n",
    "        return str(messages[1].get(\"content\", \"\"))\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def _split_thinking_by_token_id(\n",
    "    output_ids: List[int],\n",
    "    tokenizer,\n",
    "    think_end_token_id: int = THINK_END_TOKEN_ID,\n",
    ") -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Split decoded output into (thinking, content) by the last occurrence of </think> token id.\n",
    "    If not found, thinking=\"\", content=full decoded.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # index points to position *after* the </think> token\n",
    "        idx = len(output_ids) - output_ids[::-1].index(think_end_token_id)\n",
    "        thinking_ids = output_ids[:idx]\n",
    "        content_ids = output_ids[idx:]\n",
    "    except ValueError:\n",
    "        thinking_ids = []\n",
    "        content_ids = output_ids\n",
    "\n",
    "    thinking = tokenizer.decode(thinking_ids, skip_special_tokens=True).strip()\n",
    "    content = tokenizer.decode(content_ids, skip_special_tokens=True).strip()\n",
    "    return thinking, content\n",
    "\n",
    "\n",
    "def _extract_tool_call(content: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parse the first <tool_call>...</tool_call> block as JSON.\n",
    "    Returns dict if valid, else None.\n",
    "    \"\"\"\n",
    "    m = TOOL_CALL_RE.search(content or \"\")\n",
    "    if not m:\n",
    "        return None\n",
    "    payload = m.group(1)\n",
    "    try:\n",
    "        obj = json.loads(payload)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def inference_batched(\n",
    "    inst_model,\n",
    "    inst_tokenizer,\n",
    "    valid_messages_samples,\n",
    "    *,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    max_new_tokens: int = 1024,\n",
    "    temperature=0.01,\n",
    "    top_p=0.99,\n",
    "    think_end_token_id: int = THINK_END_TOKEN_ID,\n",
    "):\n",
    "\n",
    "    responses = []\n",
    "\n",
    "    total = len(valid_messages_samples)\n",
    "    n_batches = ceil(total / batch_size)\n",
    "\n",
    "    for b in tqdm(range(n_batches)):\n",
    "        batch = valid_messages_samples[b * batch_size:(b + 1) * batch_size]\n",
    "\n",
    "        # ---- Build prompts\n",
    "        prompts = [\n",
    "            inst_tokenizer.apply_chat_template(\n",
    "                row[\"messages\"],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "                enable_thinking=True,\n",
    "            )\n",
    "            for row in batch\n",
    "        ]\n",
    "\n",
    "        model_inputs = inst_tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "        ).to(inst_model.device)\n",
    "\n",
    "        # ---- One GPU forward for 8 requests\n",
    "        generated_ids = inst_model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p\n",
    "        )\n",
    "\n",
    "        for i, row in enumerate(batch):\n",
    "            prompt_len = model_inputs.input_ids.shape[1]\n",
    "            output_ids = generated_ids[i, prompt_len:].tolist()\n",
    "\n",
    "            thinking_content, content = _split_thinking_by_token_id(\n",
    "                output_ids, inst_tokenizer, think_end_token_id\n",
    "            )\n",
    "\n",
    "            tool_call = _extract_tool_call(content)\n",
    "\n",
    "            predicted_answer = None\n",
    "            parsed_tool_call = None\n",
    "            if tool_call is not None:\n",
    "                parsed_tool_call = tool_call\n",
    "                try:\n",
    "                    predicted_answer = run_tool(tool_call)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            responses.append({\n",
    "                \"prompt\": _safe_get_prompt(row[\"messages\"]),\n",
    "                \"gt_answer\": row.get(\"gt_answer\"),\n",
    "                \"gt_tool\": row.get(\"gt_tool\"),\n",
    "                \"predict_tool\": content,\n",
    "                \"predict_tool_parsed\": parsed_tool_call,\n",
    "                \"reasoning\": thinking_content,\n",
    "                \"predicted_answer\": predicted_answer,\n",
    "            })\n",
    "\n",
    "    return responses\n",
    "    \n",
    "\n",
    "def response_validity(row):\n",
    "    \"\"\"\n",
    "    Binary answer accuracy scorer.\n",
    "\n",
    "    Rules:\n",
    "    - If ground-truth answer starts with 'Error:' → return None (row ignored).\n",
    "    - If predicted answer starts with 'Error:' → return 0.\n",
    "    - Otherwise → return 1.\n",
    "\n",
    "    Returns:\n",
    "        1     → correct / usable answer\n",
    "        0     → model failure\n",
    "        None  → invalid ground-truth (excluded from evaluation)\n",
    "    \"\"\"\n",
    "\n",
    "    gt = str(row[\"gt_answer\"])\n",
    "    pred = str(row[\"predicted_answer\"])\n",
    "\n",
    "    if gt.startswith(\"Error:\"):\n",
    "        return None\n",
    "\n",
    "    if pred.startswith(\"Error:\"):\n",
    "        return 0\n",
    "\n",
    "    return 1\n",
    "\n",
    "\n",
    "def exact_answer_match(row):\n",
    "    \"\"\"\n",
    "    Strict final-answer exactness scorer.\n",
    "\n",
    "    Rules:\n",
    "    - If gt_answer starts with 'Error:' → return None (row ignored)\n",
    "    - Normalize both strings by:\n",
    "        • stripping whitespace\n",
    "        • collapsing internal spaces\n",
    "    - Return:\n",
    "        1 → exact match\n",
    "        0 → mismatch\n",
    "        None → invalid GT\n",
    "    \"\"\"\n",
    "\n",
    "    gt = str(row[\"gt_answer\"])\n",
    "    pred = str(row[\"predicted_answer\"])\n",
    "\n",
    "    if gt.startswith(\"Error:\"):\n",
    "        return None\n",
    "\n",
    "    def normalize(s):\n",
    "        return \" \".join(s.strip().split())\n",
    "\n",
    "    return 1 if normalize(gt) == normalize(pred) else 0\n",
    "\n",
    "\n",
    "def normalized_tool_call_score(gt: dict, pred) -> float:\n",
    "    \"\"\"\n",
    "    Final score ∈ [0,1]\n",
    "\n",
    "    Weighting:\n",
    "      - 0.3 → correct tool name\n",
    "      - 0.7 → recursive strict argument match (2-decimal exact), using GT schema\n",
    "\n",
    "    Notes:\n",
    "      - If pred is None / NaN / not a dict → tool_score=0 and arg_score=0 (unless GT has no args).\n",
    "    \"\"\"\n",
    "\n",
    "    # Coerce bad preds to empty dict\n",
    "    if not isinstance(pred, dict):\n",
    "        pred = {}\n",
    "\n",
    "    def value_match(gt_val, pred_val):\n",
    "        # Dict → recurse\n",
    "        if isinstance(gt_val, dict):\n",
    "            if not isinstance(pred_val, dict):\n",
    "                return 0\n",
    "            return dict_score(gt_val, pred_val)\n",
    "\n",
    "        # None leaf\n",
    "        if gt_val is None:\n",
    "            return 1 if pred_val is None else 0\n",
    "\n",
    "        # Numeric leaf (round to 2 decimals)\n",
    "        if isinstance(gt_val, (int, float)):\n",
    "            if pred_val is None:\n",
    "                return 0\n",
    "            try:\n",
    "                return 1 if round(float(gt_val), 2) == round(float(pred_val), 2) else 0\n",
    "            except Exception:\n",
    "                return 0\n",
    "\n",
    "        # Fallback strict equality (strings, bools, etc.)\n",
    "        return 1 if gt_val == pred_val else 0\n",
    "\n",
    "    def dict_score(gt_dict, pred_dict):\n",
    "        total = len(gt_dict)\n",
    "        if total == 0:\n",
    "            return 1.0\n",
    "\n",
    "        correct = 0\n",
    "        for k, gt_val in gt_dict.items():\n",
    "            if k not in pred_dict:\n",
    "                continue\n",
    "            correct += value_match(gt_val, pred_dict[k])\n",
    "\n",
    "        return correct / total\n",
    "\n",
    "    # Tool name component\n",
    "    tool_score = 0.3 if gt.get(\"name\") == pred.get(\"name\") else 0.0\n",
    "\n",
    "    gt_args = gt.get(\"arguments\", {}) or {}\n",
    "    pred_args = pred.get(\"arguments\", {}) if isinstance(pred.get(\"arguments\", {}), dict) else {}\n",
    "\n",
    "    # Argument correctness (recursive)\n",
    "    arg_component = dict_score(gt_args, pred_args) if gt_args else 1.0\n",
    "\n",
    "    return tool_score + 0.7 * arg_component\n",
    "\n",
    "\n",
    "def tool_call_schema_match(row) -> float:\n",
    "    \"\"\"\n",
    "    Build a GT tool-call from the row and score against parsed prediction.\n",
    "    Filters GT arguments to only include non-None values (keeps 0/False).\n",
    "    \"\"\"\n",
    "\n",
    "    gt_tool = row[\"gt_tool\"] or {}\n",
    "    gt_args = (gt_tool.get(\"arguments\") or {})\n",
    "\n",
    "    gt = {\n",
    "        \"name\": gt_tool.get(\"name\"),\n",
    "        # keep 0 / False, drop only None\n",
    "        \"arguments\": {k: v for k, v in gt_args.items() if v is not None},\n",
    "    }\n",
    "\n",
    "    pred = row.get(\"predict_tool_parsed\", None) if hasattr(row, \"get\") else row[\"predict_tool_parsed\"]\n",
    "    return normalized_tool_call_score(gt=gt, pred=pred)\n",
    "\n",
    "\n",
    "def compute_individual_metrics(df):\n",
    "    df[\"response_validity\"] = df.apply(response_validity, axis=1)\n",
    "    df[\"exact_answer_match\"] = df.apply(exact_answer_match, axis=1)\n",
    "    df[\"tool_call_schema_match\"] = df.apply(tool_call_schema_match, axis=1)\n",
    "    return {\n",
    "        \"response_validity\": df[\"response_validity\"].mean().round(3),\n",
    "        \"exact_answer_match\": df[\"exact_answer_match\"].mean().round(3),\n",
    "        \"tool_call_schema_match\": df[\"tool_call_schema_match\"].mean().round(3)\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c64dd1d-3500-43b9-91b9-576d4507f37a",
   "metadata": {},
   "source": [
    "### Qwen 0.6B Fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8da367a-eb2c-465a-b369-53fe6da1f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_checkpoint_path_qwen06 = \"/path/to/checkpoint/0.6B/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cde788-f73a-40b6-b088-0efb2e7671db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tuned_checkpoint_path_qwen06, padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfffe984-d891-491e-9c99-fc62420253ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned\n",
    "model_trained_qwen06 = AutoModelForCausalLM.from_pretrained(\n",
    "    tuned_checkpoint_path_qwen06,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"kernels-community/vllm-flash-attn3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc260e9-bc86-4a63-995d-9636a43ab7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_responses_qwen06 = inference_batched(\n",
    "    inst_model=model_trained_qwen06, \n",
    "    inst_tokenizer=tokenizer, \n",
    "    valid_messages_samples=valid_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a74b29-d661-42c9-b2e8-5a098473774b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_trained_qwen06 = pd.DataFrame(trained_responses_qwen06)\n",
    "trained_metrics_qwen06 = compute_individual_metrics(df_trained_qwen06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403810db-267a-4419-85a2-9ccbde03706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_metrics_qwen06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193b0c80-8543-4f55-b11c-09db7004bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trained_qwen06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9aac4a-c46b-4b76-b1f7-c820a9441e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_trained_qwen06\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af7c27e-b51f-4bc5-9eb3-651cd77d91e4",
   "metadata": {},
   "source": [
    "### Base Model for Qwen 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f5ee69-8cf0-44c2-93a1-6eb1fd25884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base\n",
    "model_base_qwen06 = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen3-0.6B\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"kernels-community/vllm-flash-attn3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5e9e9-c6ad-450e-b826-38f90304b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_responses_qwen06 = inference_batched(\n",
    "    inst_model=model_base_qwen06, \n",
    "    inst_tokenizer=tokenizer, \n",
    "    valid_messages_samples=valid_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed819266-0eb8-49e0-9b6a-49e8aa0aad79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_base_qwen06 = pd.DataFrame(base_responses_qwen06)\n",
    "base_metrics_qwen06 = compute_individual_metrics(df_base_qwen06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac264f03-2b2f-42d2-8832-0d86a4b24ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_metrics_qwen06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fb42bf-09d7-43cc-b4d1-fdf4b696f633",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_base_qwen06\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44febd28-fb30-401d-bc2a-64640179a5b7",
   "metadata": {},
   "source": [
    "### Qwen 1.7B Fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d8c4d8-a1e3-4e57-9a6b-c1f9b6004f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_checkpoint_path_qwen17 = \"/path/to/checkpoint/1.7B/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db169c04-3d50-4635-a9e5-855c7dfd5801",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_17B = AutoTokenizer.from_pretrained(tuned_checkpoint_path_qwen17, padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e32c86-badd-4dec-a558-3617d77f3d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned\n",
    "model_trained_qwen17 = AutoModelForCausalLM.from_pretrained(\n",
    "    tuned_checkpoint_path_qwen17,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"kernels-community/vllm-flash-attn3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72886ce8-da77-4656-9020-488170404efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_responses_qwen17 = inference_batched(\n",
    "    inst_model=model_trained_qwen17, \n",
    "    inst_tokenizer=tokenizer_17B, \n",
    "    valid_messages_samples=valid_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a216be23-ef90-4fa3-845e-23b460cdb581",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trained_qwen17 = pd.DataFrame(trained_responses_qwen17)\n",
    "trained_metrics_qwen17 = compute_individual_metrics(df_trained_qwen17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830297f8-aedb-40e4-9ad5-b21efa092947",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_metrics_qwen17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8964b1e-de21-4023-be8f-19b643e4899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_trained_qwen17\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26df01b6-67a2-439c-b9ed-ec5c181cf8f4",
   "metadata": {},
   "source": [
    "### Base Model for Qwen 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb70fe9-6d6e-4a6c-ad13-2c2e60a7ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base\n",
    "model_base_qwen17 = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen3-1.7B\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"kernels-community/vllm-flash-attn3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c12bf7-401f-49c4-9ff0-5568c09847cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_responses_qwen17 = inference_batched(\n",
    "    inst_model=model_base_qwen17, \n",
    "    inst_tokenizer=tokenizer_17B, \n",
    "    valid_messages_samples=valid_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51aefd9-ab9a-4806-9e43-bfb1c20fc4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_qwen17 = pd.DataFrame(base_responses_qwen17)\n",
    "base_metrics_qwen17 = compute_individual_metrics(df_base_qwen17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d881b911-92bd-4ead-8489-b10d888b939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_metrics_qwen17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03fc158-ac1c-4b2f-af9e-783936ed8695",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_base_qwen17\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91b9812-a86f-4736-bb5a-e5a068bd2f05",
   "metadata": {},
   "source": [
    "### Evaluate Foundation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1a028c-281c-45eb-91be-eabb3ec1a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from litellm import completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b9bc50-84be-4af9-9f18-40d43c301efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"\"\n",
    "os.environ[\"AWS_REGION_NAME\"] = \"\"\n",
    "os.environ[\"AWS_SESSION_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e126355-13b3-4b09-944f-a1813af31b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_for_models = {\n",
    "    \"Qwen/Qwen3-0.6B (rl-tuned)\": trained_metrics_qwen06,\n",
    "    \"Qwen/Qwen3-0.6B (base)\": base_metrics_qwen06,\n",
    "    \"Qwen/Qwen3-1.7B (rl-tuned)\": trained_metrics_qwen17,\n",
    "    \"Qwen/Qwen3-1.7B (base)\": base_metrics_qwen17,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16f3b41-bfc3-4252-bb90-a2c34de9638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_for_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58207fad-34af-46d3-ae8f-36a9115b757a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_br_mappings = {\n",
    "    \"bedrock/openai.gpt-oss-20b-1:0\": \"openai/gpt-oss-20b\",\n",
    "    \"bedrock/openai.gpt-oss-120b-1:0\": \"openai/gpt-oss-120b\",\n",
    "    \"bedrock/us.amazon.nova-lite-v1:0\": \"amazon/nova-lite\",\n",
    "    \"bedrock/us.amazon.nova-pro-v1:0\": \"amazon/nova-pro\",\n",
    "    \"bedrock/us.anthropic.claude-sonnet-4-5-20250929-v1:0\": \"anthropic/claude-sonnet-4.5\"\n",
    "}\n",
    "\n",
    "model_reasoning_mappings = {\n",
    "    \"bedrock/openai.gpt-oss-20b-1:0\": \"low\",\n",
    "    \"bedrock/openai.gpt-oss-120b-1:0\": \"low\",\n",
    "    \"bedrock/us.amazon.nova-lite-v1:0\": None,\n",
    "    \"bedrock/us.amazon.nova-pro-v1:0\": None,\n",
    "    \"bedrock/us.anthropic.claude-sonnet-4-5-20250929-v1:0\": \"low\"\n",
    "}\n",
    "\n",
    "dicts_of_dfs = {}\n",
    "\n",
    "for model_key in list(model_br_mappings.keys()):\n",
    "    responses_from_prop_llm = []\n",
    "    _samples = valid_samples\n",
    "\n",
    "    reasoning_effort = model_reasoning_mappings[model_key]\n",
    "    if reasoning_effort:\n",
    "        kwargs = {\n",
    "            \"model\": model_key,\n",
    "            \"reasoning_effort\": reasoning_effort,\n",
    "            # \"temperature\": 0.01,\n",
    "            # \"max_tokens\": 1024\n",
    "        }\n",
    "    else:\n",
    "        kwargs = {\n",
    "            \"model\": model_key,\n",
    "            \"temperature\": 0.01,\n",
    "            # \"max_tokens\": 1024\n",
    "        }\n",
    "    for row in tqdm(_samples, total=len(_samples)):\n",
    "        messages = row[\"messages\"]\n",
    "        \n",
    "        kwargs[\"messages\"] = row[\"messages\"]\n",
    "        try:\n",
    "            response = completion(\n",
    "                **kwargs\n",
    "            )\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        \n",
    "        pattern = re.compile(r\"<tool_call>\\s*(.*?)\\s*</tool_call>\", re.DOTALL)\n",
    "        if reasoning_effort:\n",
    "            thinking_content = response.choices[0].message.reasoning_content\n",
    "        else:\n",
    "            thinking_content = None\n",
    "            \n",
    "        content = response.choices[0].message.content\n",
    "        match = pattern.search(content)\n",
    "        if match:\n",
    "            payload = match.group(1)\n",
    "            try:\n",
    "                tool_call = json.loads(payload)\n",
    "                answer = run_tool(tool_call)\n",
    "                \n",
    "                responses_from_prop_llm.append({\n",
    "                    \"prompt\": messages[1][\"content\"],\n",
    "                    \"gt_answer\": row[\"gt_answer\"],\n",
    "                    \"gt_tool\": row[\"gt_tool\"],\n",
    "                    \"predict_tool\": content,\n",
    "                    \"predict_tool_parsed\": tool_call,\n",
    "                    \"reasoning\": thinking_content,\n",
    "                    \"predicted_answer\": answer\n",
    "                    \n",
    "                })\n",
    "            except Exception as e:\n",
    "                responses_from_prop_llm.append({\n",
    "                    \"prompt\": messages[1][\"content\"],\n",
    "                    \"gt_answer\": row[\"gt_answer\"],\n",
    "                    \"gt_tool\": row[\"gt_tool\"],\n",
    "                    \"predict_tool\": content,\n",
    "                    \"predict_tool_parsed\": None,\n",
    "                    \"reasoning\": thinking_content,\n",
    "                    \"predicted_answer\": None\n",
    "                    \n",
    "                })\n",
    "            \n",
    "        else:\n",
    "            responses_from_prop_llm.append({\n",
    "                \"prompt\": messages[1][\"content\"],\n",
    "                \"gt_answer\": row[\"gt_answer\"],\n",
    "                \"gt_tool\": row[\"gt_tool\"],\n",
    "                \"predict_tool\": content,\n",
    "                \"predict_tool_parsed\": None,\n",
    "                \"reasoning\": None,\n",
    "                \"predicted_answer\": None\n",
    "                \n",
    "            })\n",
    "        if 'claude' in model_key:\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            time.sleep(0.2)\n",
    "    df_frontier = pd.DataFrame(responses_from_prop_llm)\n",
    "    frontier_metrics = compute_individual_metrics(df_frontier)\n",
    "\n",
    "    print(f\"Accuracy for {model_key} model: {frontier_metrics}\")\n",
    "    results_for_models[model_br_mappings[model_key]] = frontier_metrics\n",
    "    dicts_of_dfs[model_br_mappings[model_key]] = df_frontier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b46efb-4dce-48c0-8550-a068abaf5f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_for_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac6e2b0-4977-4e7e-a62a-b850508b84d9",
   "metadata": {},
   "source": [
    "### Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985922a1-c6ee-4351-a70a-3b5dc315652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cfc68d-31b7-46d7-8e0a-281f77562895",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_three_benchmark_charts(results_for_models: dict):\n",
    "    \"\"\"\n",
    "    Creates three bar charts:\n",
    "      1. Binary Answer Accuracy\n",
    "      2. Exact Answer Accuracy\n",
    "      3. Tool Semantic Accuracy\n",
    "\n",
    "    Each chart:\n",
    "      - Sorted by metric value\n",
    "      - Uses consistent static colors per model\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Build DataFrame ----\n",
    "    df = pd.DataFrame.from_dict(results_for_models, orient=\"index\").reset_index()\n",
    "    df = df.rename(columns={\"index\": \"Model\"})\n",
    "\n",
    "    # ---- Static color map (consistent across all charts) ----\n",
    "    palette = sns.color_palette(\"muted\", len(df))\n",
    "    color_map = dict(zip(df[\"Model\"], palette))\n",
    "\n",
    "    metrics = [\n",
    "        (\"response_validity\", \"Response Validity\"),\n",
    "        (\"exact_answer_match\", \"Exact Answer Match\"),\n",
    "        (\"tool_call_schema_match\", \"Tool Call Schema Match\"),\n",
    "    ]\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\", font_scale=1.05)\n",
    "\n",
    "    for metric_key, metric_title in metrics:\n",
    "\n",
    "        plot_df = df.sort_values(metric_key, ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        ax = sns.barplot(\n",
    "            data=plot_df,\n",
    "            x=\"Model\",\n",
    "            y=metric_key,\n",
    "            palette=color_map\n",
    "        )\n",
    "\n",
    "        ax.set_ylim(0, 1.05)\n",
    "        ax.set_ylabel(\"Score\")\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_title(metric_title, fontsize=14, pad=10)\n",
    "        plt.xticks(rotation=35, ha=\"right\")\n",
    "\n",
    "        # ---- Value labels ----\n",
    "        for p in ax.patches:\n",
    "            h = p.get_height()\n",
    "            ax.text(\n",
    "                p.get_x() + p.get_width() / 2,\n",
    "                h + 0.01,\n",
    "                f\"{h:.2f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=9\n",
    "            )\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{metric_key}.png') \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f92a94-17ad-4bae-b1b7-34f4a054b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_three_benchmark_charts(results_for_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b52605-c77a-4159-a57d-e76d7bbadab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_for_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb6f899-1a64-4f75-af9f-87cfceeef509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
