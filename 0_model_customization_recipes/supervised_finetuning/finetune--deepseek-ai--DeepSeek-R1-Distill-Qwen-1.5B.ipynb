{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60b91a9-7d9c-4c45-be0f-5a224581f644",
   "metadata": {},
   "source": [
    "# ðŸš€ Customize and Deploy `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` on Amazon SageMaker AI\n",
    "---\n",
    "In this notebook, we explore how to use **DeepSeek-R1-Distill-Qwen-1.5B**, a lightweight distilled reasoning model based on Qwen. Youâ€™ll learn how to fine-tune it on your dataset, evaluate its performance, and deploy it at scale with SageMaker.\n",
    "\n",
    "**What is DeepSeek-R1-Distill-Qwen-1.5B?**\n",
    "\n",
    "The **DeepSeek-R1-Distill-Qwen-1.5B** model is a **1.5-billion-parameter distilled reasoning model** derived from the DeepSeek-R1 series and Qwen-2.5-Math-1.5B. It is optimized for efficiency while retaining strong reasoning capabilities, especially for mathematical and logical problem solving. The model is distributed under the **Apache-2.0 license**.  \n",
    "ðŸ”— Model card: [deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B on Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)\n",
    "\n",
    "---\n",
    "\n",
    "**Key Specifications**\n",
    "\n",
    "| Feature | Details |\n",
    "|---|---|\n",
    "| **Parameters** | ~1.5 billion |\n",
    "| **Architecture** | Transformer; distilled from DeepSeek-R1 outputs on top of Qwen-2.5-Math-1.5B |\n",
    "| **Input / Output** | Text-in / Text-out |\n",
    "| **Context Length** | Up to **131 K tokens** |\n",
    "| **Optimization** | Distilled for compact size while preserving reasoning quality |\n",
    "| **License** | Apache-2.0 |\n",
    "\n",
    "---\n",
    "\n",
    "**Benchmarks & Behavior**\n",
    "\n",
    "- Strong performance on **mathematical reasoning and competition benchmarks** such as AIME and MATH-500.  \n",
    "- Efficient deployment for environments with **limited compute or memory resources**.  \n",
    "- Retains much of the reasoning strength of larger DeepSeek models while being faster and cheaper to run.  \n",
    "\n",
    "---\n",
    "\n",
    "**Using This Notebook**\n",
    "\n",
    "Hereâ€™s what youâ€™ll cover:\n",
    "\n",
    "* Load a sample dataset from Hugging Face and prepare it for fine-tuning  \n",
    "* Fine-tune with SageMaker Training Jobs  \n",
    "* Run Model Evaluation  \n",
    "* Deploy to SageMaker Endpoints  \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce40054-610a-4acc-a546-943893f293c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b791e72e-82b5-4f8e-a7fe-700e8afdeba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c673a9-5b9f-47e0-92cf-bb97486b3e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "\n",
    "from sagemaker.local import LocalSession \n",
    "sess = LocalSession() #sagemaker.Session(boto3.Session(region_name=region))\n",
    "sess.config = {\"local\": {\"local_code\": True}}\n",
    "\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = \"arn:aws:iam::111111111111:role/service-role/AmazonSageMaker-ExecutionRole-20200101T000001\" #sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9786901-b012-41ff-a98a-b16e5f60ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b2bf05-a59f-43f5-ad2a-8279d3ab8f1c",
   "metadata": {},
   "source": [
    "## Data Preparation for Supervised Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad12e3-c97a-4fde-a16e-3db093b318c1",
   "metadata": {},
   "source": [
    "### [NuminaMath-CoT](https://huggingface.co/datasets/AI-MO/NuminaMath-CoT)\n",
    "\n",
    "**NuminaMath-CoT** is a large-scale dataset of **~860,000+ math competition question-solution pairs**, designed to support chain-of-thought reasoning in mathematical problem solving.\n",
    "\n",
    "**Data Format & Structure**:\n",
    "- Each example is a question followed by a solution; the solution is formatted with detailed **Chain-of-Thought (CoT)** reasoning.  \n",
    "- The data sources include *Chinese high school math exercises*, *US and international mathematics competition problems*, *online test-papers PDFs*, and *math discussion forums*.  \n",
    "- Preprocessing includes OCR from PDFs, segmentation to extract problem-solution pairs, translation into English, alignment into CoT style, and formatting of final answers.  \n",
    "\n",
    "**License**: Released under the **Apache-2.0** license.  \n",
    "\n",
    "**Applications**:\n",
    "\n",
    "This dataset is useful for training and evaluating models on tasks including:  \n",
    "- Complex math problem solving with reasoning steps (algebra, geometry, number theory, etc.)  \n",
    "- Benchmarking chain-of-thought performance of LLMs on competition-level math tasks  \n",
    "- Educational tools and tutoring systems that require explainable math solutions  \n",
    "- Fine-tuning models to improve consistency, reasoning depth, and accuracy in mathematical domains  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd6608-a09f-4521-8ac8-36ec94cc76b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a6564-4a41-4d05-a550-e43784cb2901",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_parent_path = os.path.join(os.getcwd(), \"tmp_cache_local_dataset\")\n",
    "os.makedirs(dataset_parent_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d27052-23fd-4a8b-863b-06b325be7502",
   "metadata": {},
   "source": [
    "**Preparing Your Dataset in `messages` format**\n",
    "\n",
    "This section walks you through creating a conversation-style datasetâ€”the required `messages` formatâ€”for directly training LLMs using SageMaker AI.\n",
    "\n",
    "**What Is the `messages` Format?**\n",
    "\n",
    "The `messages` format structures instances as chat-like exchanges, wrapping each conversation turn into a role-labeled JSON array. Itâ€™s widely used by frameworks like TRL.\n",
    "\n",
    "Example entry:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "    { \"role\": \"user\", \"content\": \"How do I bake sourdough?\" },\n",
    "    { \"role\": \"assistant\", \"content\": \"First, you need to create a starter by...\" }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c962155-197b-447a-99ad-de6642153d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"AI-MO/NuminaMath-CoT\"\n",
    "dataset = load_dataset(dataset_name, split=\"train[:1000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c87fc-597c-4ee7-9181-27c9a101299e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint.pp(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48951960-8766-4f89-9e44-ae736f370ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total number of fine-tunable samples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889d18f-a156-4b0e-975f-51684421aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_messages_reasoning(row):\n",
    "    system_content = \"You are a mathematical reasoning assistant. Read the problem, restate the key givens and goal, then solve step-by-step with clear algebra (use LaTeX), keeping exact arithmetic (fractions/surds) and justifying each transformation (e.g., equal differences for arithmetic sequences). Verify any domain or extraneous-solution constraints, and present the final simplified answer concisely on the last line.\"\n",
    "    \n",
    "    messages_user_row = row[\"messages\"][0]\n",
    "    assert messages_user_row[\"role\"] == \"user\", f\"user row unmatched\"\n",
    "    user_content = messages_user_row[\"content\"]\n",
    "    \n",
    "    messages_assistant_row = row[\"messages\"][1]\n",
    "    assert messages_assistant_row[\"role\"] == \"assistant\", f\"assistant row unmatched\"\n",
    "    assistant_content = messages_assistant_row[\"content\"]\n",
    "\n",
    "    think_block = f\"<think>{row['solution']}</think>\"\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            { \"role\": \"system\", \"content\": system_content},\n",
    "            { \"role\": \"user\", \"content\": user_content },\n",
    "            { \"role\": \"assistant\", \"content\": f\"{think_block}\\n\\n{assistant_content}\" }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    \n",
    "dataset = dataset.map(convert_to_messages_reasoning, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f470cad1-5bd4-4c8c-89df-d73fff34b06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filename = os.path.join(dataset_parent_path, f\"{dataset_name.replace('/', '--').replace('.', '-')}.jsonl\")\n",
    "dataset.to_json(dataset_filename, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53373d8c-1ddf-4575-a26d-11d7f7df3a90",
   "metadata": {},
   "source": [
    "#### Upload file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2c2989-d283-4305-afb7-a7a04a5b60b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9455282f-5330-41e2-a020-b94a60d1a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s3_uri = f\"s3://{sess.default_bucket()}/dataset\"\n",
    "\n",
    "uploaded_s3_uri = S3Uploader.upload(\n",
    "    local_path=dataset_filename,\n",
    "    desired_s3_uri=data_s3_uri\n",
    ")\n",
    "print(f\"Uploaded {dataset_filename} to > {uploaded_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc65e320-cb5c-4cce-8d71-ff7152ac9f95",
   "metadata": {},
   "source": [
    "## Fine-Tune LLMs using SageMaker `Estimator`/`ModelTrainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c12ae-a6d4-4225-af55-89776bf09cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from getpass import getpass\n",
    "import yaml\n",
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773fca02-88bc-4d1b-a44e-2afe267fa00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a530ca5-253a-4202-a144-7203a207b8d2",
   "metadata": {},
   "source": [
    "### Training using `PyTorch` Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9491c07-723f-4fa6-b9d2-f5e8c4cf2fe6",
   "metadata": {},
   "source": [
    "**Training Using `PyTorch` Estimator**\n",
    "Leverages the official PyTorch SageMaker container to run a custom training script using the Accelerate and DeepSpeed libraries. This option is ideal for users who want full control over the training pipeline "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ed21b81-1747-4646-9c93-52e402141165",
   "metadata": {},
   "source": [
    "---\n",
    "**Observability**: SageMaker AI has [SageMaker MLflow](https://docs.aws.amazon.com/sagemaker/latest/dg/mlflow.html) which enables you to accelerate generative AI by making it easier to track experiments and monitor performance of models and AI applications using a single tool.\n",
    "\n",
    "You can choose to include MLflow as a part of your training workflow to track your model fine-tuning metrics in realtime by simply specifying a **mlflow** tracking arn.\n",
    "\n",
    "Optionally you can also report to : **tensorboard**, **wandb**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182e8ef-1cb0-4ca7-89d9-0ab229991d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_TRACKING_SERVER_ARN = None # or \"arn:aws:sagemaker:us-west-2:<account-id>:mlflow-tracking-server/<server-name>\"\n",
    "\n",
    "if MLFLOW_TRACKING_SERVER_ARN:\n",
    "    reports_to = \"mlflow\"\n",
    "else:\n",
    "    reports_to = \"tensorboard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f210f-75ba-4e0f-afcb-08df925f6b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 'deepseek-ai--DeepSeek-R1-Distill-Qwen-1-5B'\n",
    "training_instance_type = \"local_gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dff319d-f05e-44c2-8067-3a67b5f8fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MLFLOW_TRACKING_SERVER_ARN:\n",
    "    training_env = {\n",
    "        \"MLFLOW_EXPERIMENT_NAME\": f\"exp-{job_name}\",\n",
    "        \"MLFLOW_TAGS\": '{\"source.job\": \"sm-training-jobs\", \"source.type\": \"sft\", \"source.framework\": \"pytorch\"}',\n",
    "        \"HF_TOKEN\": hf_token,\n",
    "        \"MLFLOW_TRACKING_URI\": MLFLOW_TRACKING_SERVER_ARN,\n",
    "    }\n",
    "else:\n",
    "    training_env = {\n",
    "        \"HF_TOKEN\": hf_token\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480ab403-44f0-4506-86b1-e8e7461e6045",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training:2.8.0-gpu-py312-cu129-ubuntu22.04-sagemaker\"\n",
    "print(f\"Using image: {pytorch_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaee824-bf6a-410f-bb35-228e8845b9ae",
   "metadata": {},
   "source": [
    "#### Training strategy: `PeFT/LoRA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad8abf8-fe48-4285-a460-41f034473383",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_estimator = PyTorch(\n",
    "    image_uri=pytorch_image_uri,\n",
    "    entry_point=\"sm_accelerate_train.sh\", # Adapted bash script to train using accelerate on SageMaker - Multi-GPU\n",
    "    source_dir=\"sagemaker_code\",\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"{job_name}-pytorch\",\n",
    "    role=role,\n",
    "    volume_size=300,\n",
    "    py_version=\"py312\",\n",
    "    keep_alive_period_in_seconds=3600,\n",
    "    environment=training_env,\n",
    "    sagemaker_session=sess,\n",
    "    hyperparameters={\n",
    "        \"config\": \"hf_recipes/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B--vanilla-peft-qlora.yaml\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# fit or train\n",
    "pytorch_estimator.fit(\n",
    "    {\"training\": uploaded_s3_uri}, \n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8fdb5-da51-444b-9b84-f56a88b62c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_data_uri = pytorch_estimator.model_data\n",
    "print(f\"Fine-tuned model location: {s3_model_data_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db88465-a55e-47c1-ab34-0ed62a49b2d9",
   "metadata": {},
   "source": [
    "#### Training strategy: `Spectrum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d90cec-704a-48e8-9e4b-62ca5089ac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_estimator = PyTorch(\n",
    "    image_uri=pytorch_image_uri,\n",
    "    entry_point=\"sm_accelerate_train.sh\", # Adapted bash script to train using accelerate on SageMaker - Multi-GPU\n",
    "    source_dir=\"sagemaker_code\",\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"{job_name}-pytorch\",\n",
    "    role=role,\n",
    "    volume_size=300,\n",
    "    py_version=\"py312\",\n",
    "    keep_alive_period_in_seconds=3600,\n",
    "    environment=training_env,\n",
    "    sagemaker_session=sess,\n",
    "    hyperparameters={\n",
    "        \"config\": \"hf_recipes/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B--vanilla-spectrum.yaml\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# fit or train\n",
    "pytorch_estimator.fit(\n",
    "    {\"training\": uploaded_s3_uri}, \n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ef234-af18-4962-821e-f868eef75a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_data_uri = pytorch_estimator.model_data\n",
    "print(f\"Fine-tuned model location: {s3_model_data_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fbad3c-11d4-4ed5-9379-3b2ec109c791",
   "metadata": {},
   "source": [
    "#### Training strategy: `Full-Finetuning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c620145b-7da7-4b4b-85c6-2a9cfd53fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_estimator = PyTorch(\n",
    "    image_uri=pytorch_image_uri,\n",
    "    entry_point=\"sm_accelerate_train.sh\", # Adapted bash script to train using accelerate on SageMaker - Multi-GPU\n",
    "    source_dir=\"sagemaker_code\",\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"{job_name}-pytorch\",\n",
    "    role=role,\n",
    "    volume_size=300,\n",
    "    py_version=\"py312\",\n",
    "    keep_alive_period_in_seconds=3600,\n",
    "    environment=training_env,\n",
    "    sagemaker_session=sess,\n",
    "    hyperparameters={\n",
    "        \"config\": \"hf_recipes/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B--vanilla-full.yaml\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# fit or train\n",
    "pytorch_estimator.fit(\n",
    "    {\"training\": uploaded_s3_uri}, \n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9cdc4d-b5cf-471d-9727-dd0a1827696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_data_uri = pytorch_estimator.model_data\n",
    "print(f\"Fine-tuned model location: {s3_model_data_uri}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
