{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60b91a9-7d9c-4c45-be0f-5a224581f644",
   "metadata": {},
   "source": [
    "# ðŸš€ Customize and Deploy `Qwen/Qwen2-Audio-7B-Instruct` on Amazon SageMaker AI\n",
    "---\n",
    "In this notebook, we explore **Qwen2-Audio-7B-Instruct**, a 7-billion-parameter audio-language model from Alibabaâ€™s Qwen2-Audio series. Youâ€™ll learn how to fine-tune it (or use it out-of-the-box), evaluate its audio + text understanding, and deploy it using SageMaker for voice interaction, audio analysis, and multimodal workflows.\n",
    "\n",
    "**What is Qwen2-Audio-7B-Instruct?**\n",
    "\n",
    "Qwen2-Audio-7B-Instruct is the instruction-tuned variant of the Qwen2-Audio-7B model. It can take **audio inputs** (speech, other sound signals) and **text instructions** to perform tasks such as transcription, translation, conversational voice chat, audio analysis, and sound-event detection. It supports two primary audio interaction modes: *voice chat* (audio in, audio/text out conversationally) and *audio analysis* (audio + text instructions to analyze or respond). It is released under the **Apache-2.0 license**.  \n",
    "ðŸ”— Model card: [Qwen/Qwen2-Audio-7B-Instruct on Hugging Face](https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct)\n",
    "\n",
    "---\n",
    "\n",
    "**Key Specifications**â‰ˆ\n",
    "\n",
    "| Feature | Details |\n",
    "|---|---|\n",
    "| **Parameters** | ~7 billion |\n",
    "| **Modalities** | Audio input + Text input â†’ Text output |\n",
    "| **Interaction Modes** | Voice chat; Audio analysis (audio + instruction) |\n",
    "| **Context / Window Length** | ~32,768 tokens for text; audio input support for various sampling rates |\n",
    "| **Languages Supported** | English and Chinese (at minimum) |\n",
    "| **License** | Apache-2.0 |\n",
    "\n",
    "---\n",
    "\n",
    "**Benchmarks & Behavior**\n",
    "\n",
    "- Qwen2-Audio has been evaluated across tasks like automatic speech recognition (ASR), speech-to-text translation, emotion recognition, and sound event classification, showing state-of-the-art or competitive performance.  \n",
    "- It is designed to understand complex audio environments (multi-speaker or noisy input) and respond appropriately to natural instructions.  \n",
    "- Strong fit for applications in **multilingual speech agents, customer service bots, transcription services, and audio analytics pipelines**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce40054-610a-4acc-a546-943893f293c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker \"datasets[audio]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b791e72e-82b5-4f8e-a7fe-700e8afdeba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c673a9-5b9f-47e0-92cf-bb97486b3e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "\n",
    "sess = sagemaker.Session(boto3.Session(region_name=region))\n",
    "\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9786901-b012-41ff-a98a-b16e5f60ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b2bf05-a59f-43f5-ad2a-8279d3ab8f1c",
   "metadata": {},
   "source": [
    "## Data Preparation for Supervised Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad12e3-c97a-4fde-a16e-3db093b318c1",
   "metadata": {},
   "source": [
    "### [AudioSet-Audio-Instructions](https://huggingface.co/datasets/mesolitica/AudioSet-Audio-Instructions)\n",
    "\n",
    "**AudioSet-Audio-Instructions** is a multimodal dataset created by pairing **Google AudioSet audio clips** with human-written instructionâ€“response examples. It is designed to support **instruction tuning for audio understanding** tasks such as event detection, audio classification, and acoustic reasoning.\n",
    "\n",
    "**Data Format & Structure**:\n",
    "- Built on **AudioSet**, a large ontology of over 500 audio event classes.  \n",
    "- Each example contains:  \n",
    "  â€¢ `audio` â€” a 10-second audio clip from AudioSet.  \n",
    "  â€¢ `instruction` â€” a natural language prompt about the audio (e.g., â€œIdentify the main sound in this clipâ€).  \n",
    "  â€¢ `response` â€” the assistantâ€™s answer (e.g., â€œThis audio contains the sound of a violin being playedâ€).  \n",
    "- Distributed in **JSON/Parquet** formats for ease of use.  \n",
    "- Includes `train`, `validation`, and `test` splits.  \n",
    "\n",
    "**License**: Same as **AudioSet** (Creative Commons Attribution license) with additional generated annotations shared openly for research.  \n",
    "\n",
    "**Applications**:\n",
    "\n",
    "The dataset is useful for training and evaluating multimodal models on:  \n",
    "- Audio event classification and tagging  \n",
    "- Instruction-following tasks for sound and acoustic scenes  \n",
    "- Building conversational audio assistants  \n",
    "- Enhancing models for **multimodal reasoning** (sound + text)  \n",
    "\n",
    "---\n",
    "\n",
    "*This dataset bridges raw audio perception with natural language supervision, making it a valuable resource for instruction-tuned audio understanding models.*  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd6608-a09f-4521-8ac8-36ec94cc76b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "from datasets import load_dataset, Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a6564-4a41-4d05-a550-e43784cb2901",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_parent_path = os.path.join(os.getcwd(), \"tmp_cache_local_dataset\")\n",
    "os.makedirs(dataset_parent_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d27052-23fd-4a8b-863b-06b325be7502",
   "metadata": {},
   "source": [
    "**Preparing Your Dataset in `messages` format**\n",
    "\n",
    "This section walks you through creating a conversation-style datasetâ€”the required `messages` formatâ€”for directly training LLMs using SageMaker AI.\n",
    "\n",
    "**What Is the `messages` Format?**\n",
    "\n",
    "The `messages` format structures instances as chat-like exchanges, wrapping each conversation turn into a role-labeled JSON array. Itâ€™s widely used by frameworks like TRL.\n",
    "\n",
    "Example entry:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "    { \"role\": \"user\", \"content\": \"How do I bake sourdough?\" },\n",
    "    { \"role\": \"assistant\", \"content\": \"First, you need to create a starter by...\" }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c962155-197b-447a-99ad-de6642153d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"mesolitica/AudioSet-Audio-Instructions\"\n",
    "dataset = load_dataset(dataset_name, split=\"500k_part1_speech[:200]\") \n",
    "dataset = dataset.cast_column(\"audio_filename\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9802dfe8-7aec-44fd-a288-eceeaaf5c9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_parent_path = os.path.join(os.getcwd(), \"tmp_cache_local_dataset\")\n",
    "os.makedirs(dataset_parent_path, exist_ok=True)\n",
    "audio_dir = os.path.join(dataset_parent_path, \"audio_files\")\n",
    "\n",
    "# final dataset file path\n",
    "dataset_filename = os.path.join(\n",
    "    dataset_parent_path,\n",
    "    f\"{dataset_name.replace('/', '--').replace('.', '-')}.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c87fc-597c-4ee7-9181-27c9a101299e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint.pp(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48951960-8766-4f89-9e44-ae736f370ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total number of fine-tunable samples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889d18f-a156-4b0e-975f-51684421aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_messages_audio(row, idx):\n",
    "    os.makedirs(audio_dir, exist_ok=True)\n",
    "\n",
    "    # decode audio\n",
    "    audio = row[\"audio_filename\"]\n",
    "    array, sr = audio[\"array\"], audio[\"sampling_rate\"]\n",
    "\n",
    "    # choose filename\n",
    "    key = f\"sample-{idx:06d}\"\n",
    "    filename = f\"{key}.wav\"\n",
    "    filepath = os.path.join(audio_dir, filename)\n",
    "\n",
    "    # save wav\n",
    "    sf.write(filepath, array, sr)\n",
    "\n",
    "    # system instruction\n",
    "    system_content = (\n",
    "        \"You are an audio understanding assistant. Listen carefully to the audio clip, \"\n",
    "        \"analyze the sounds, and provide a clear and concise description.\"\n",
    "    )\n",
    "\n",
    "    # messages schema (all content fields are lists of dicts)\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": system_content}\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"audio\",\n",
    "                        \"audio_url\": f\"file:///opt/ml/input/data/training/{os.path.relpath(filepath, dataset_parent_path)}\"\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": row[\"question\"]},\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": row[\"answer\"]}\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "with open(dataset_filename, \"w\") as f: \n",
    "    for idx, row in tqdm(enumerate(dataset), total=len(dataset)): \n",
    "        processed = convert_to_messages_audio(row, idx) \n",
    "        f.write(json.dumps(processed) + \"\\n\") \n",
    "        \n",
    "print(f\"âœ… Saved JSONL dataset to {dataset_filename}\") \n",
    "print(f\"âœ… Audio files saved under {audio_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53373d8c-1ddf-4575-a26d-11d7f7df3a90",
   "metadata": {},
   "source": [
    "#### Upload file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2c2989-d283-4305-afb7-a7a04a5b60b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9455282f-5330-41e2-a020-b94a60d1a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s3_uri = f\"s3://{sess.default_bucket()}/dataset\"\n",
    "\n",
    "uploaded_s3_uri = S3Uploader.upload(\n",
    "    local_path=dataset_filename,\n",
    "    desired_s3_uri=data_s3_uri\n",
    ")\n",
    "print(f\"Uploaded {dataset_filename} to > {uploaded_s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb233a03-408b-46e9-a7ca-7441723c4852",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s3_uri = f\"s3://{sess.default_bucket()}/dataset\"\n",
    "\n",
    "uploaded_audio_s3_uri = S3Uploader.upload(\n",
    "    local_path=os.path.join(os.path.dirname(dataset_filename), \"audio_files\"),\n",
    "    desired_s3_uri=data_s3_uri\n",
    ")\n",
    "\n",
    "print(f\"Uploaded {dataset_filename} to > {uploaded_audio_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc65e320-cb5c-4cce-8d71-ff7152ac9f95",
   "metadata": {},
   "source": [
    "## Fine-Tune LLMs using SageMaker `Estimator`/`ModelTrainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c12ae-a6d4-4225-af55-89776bf09cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.modules.configs import (\n",
    "    CheckpointConfig,\n",
    "    Compute,\n",
    "    OutputDataConfig,\n",
    "    SourceCode,\n",
    "    StoppingCondition,\n",
    ")\n",
    "from sagemaker.modules.configs import InputData\n",
    "from sagemaker.modules.train import ModelTrainer\n",
    "from getpass import getpass\n",
    "import yaml\n",
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bb3f80-313a-453f-b21d-750ec917978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"Qwen/Qwen2-Audio-7B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773fca02-88bc-4d1b-a44e-2afe267fa00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a530ca5-253a-4202-a144-7203a207b8d2",
   "metadata": {},
   "source": [
    "### Training using `PyTorch` `ModelTrainer`\n",
    "---\n",
    "**Observability**: SageMaker AI has [SageMaker MLflow](https://docs.aws.amazon.com/sagemaker/latest/dg/mlflow.html) which enables you to accelerate generative AI by making it easier to track experiments and monitor performance of models and AI applications using a single tool.\n",
    "\n",
    "You can choose to include MLflow as a part of your training workflow to track your model fine-tuning metrics in realtime by simply specifying a **mlflow** tracking arn.\n",
    "\n",
    "Optionally you can also report to : **tensorboard**, **wandb**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182e8ef-1cb0-4ca7-89d9-0ab229991d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_TRACKING_SERVER_ARN = \"arn:aws:sagemaker:us-east-1:811828458885:mlflow-tracking-server/mlflow-demos\"\n",
    "\n",
    "if MLFLOW_TRACKING_SERVER_ARN:\n",
    "    reports_to = \"mlflow\"\n",
    "else:\n",
    "    reports_to = \"tensorboard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f210f-75ba-4e0f-afcb-08df925f6b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = MODEL_ID.replace('/', '--').replace('.', '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dff319d-f05e-44c2-8067-3a67b5f8fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MLFLOW_TRACKING_SERVER_ARN:\n",
    "    training_env = {\n",
    "        # mlflow tracking metrics\n",
    "        \"MLFLOW_EXPERIMENT_NAME\": f\"{job_name}-exp\",\n",
    "        \"MLFLOW_TAGS\": json.dumps(\n",
    "            {\n",
    "                \"source.job\": \"sm-training-jobs\", \n",
    "                \"source.type\": \"sft\", \n",
    "                \"source.framework\": \"pytorch\"\n",
    "            }\n",
    "        ),\n",
    "        \"MLFLOW_TRACKING_URI\": MLFLOW_TRACKING_SERVER_ARN,\n",
    "        \"MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING\": \"true\",\n",
    "        # non tracking metrics - enabled\n",
    "        \"HF_TOKEN\": hf_token,\n",
    "        \"FI_EFA_USE_DEVICE_RDMA\": \"1\",\n",
    "        \"NCCL_DEBUG\": \"INFO\",\n",
    "        \"NCCL_SOCKET_IFNAME\": \"eth0\",\n",
    "        \"FI_PROVIDER\": \"efa\",\n",
    "        \"NCCL_PROTO\": \"simple\",\n",
    "        \"NCCL_NET_GDR_LEVEL\": \"5\"\n",
    "    }\n",
    "else:\n",
    "    training_env = {\n",
    "        # non tracking metrics\n",
    "        \"HF_TOKEN\": hf_token,\n",
    "        \"FI_EFA_USE_DEVICE_RDMA\": \"1\",\n",
    "        \"NCCL_DEBUG\": \"INFO\",\n",
    "        \"NCCL_SOCKET_IFNAME\": \"eth0\",\n",
    "        \"FI_PROVIDER\": \"efa\",\n",
    "        \"NCCL_PROTO\": \"simple\",\n",
    "        \"NCCL_NET_GDR_LEVEL\": \"5\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7fa5fa-d826-4c8c-a4ce-c121004c6d8b",
   "metadata": {},
   "source": [
    "#### Training strategy - Choose between: `PeFT`/`Spectrum`/`Full-Finetuning`\n",
    "\n",
    "Here we create a measured mapping of strategy to instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eea05c-dd1c-4ad8-aec5-50895ca45bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sagemaker_code/requirements.txt\n",
    "transformers==4.57.0\n",
    "peft==0.17.0\n",
    "accelerate==1.11.0\n",
    "bitsandbytes==0.46.1\n",
    "datasets==4.0.0\n",
    "deepspeed==0.17.5\n",
    "hf-transfer==0.1.8\n",
    "hf_xet\n",
    "liger-kernel==0.6.1\n",
    "lm-eval[api]==0.4.9\n",
    "kernels>=0.9.0\n",
    "mlflow\n",
    "Pillow\n",
    "safetensors>=0.6.2\n",
    "sagemaker==2.251.1\n",
    "sagemaker-mlflow==0.1.0\n",
    "sentencepiece==0.2.0\n",
    "tokenizers>=0.21.4\n",
    "triton\n",
    "trl==0.21.0\n",
    "tensorboard\n",
    "psutil\n",
    "py7zr\n",
    "git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels\n",
    "vllm==0.10.1\n",
    "poetry\n",
    "yq\n",
    "psutil\n",
    "nvidia-ml-py\n",
    "pyrsmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d0f7cf-82a5-4e55-8b8d-e0ca3bbdb82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PeFT\n",
    "args = [\n",
    "    \"--config\",\n",
    "    \"hf_recipes/Qwen/Qwen2-Audio-7B-Instruct-vanilla-peft-qlora.yaml\",\n",
    "    # \"--run-eval\" # enable this for small models to run eval + tune\n",
    "]\n",
    "training_instance_type = \"ml.g6e.2xlarge\"\n",
    "training_instance_count = 1\n",
    "\n",
    "## For Full-Finetuning\n",
    "# args = [\n",
    "#     \"--config\",\n",
    "#     \"hf_recipes/Qwen/Qwen2-Audio-7B-Instruct-vanilla-full.yaml\",\n",
    "#     # \"--run-eval\" # enable this for small models if you're looking to bundle eval with fine-tuning\n",
    "# ]\n",
    "# training_instance_type = \"ml.g6e.4xlarge\"\n",
    "# training_instance_count = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8d8a73-c41a-46ca-b144-8dcaf0c99a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=sess.boto_session.region_name,\n",
    "    version=\"2.7.1\",\n",
    "    instance_type=training_instance_type,\n",
    "    image_scope=\"training\",\n",
    ")\n",
    "print(f\"Using image: {pytorch_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad8abf8-fe48-4285-a460-41f034473383",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_code = SourceCode(\n",
    "    source_dir=\"./sagemaker_code\",\n",
    "    command=f\"bash sm_accelerate_train.sh {' '.join(args)}\",\n",
    ")\n",
    "\n",
    "compute_configs = Compute(\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=training_instance_count,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    volume_size_in_gb=300\n",
    ")\n",
    "\n",
    "base_job_name = f\"{job_name}-finetune\"\n",
    "output_path = f\"s3://{sess.default_bucket()}/{base_job_name}\"\n",
    "\n",
    "model_trainer = ModelTrainer(\n",
    "    training_image=pytorch_image_uri,\n",
    "    source_code=source_code,\n",
    "    base_job_name=base_job_name,\n",
    "    compute=compute_configs,\n",
    "    stopping_condition=StoppingCondition(max_runtime_in_seconds=18000),\n",
    "    output_data_config=OutputDataConfig(\n",
    "        s3_output_path=output_path,\n",
    "    ),\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        s3_uri=output_path + dataset_name.replace('/', '--').replace('.', '-') + \"/checkpoints\", local_path=\"/opt/ml/checkpoints\"\n",
    "    ),\n",
    "    role=role,\n",
    "    environment=training_env\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98df5c04-80e1-4e1f-af74-044bad6990de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.train(\n",
    "    input_data_config=[\n",
    "        InputData(\n",
    "            channel_name=\"training\",\n",
    "            data_source=uploaded_s3_uri,  \n",
    "        )\n",
    "    ], \n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52f0085-6635-46a3-a6a3-ae5ec42598df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
