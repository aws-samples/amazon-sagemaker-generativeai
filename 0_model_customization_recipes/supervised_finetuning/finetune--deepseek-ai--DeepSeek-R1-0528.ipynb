{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60b91a9-7d9c-4c45-be0f-5a224581f644",
   "metadata": {},
   "source": [
    "# ðŸš€ Customize and Deploy `deepseek-ai/DeepSeek-R1-0528` on Amazon SageMaker AI\n",
    "\n",
    "In this notebook, we explore **DeepSeek-R1-0528**, a cutting-edge reasoning model from DeepSeek AI. You'll learn how to fine-tune it on reasoning datasets, evaluate its mathematical and logical capabilities, and deploy it using SageMaker for advanced reasoning tasks.\n",
    "\n",
    "## What is DeepSeek-R1-0528?\n",
    "DeepSeek-R1-0528 is part of DeepSeek's R1 series, specifically designed for advanced reasoning capabilities. This model represents a significant advancement in AI reasoning, combining deep learning with sophisticated reasoning mechanisms to tackle complex mathematical, logical, and analytical problems. It builds upon DeepSeek's expertise in creating efficient and powerful language models.  \n",
    "ðŸ”— Model card: [deepseek-ai/DeepSeek-R1-0528 on Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-R1-0528)\n",
    "\n",
    "## Key Specifications\n",
    "| Feature | Details |\n",
    "|---|---|\n",
    "| **Parameters** | Multi-billion parameter architecture optimized for reasoning |\n",
    "| **Architecture** | Advanced Transformer with specialized reasoning modules |\n",
    "| **Context Length** | Extended context window for complex reasoning chains |\n",
    "| **Modalities** | Text-in / Text-out with focus on reasoning tasks |\n",
    "| **License** | Check model card for specific licensing terms |\n",
    "| **Release Date** | May 28th release (0528) |\n",
    "\n",
    "## Benchmarks & Behavior\n",
    "- Exceptional performance on **mathematical reasoning, logical inference, and complex problem-solving** benchmarks.  \n",
    "- Designed to excel at **multi-step reasoning tasks** with clear chain-of-thought capabilities.  \n",
    "- Strong performance on competition mathematics, coding challenges, and analytical reasoning tasks.  \n",
    "- Optimized for **step-by-step problem decomposition** and systematic solution approaches.\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce40054-610a-4acc-a546-943893f293c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b791e72e-82b5-4f8e-a7fe-700e8afdeba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c673a9-5b9f-47e0-92cf-bb97486b3e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "\n",
    "sess = sagemaker.Session(boto3.Session(region_name=region))\n",
    "\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9786901-b012-41ff-a98a-b16e5f60ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b2bf05-a59f-43f5-ad2a-8279d3ab8f1c",
   "metadata": {},
   "source": [
    "## Data Preparation for Supervised Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad12e3-c97a-4fde-a16e-3db093b318c1",
   "metadata": {},
   "source": [
    "### [NuminaMath-CoT](https://huggingface.co/datasets/AI-MO/NuminaMath-CoT)\n",
    "\n",
    "**NuminaMath-CoT** is a large-scale dataset of **~860,000+ math competition question-solution pairs**, designed to support chain-of-thought reasoning in mathematical problem solving.\n",
    "\n",
    "**Data Format & Structure**:\n",
    "- Each example is a question followed by a solution; the solution is formatted with detailed **Chain-of-Thought (CoT)** reasoning.  \n",
    "- The data sources include *Chinese high school math exercises*, *US and international mathematics competition problems*, *online test-papers PDFs*, and *math discussion forums*.  \n",
    "- Preprocessing includes OCR from PDFs, segmentation to extract problem-solution pairs, translation into English, alignment into CoT style, and formatting of final answers.  \n",
    "\n",
    "**License**: Released under the **Apache-2.0** license.  \n",
    "\n",
    "**Applications**:\n",
    "\n",
    "This dataset is useful for training and evaluating models on tasks including:  \n",
    "- Complex math problem solving with reasoning steps (algebra, geometry, number theory, etc.)  \n",
    "- Benchmarking chain-of-thought performance of LLMs on competition-level math tasks  \n",
    "- Educational tools and tutoring systems that require explainable math solutions  \n",
    "- Fine-tuning models to improve consistency, reasoning depth, and accuracy in mathematical domains  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd6608-a09f-4521-8ac8-36ec94cc76b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a6564-4a41-4d05-a550-e43784cb2901",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_parent_path = os.path.join(os.getcwd(), \"tmp_cache_local_dataset\")\n",
    "os.makedirs(dataset_parent_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d27052-23fd-4a8b-863b-06b325be7502",
   "metadata": {},
   "source": [
    "**Preparing Your Dataset in `messages` format**\n",
    "\n",
    "This section walks you through creating a conversation-style datasetâ€”the required `messages` formatâ€”for directly training LLMs using SageMaker AI.\n",
    "\n",
    "**What Is the `messages` Format?**\n",
    "\n",
    "The `messages` format structures instances as chat-like exchanges, wrapping each conversation turn into a role-labeled JSON array. Itâ€™s widely used by frameworks like TRL.\n",
    "\n",
    "Example entry:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "    { \"role\": \"user\", \"content\": \"How do I bake sourdough?\" },\n",
    "    { \"role\": \"assistant\", \"content\": \"First, you need to create a starter by...\" }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c962155-197b-447a-99ad-de6642153d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"AI-MO/NuminaMath-CoT\"\n",
    "dataset = load_dataset(dataset_name, split=\"train[:1000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c87fc-597c-4ee7-9181-27c9a101299e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint.pp(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48951960-8766-4f89-9e44-ae736f370ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total number of fine-tunable samples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889d18f-a156-4b0e-975f-51684421aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_messages_reasoning(row):\n",
    "    system_content = \"You are a mathematical reasoning assistant. Read the problem, restate the key givens and goal, then solve step-by-step with clear algebra (use LaTeX), keeping exact arithmetic (fractions/surds) and justifying each transformation (e.g., equal differences for arithmetic sequences). Verify any domain or extraneous-solution constraints, and present the final simplified answer concisely on the last line.\"\n",
    "    \n",
    "    messages_user_row = row[\"messages\"][0]\n",
    "    assert messages_user_row[\"role\"] == \"user\", f\"user row unmatched\"\n",
    "    user_content = messages_user_row[\"content\"]\n",
    "    \n",
    "    messages_assistant_row = row[\"messages\"][1]\n",
    "    assert messages_assistant_row[\"role\"] == \"assistant\", f\"assistant row unmatched\"\n",
    "    assistant_content = messages_assistant_row[\"content\"]\n",
    "\n",
    "    think_block = f\"<think>{row['solution']}</think>\"\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            { \"role\": \"system\", \"content\": system_content},\n",
    "            { \"role\": \"user\", \"content\": user_content },\n",
    "            { \"role\": \"assistant\", \"content\": f\"{think_block}\\n\\n{assistant_content}\" }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    \n",
    "dataset = dataset.map(convert_to_messages_reasoning, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f470cad1-5bd4-4c8c-89df-d73fff34b06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filename = os.path.join(dataset_parent_path, f\"{dataset_name.replace('/', '--').replace('.', '-')}.jsonl\")\n",
    "dataset.to_json(dataset_filename, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53373d8c-1ddf-4575-a26d-11d7f7df3a90",
   "metadata": {},
   "source": [
    "#### Upload file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2c2989-d283-4305-afb7-a7a04a5b60b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9455282f-5330-41e2-a020-b94a60d1a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s3_uri = f\"s3://{sess.default_bucket()}/dataset\"\n",
    "\n",
    "uploaded_s3_uri = S3Uploader.upload(\n",
    "    local_path=dataset_filename,\n",
    "    desired_s3_uri=data_s3_uri\n",
    ")\n",
    "print(f\"Uploaded {dataset_filename} to > {uploaded_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc65e320-cb5c-4cce-8d71-ff7152ac9f95",
   "metadata": {},
   "source": [
    "## Fine-Tune LLMs using SageMaker AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c12ae-a6d4-4225-af55-89776bf09cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.modules.configs import (\n",
    "    CheckpointConfig,\n",
    "    Compute,\n",
    "    OutputDataConfig,\n",
    "    SourceCode,\n",
    "    StoppingCondition,\n",
    ")\n",
    "from sagemaker.modules.configs import InputData\n",
    "from sagemaker.modules.train import ModelTrainer\n",
    "from getpass import getpass\n",
    "import yaml\n",
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded5890-d48d-412b-8595-b6871afcad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"deepseek-ai/DeepSeek-R1-0528\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773fca02-88bc-4d1b-a44e-2afe267fa00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a530ca5-253a-4202-a144-7203a207b8d2",
   "metadata": {},
   "source": [
    "### Train FM using SageMaker AI `ModelTrainer`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ed21b81-1747-4646-9c93-52e402141165",
   "metadata": {},
   "source": [
    "---\n",
    "**Observability**: SageMaker AI has [SageMaker MLflow](https://docs.aws.amazon.com/sagemaker/latest/dg/mlflow.html) which enables you to accelerate generative AI by making it easier to track experiments and monitor performance of models and AI applications using a single tool.\n",
    "\n",
    "You can choose to include MLflow as a part of your training workflow to track your model fine-tuning metrics in realtime by simply specifying a **mlflow** tracking arn.\n",
    "\n",
    "Optionally you can also report to : **tensorboard**, **wandb**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182e8ef-1cb0-4ca7-89d9-0ab229991d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_TRACKING_SERVER_ARN = \"arn:aws:sagemaker:us-east-1:811828458885:mlflow-tracking-server/mlflow-demos\"\n",
    "\n",
    "if MLFLOW_TRACKING_SERVER_ARN:\n",
    "    reports_to = \"mlflow\"\n",
    "else:\n",
    "    reports_to = \"tensorboard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f210f-75ba-4e0f-afcb-08df925f6b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = MODEL_ID.replace('/', '--').replace('.', '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dff319d-f05e-44c2-8067-3a67b5f8fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MLFLOW_TRACKING_SERVER_ARN:\n",
    "    training_env = {\n",
    "        # mlflow tracking metrics\n",
    "        \"MLFLOW_EXPERIMENT_NAME\": f\"{job_name}-exp\",\n",
    "        \"MLFLOW_TAGS\": json.dumps(\n",
    "            {\n",
    "                \"source.job\": \"sm-training-jobs\", \n",
    "                \"source.type\": \"sft\", \n",
    "                \"source.framework\": \"pytorch\"\n",
    "            }\n",
    "        ),\n",
    "        \"MLFLOW_TRACKING_URI\": MLFLOW_TRACKING_SERVER_ARN,\n",
    "        \"MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING\": \"true\",\n",
    "        # non tracking metrics - enabled\n",
    "        \"HF_TOKEN\": hf_token,\n",
    "        \"FI_EFA_USE_DEVICE_RDMA\": \"1\",\n",
    "        \"NCCL_DEBUG\": \"INFO\",\n",
    "        \"NCCL_SOCKET_IFNAME\": \"eth0\",\n",
    "        \"FI_PROVIDER\": \"efa\",\n",
    "        \"NCCL_PROTO\": \"simple\",\n",
    "        \"NCCL_NET_GDR_LEVEL\": \"5\"\n",
    "    }\n",
    "else:\n",
    "    training_env = {\n",
    "        # non tracking metrics\n",
    "        \"HF_TOKEN\": hf_token,\n",
    "        \"FI_EFA_USE_DEVICE_RDMA\": \"1\",\n",
    "        \"NCCL_DEBUG\": \"INFO\",\n",
    "        \"NCCL_SOCKET_IFNAME\": \"eth0\",\n",
    "        \"FI_PROVIDER\": \"efa\",\n",
    "        \"NCCL_PROTO\": \"simple\",\n",
    "        \"NCCL_NET_GDR_LEVEL\": \"5\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaee824-bf6a-410f-bb35-228e8845b9ae",
   "metadata": {},
   "source": [
    "#### Training strategy - Choose between: `PeFT`/`Spectrum`/`Full-Finetuning`\n",
    "\n",
    "Here we create a measured mapping of strategy to instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd63690-92f3-4a15-8ae2-430b758fec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sagemaker_code/requirements.txt\n",
    "transformers==4.57.1\n",
    "peft==0.17.0\n",
    "accelerate==1.11.0\n",
    "bitsandbytes==0.46.1\n",
    "datasets==4.0.0\n",
    "deepspeed==0.17.5\n",
    "hf-transfer==0.1.8\n",
    "hf_xet\n",
    "liger-kernel==0.6.1\n",
    "lm-eval[api]==0.4.9\n",
    "kernels>=0.9.0\n",
    "mlflow\n",
    "Pillow\n",
    "safetensors>=0.6.2\n",
    "sagemaker==2.251.1\n",
    "sagemaker-mlflow==0.1.0\n",
    "sentencepiece==0.2.0\n",
    "tokenizers>=0.21.4\n",
    "triton\n",
    "trl==0.21.0\n",
    "tensorboard\n",
    "psutil\n",
    "py7zr\n",
    "git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels\n",
    "vllm==0.10.1\n",
    "poetry\n",
    "yq\n",
    "psutil\n",
    "nvidia-ml-py\n",
    "pyrsmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d9b70d-db2b-4fe1-bb01-2be6a179dc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PeFT\n",
    "args = [\n",
    "    \"--config\",\n",
    "    \"hf_recipes/deepseek-ai/DeepSeek-R1-0528--vanilla-peft-qlora.yaml\",\n",
    "    # \"--run-eval\" # enable this for small models to run eval + tune\n",
    "]\n",
    "training_instance_type = \"ml.p5en.48xlarge\"\n",
    "training_instance_count = 1\n",
    "\n",
    "## For Spectrum\n",
    "# args = [\n",
    "#     \"--config\",\n",
    "#     \"hf_recipes/deepseek-ai/DeepSeek-R1-0528--vanilla-spectrum.yaml\",\n",
    "#     # \"--run-eval\" # enable this for small models if you're looking to bundle eval with fine-tuning\n",
    "# ]\n",
    "# training_instance_type = \"ml.p5en.48xlarge\"\n",
    "# training_instance_count = 1\n",
    "\n",
    "## For Full-Finetuning\n",
    "# args = [\n",
    "#     \"--config\",\n",
    "#     \"hf_recipes/deepseek-ai/DeepSeek-R1-0528--vanilla-full.yaml\",,\n",
    "#     # \"--run-eval\" # enable this for small models if you're looking to bundle eval with fine-tuning\n",
    "# ]\n",
    "# training_instance_type = \"ml.p5en.48xlarge\"\n",
    "# training_instance_count = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4ddd9d-4a6d-4adc-b2f3-e77cef62dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=sess.boto_session.region_name,\n",
    "    version=\"2.8.0\",\n",
    "    instance_type=training_instance_type,\n",
    "    image_scope=\"training\",\n",
    ")\n",
    "print(f\"Using image: {pytorch_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845b144e-a670-4cb2-8997-d4a21a2a9b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_code = SourceCode(\n",
    "    source_dir=\"./sagemaker_code\",\n",
    "    command=f\"bash sm_accelerate_train.sh {' '.join(args)}\",\n",
    ")\n",
    "\n",
    "compute_configs = Compute(\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=training_instance_count,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    volume_size_in_gb=300\n",
    ")\n",
    "\n",
    "base_job_name = f\"{job_name}-finetune\"\n",
    "output_path = f\"s3://{sess.default_bucket()}/{base_job_name}\"\n",
    "\n",
    "model_trainer = ModelTrainer(\n",
    "    training_image=pytorch_image_uri,\n",
    "    source_code=source_code,\n",
    "    base_job_name=base_job_name,\n",
    "    compute=compute_configs,\n",
    "    stopping_condition=StoppingCondition(max_runtime_in_seconds=18000),\n",
    "    output_data_config=OutputDataConfig(\n",
    "        s3_output_path=output_path,\n",
    "    ),\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        s3_uri=os.path.join(\n",
    "            output_path,\n",
    "            dataset_name.replace('/', '--').replace('.', '-'), \n",
    "            job_name,\n",
    "            \"checkpoints\"\n",
    "        ), \n",
    "        local_path=\"/opt/ml/checkpoints\"\n",
    "    ),\n",
    "    role=role,\n",
    "    environment=training_env\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b05fc-ec91-4f7a-846d-2c3c30886222",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.train(\n",
    "    input_data_config=[\n",
    "        InputData(\n",
    "            channel_name=\"training\",\n",
    "            data_source=uploaded_s3_uri,  \n",
    "        )\n",
    "    ], \n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb1831c-16af-4796-ac21-843c74a4c94d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
