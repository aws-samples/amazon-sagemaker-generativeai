{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60b91a9-7d9c-4c45-be0f-5a224581f644",
   "metadata": {},
   "source": [
    "# ðŸš€ Customize and Deploy `meta-llama/Llama-4-Maverick-17B-128E-Instruct` on Amazon SageMaker AI\n",
    "---\n",
    "In this notebook, we explore **Llama-4-Maverick-17B-128E-Instruct**, Meta's groundbreaking multimodal model that combines vision and language understanding with expert routing capabilities. You'll learn how to fine-tune this advanced model on multimodal datasets, evaluate its vision-language performance, and deploy it using SageMaker.\n",
    "\n",
    "**What is Llama-4-Maverick-17B-128E-Instruct?**\n",
    "\n",
    "Meta's **Llama-4-Maverick-17B-128E-Instruct** represents a significant advancement in multimodal AI, featuring a 17-billion-parameter architecture with 128 expert modules (128E) that enable efficient processing of both visual and textual information. This model combines the proven Llama architecture with advanced vision capabilities and mixture-of-experts routing for optimal performance across diverse tasks.  \n",
    "ðŸ”— Model card: [meta-llama/Llama-4-Maverick-17B-128E-Instruct on Hugging Face](https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct)\n",
    "\n",
    "---\n",
    "\n",
    "**Key Specifications**\n",
    "\n",
    "| Feature | Details |\n",
    "|---|---|\n",
    "| **Total Parameters** | ~17 billion |\n",
    "| **Active Parameters** | ~2-3 billion per token (via expert routing) |\n",
    "| **Architecture** | Mixture-of-Experts Transformer with Vision Encoder |\n",
    "| **Expert Modules** | 128 expert networks (128E) with dynamic routing |\n",
    "| **Modalities** | Image + Text input â†’ Text output |\n",
    "| **Context Length** | Extended context window for complex multimodal reasoning |\n",
    "| **Vision Encoder** | Advanced vision transformer for high-resolution image processing |\n",
    "| **License** | Llama 4 Community License |\n",
    "\n",
    "---\n",
    "\n",
    "**Benchmarks & Behavior**\n",
    "\n",
    "- Llama-4-Maverick achieves **state-of-the-art performance** on multimodal benchmarks including VQA, image captioning, and visual reasoning.  \n",
    "- Exceptional **vision-language understanding** with detailed scene analysis and contextual reasoning.  \n",
    "- Advanced **instruction following** capabilities for complex multimodal tasks.  \n",
    "- Efficient **expert routing** enables high performance while maintaining computational efficiency.  \n",
    "- Strong **multilingual and multicultural** understanding across diverse visual contexts.  \n",
    "\n",
    "---\n",
    "\n",
    "**Using This Notebook**\n",
    "\n",
    "Here's what you'll cover:\n",
    "\n",
    "* Load multimodal datasets and prepare them for vision-language fine-tuning  \n",
    "* Fine-tune with SageMaker Training Jobs using MoE-optimized configurations  \n",
    "* Run Model Evaluation on vision-language benchmarks  \n",
    "* Deploy to SageMaker Endpoints for multimodal inference  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ce40054-610a-4acc-a546-943893f293c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b791e72e-82b5-4f8e-a7fe-700e8afdeba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23c673a9-5b9f-47e0-92cf-bb97486b3e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "\n",
    "from sagemaker.local import LocalSession \n",
    "sess = LocalSession() #sagemaker.Session(boto3.Session(region_name=region))\n",
    "sess.config = {\"local\": {\"local_code\": True}}\n",
    "\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9786901-b012-41ff-a98a-b16e5f60ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7551a0f0",
   "metadata": {},
   "source": [
    "## Data Preparation for Supervised Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad736a22",
   "metadata": {},
   "source": [
    "### [Visual-TableQA](https://huggingface.co/datasets/AI-4-Everyone/Visual-TableQA)\n",
    "\n",
    "**Visual-TableQA** is a large-scale benchmark for **open-domain reasoning over table images**, designed to advance research in multimodal understanding of structured data. The dataset provides high-quality synthetic questionâ€“answer pairs associated with rendered LaTeX table images, making it well-suited for training and evaluating visionâ€“language models on table reasoning tasks.\n",
    "\n",
    "**Data Format & Structure**:\n",
    "- Distributed in **JSON** format.  \n",
    "- Contains standard splits for training, validation, and testing.  \n",
    "- Each record includes:  \n",
    "  - `table_id` â€“ a unique identifier for the table  \n",
    "  - `image` â€“ rendered PNG image of the LaTeX table  \n",
    "  - `question` â€“ a natural language query about the table  \n",
    "  - `answer` â€“ the ground-truth response grounded in the table  \n",
    "\n",
    "**Dataset Quality**:\n",
    "- Questions are automatically generated and verified with reasoning-oriented LLMs.  \n",
    "- Ensures strong alignment between the table structure, visual representation, and annotated answers.  \n",
    "\n",
    "**License**: Released under the **Apache-2.0** license.  \n",
    "\n",
    "**Applications**:\n",
    "\n",
    "The dataset can support a variety of multimodal and structured reasoning tasks, including:  \n",
    "- Table-based question answering (QA)  \n",
    "- Document QA and table parsing  \n",
    "- Multimodal reasoning and visual understanding  \n",
    "- Benchmarking pipelines for table reasoning tasks  \n",
    "- Fine-tuning and evaluation of visionâ€“language models on structured visual data  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dataset_loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import base64\n",
    "import json\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c82d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_parent_path = os.path.join(os.getcwd(), \"tmp_cache_local_dataset\")\n",
    "os.makedirs(dataset_parent_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26dbf64",
   "metadata": {},
   "source": [
    "**Preparing Your Dataset in `messages` format**\n",
    "\n",
    "This section walks you through creating a conversation-style datasetâ€”the required `messages` formatâ€”for directly training LLMs using SageMaker AI.\n",
    "\n",
    "**What Is the `messages` Format?**\n",
    "\n",
    "The `messages` format structures instances as chat-like exchanges, wrapping each conversation turn into a role-labeled JSON array. Itâ€™s widely used by frameworks like TRL.\n",
    "\n",
    "Example entry:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "    { \"role\": \"user\", \"content\": \"How do I bake sourdough?\" },\n",
    "    { \"role\": \"assistant\", \"content\": \"First, you need to create a starter by...\" }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a563bb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"AI-4-Everyone/Visual-TableQA\"\n",
    "dataset = load_dataset(dataset_name, split=\"train[:1000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b76bd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pp(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a839d663",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total number of fine-tunable samples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "multimodal_format",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_to_base64(pil_img):\n",
    "    \"\"\"Convert a PIL image to base64-encoded PNG string.\"\"\"\n",
    "    buffer = io.BytesIO()\n",
    "    pil_img.save(buffer, format=\"PNG\")\n",
    "    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "def convert_to_messages_multimodal(row):\n",
    "    system_content = (\n",
    "        \"You are a multimodal reasoning assistant. Given a table (and its image if present) \"\n",
    "        \"and a question, provide a clear, concise answer followed by a brief explanation of \"\n",
    "        \"how the table supports your conclusion. Keep the reasoning grounded in the data and avoid speculation.\"\n",
    "    )\n",
    "    user_content = row[\"question\"]\n",
    "    assistant_content = row[\"answer\"]\n",
    "    image_content = row[\"image\"]\n",
    "\n",
    "    images = []\n",
    "    if image_content is not None:\n",
    "        if isinstance(image_content, list):\n",
    "            for img in image_content:\n",
    "                if hasattr(img, \"save\"):  # PIL image\n",
    "                    b64_img = pil_to_base64(img)\n",
    "                    images.append({\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/png;base64,{b64_img}\"}\n",
    "                    })\n",
    "        else:\n",
    "            if hasattr(image_content, \"save\"):  # PIL image\n",
    "                b64_img = pil_to_base64(image_content)\n",
    "                images.append({\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/png;base64,{b64_img}\"}\n",
    "                })\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_content}]},\n",
    "            {\"role\": \"user\", \"content\": images + [{\"type\": \"text\", \"text\": user_content}]},\n",
    "            {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": assistant_content}]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(convert_to_messages_multimodal, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c53a311",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filename = os.path.join(dataset_parent_path, f\"{dataset_name.replace('/', '--').replace('.', '-')}.jsonl\")\n",
    "dataset.to_json(dataset_filename, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf26801",
   "metadata": {},
   "source": [
    "#### Upload file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5ad7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8ded00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s3_uri = f\"s3://{sess.default_bucket()}/dataset\"\n",
    "\n",
    "uploaded_s3_uri = S3Uploader.upload(\n",
    "    local_path=dataset_filename,\n",
    "    desired_s3_uri=data_s3_uri\n",
    ")\n",
    "print(f\"Uploaded {dataset_filename} to > {uploaded_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_section",
   "metadata": {},
   "source": [
    "## Fine-Tune LLMs using SageMaker `Estimator`/`ModelTrainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6511fbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from getpass import getpass\n",
    "import yaml\n",
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fe785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Hugging Face token for model downloads (if needed)\n",
    "hf_token = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd624eaa",
   "metadata": {},
   "source": [
    "### Training using `PyTorch` Estimator\n",
    "\n",
    "**Training Using `PyTorch` Estimator**\n",
    "Leverages the official PyTorch SageMaker container to run a custom training script using the Accelerate and DeepSpeed libraries. This option is ideal for users who want full control over the training pipeline \n",
    "\n",
    "---\n",
    "**Observability**: SageMaker AI has [SageMaker MLflow](https://docs.aws.amazon.com/sagemaker/latest/dg/mlflow.html) which enables you to accelerate generative AI by making it easier to track experiments and monitor performance of models and AI applications using a single tool.\n",
    "\n",
    "You can choose to include MLflow as a part of your training workflow to track your model fine-tuning metrics in realtime by simply specifying a **mlflow** tracking arn.\n",
    "\n",
    "Optionally you can also report to : **tensorboard**, **wandb**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "estimator_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_TRACKING_SERVER_ARN = None # or \"arn:aws:sagemaker:us-west-2:<account-id>:mlflow-tracking-server/<server-name>\"\n",
    "\n",
    "if MLFLOW_TRACKING_SERVER_ARN:\n",
    "    reports_to = \"mlflow\"\n",
    "else:\n",
    "    reports_to = \"tensorboard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a4830",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 'meta-llama--Llama-3.2-11B-Instruct'\n",
    "training_instance_type = \"local_gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f593d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MLFLOW_TRACKING_SERVER_ARN:\n",
    "    training_env = {\n",
    "        \"MLFLOW_EXPERIMENT_NAME\": f\"exp-{job_name}\",\n",
    "        \"MLFLOW_TAGS\": '{\"source.job\": \"sm-training-jobs\", \"source.type\": \"sft\", \"source.framework\": \"pytorch\"}',\n",
    "        \"HF_TOKEN\": hf_token,\n",
    "        \"MLFLOW_TRACKING_URI\": MLFLOW_TRACKING_SERVER_ARN,\n",
    "    }\n",
    "else:\n",
    "    training_env = {\n",
    "        \"HF_TOKEN\": hf_token\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2801deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training:2.8.0-gpu-py312-cu129-ubuntu22.04-sagemaker\"\n",
    "print(f\"Using image: {pytorch_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb0dae4",
   "metadata": {},
   "source": [
    "#### Training strategy: `PeFT/LoRA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcea368",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_estimator = PyTorch(\n",
    "    image_uri=pytorch_image_uri,\n",
    "    entry_point=\"sm_accelerate_train.sh\", # Adapted bash script to train using accelerate on SageMaker - Multi-GPU\n",
    "    source_dir=\"sagemaker_code\",\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"{job_name}-pytorch\",\n",
    "    role=role,\n",
    "    volume_size=300,\n",
    "    py_version=\"py312\",\n",
    "    keep_alive_period_in_seconds=3600,\n",
    "    environment=training_env,\n",
    "    sagemaker_session=sess,\n",
    "    hyperparameters={\n",
    "        \"config\": \"hf_recipes/meta-llama/Llama-4-Maverick-17B-128E-Instruct--vanilla-peft-qlora.yaml\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# fit or train\n",
    "pytorch_estimator.fit(\n",
    "    {\"training\": uploaded_s3_uri}, \n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7531b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_data_uri = pytorch_estimator.model_data\n",
    "print(f\"Fine-tuned model location: {s3_model_data_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788fa859",
   "metadata": {},
   "source": [
    "#### Training strategy: `Spectrum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb093974",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_estimator = PyTorch(\n",
    "    image_uri=pytorch_image_uri,\n",
    "    entry_point=\"sm_accelerate_train.sh\", # Adapted bash script to train using accelerate on SageMaker - Multi-GPU\n",
    "    source_dir=\"sagemaker_code\",\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"{job_name}-pytorch\",\n",
    "    role=role,\n",
    "    volume_size=300,\n",
    "    py_version=\"py312\",\n",
    "    keep_alive_period_in_seconds=3600,\n",
    "    environment=training_env,\n",
    "    sagemaker_session=sess,\n",
    "    hyperparameters={\n",
    "        \"config\": \"hf_recipes/meta-llama/Llama-4-Maverick-17B-128E-Instruct--vanilla-spectrum.yaml\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# fit or train\n",
    "pytorch_estimator.fit(\n",
    "    {\"training\": uploaded_s3_uri}, \n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ac8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_data_uri = pytorch_estimator.model_data\n",
    "print(f\"Fine-tuned model location: {s3_model_data_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfd7bac",
   "metadata": {},
   "source": [
    "#### Training strategy: `Full-Finetuning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8182557",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_estimator = PyTorch(\n",
    "    image_uri=pytorch_image_uri,\n",
    "    entry_point=\"sm_accelerate_train.sh\", # Adapted bash script to train using accelerate on SageMaker - Multi-GPU\n",
    "    source_dir=\"sagemaker_code\",\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"{job_name}-pytorch\",\n",
    "    role=role,\n",
    "    volume_size=300,\n",
    "    py_version=\"py312\",\n",
    "    keep_alive_period_in_seconds=3600,\n",
    "    environment=training_env,\n",
    "    sagemaker_session=sess,\n",
    "     hyperparameters={\n",
    "        \"config\": \"hf_recipes/meta-llama/Llama-4-Maverick-17B-128E-Instruct--vanilla-full.yaml\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# fit or train\n",
    "pytorch_estimator.fit(\n",
    "    {\"training\": uploaded_s3_uri}, \n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6511d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_data_uri = pytorch_estimator.model_data\n",
    "print(f\"Fine-tuned model location: {s3_model_data_uri}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
