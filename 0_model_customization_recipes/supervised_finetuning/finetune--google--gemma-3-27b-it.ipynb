{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60b91a9-7d9c-4c45-be0f-5a224581f644",
   "metadata": {},
   "source": [
    "# ðŸš€ Customize and Deploy `google/gemma-3-27b-it` on Amazon SageMaker AI\n",
    "---\n",
    "In this notebook, we explore **Gemma-3-27B-IT**, Google's latest and most advanced instruction-tuned model in the Gemma family. You'll learn how to fine-tune this powerful model, evaluate its exceptional capabilities, and deploy it using SageMaker for production workloads.\n",
    "\n",
    "**What is Gemma-3-27B-IT?**\n",
    "\n",
    "Google's **Gemma-3-27B-IT** represents the pinnacle of the Gemma model series, featuring 27 billion parameters and advanced instruction-tuning. Built on cutting-edge research from the Gemini team, this model delivers state-of-the-art performance across reasoning, coding, mathematics, and complex instruction-following tasks.  \n",
    "ðŸ”— Model card: [google/gemma-3-27b-it on Hugging Face](https://huggingface.co/google/gemma-3-27b-it)\n",
    "\n",
    "---\n",
    "\n",
    "**Key Specifications**\n",
    "\n",
    "| Feature | Details |\n",
    "|---|---|\n",
    "| **Parameters** | ~27 billion |\n",
    "| **Architecture** | Advanced Transformer with optimized attention and MLP layers |\n",
    "| **Context Length** | Extended context window for complex reasoning |\n",
    "| **Training Data** | High-quality curated datasets with advanced filtering |\n",
    "| **Modalities** | Text-in / Text-out |\n",
    "| **License** | Gemma Terms of Use |\n",
    "| **Instruction Tuning** | Advanced RLHF and supervised fine-tuning |\n",
    "\n",
    "---\n",
    "\n",
    "**Benchmarks & Behavior**\n",
    "\n",
    "- Gemma-3-27B-IT achieves **exceptional performance** on reasoning and instruction-following benchmarks.  \n",
    "- Outstanding **mathematical reasoning** and competitive programming capabilities.  \n",
    "- Advanced **code generation and debugging** across multiple programming languages.  \n",
    "- Excellent **multilingual capabilities** with strong performance across languages.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce40054-610a-4acc-a546-943893f293c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq \"datasets==4.3.0\" \\\n",
    "    \"sagemaker==2.253.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b791e72e-82b5-4f8e-a7fe-700e8afdeba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c673a9-5b9f-47e0-92cf-bb97486b3e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "\n",
    "sess = sagemaker.Session(boto3.Session(region_name=region))\n",
    "\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9786901-b012-41ff-a98a-b16e5f60ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fa2e0a-1ada-485a-9de6-2e14e8e60841",
   "metadata": {},
   "source": [
    "## Data Preparation for Supervised Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc6f8c4-a511-40aa-b0ee-d99e1c980aa6",
   "metadata": {},
   "source": [
    "### [Finance-Instruct-500k](https://huggingface.co/datasets/Josephgflowers/Finance-Instruct-500k)\n",
    "\n",
    "**Finance-Instruct-500k** is a large-scale dataset with about **518,000 entries** focused on the financial domain. It spans topics such as investments, banking, markets, accounting, and corporate finance, offering a wide variety of instructionâ€“response examples.\n",
    "\n",
    "**Data Format & Structure**:\n",
    "- Distributed in **JSON** format, with simple conversion to Parquet.  \n",
    "- Contains a single `train` split with ~518k records.  \n",
    "- Each record includes:  \n",
    "  - `system` â€“ context or metadata for the task  \n",
    "  - `user` â€“ the financial prompt or query  \n",
    "  - `assistant` â€“ the corresponding response  \n",
    "\n",
    "**License**: Released under the **Apache-2.0** license.  \n",
    "\n",
    "**Applications**:\n",
    "\n",
    "The dataset can support finance-focused tasks such as:  \n",
    "- Financial question answering  \n",
    "- Market and investment analysis  \n",
    "- Topic and sentiment classification  \n",
    "- Financial entity extraction and document understanding  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ef2937-5c2b-45ba-9a0b-0a938f7d9fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10456175-6baf-499a-b69a-61e58493775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_parent_path = os.path.join(os.getcwd(), \"tmp_cache_local_dataset\")\n",
    "os.makedirs(dataset_parent_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe1f465-666b-4c40-ad8e-3a4d24c9542f",
   "metadata": {},
   "source": [
    "**Preparing Your Dataset in `messages` format**\n",
    "\n",
    "This section walks you through creating a conversation-style datasetâ€”the required `messages` formatâ€”for directly training LLMs using SageMaker AI.\n",
    "\n",
    "**What Is the `messages` Format?**\n",
    "\n",
    "The `messages` format structures instances as chat-like exchanges, wrapping each conversation turn into a role-labeled JSON array. Itâ€™s widely used by frameworks like TRL.\n",
    "\n",
    "Example entry:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "    { \"role\": \"user\", \"content\": \"How do I bake sourdough?\" },\n",
    "    { \"role\": \"assistant\", \"content\": \"First, you need to create a starter by...\" }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b262cec-f6b9-4a46-9bfa-846b20a48d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Josephgflowers/Finance-Instruct-500k\"\n",
    "dataset = load_dataset(dataset_name, split=\"train[:1000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7f8f4-2a2a-4b1e-97ca-a2dc7f1335fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pp(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124421b1-aaaf-4956-bb84-8a0b25129f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total number of fine-tunable samples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59939524-6ace-4cb7-bce7-8ce776999301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_messages(row):\n",
    "    system_content = \"You are a financial reasoning assistant. Read the userâ€™s query, restate the key data, and solve step by step. Show calculations clearly, explain any rounding or adjustments, and present the final answer in a concise and professional manner.\"\n",
    "    user_content = row[\"user\"]\n",
    "    assistant_content = row[\"assistant\"]\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            { \"role\": \"system\", \"content\": system_content},\n",
    "            { \"role\": \"user\", \"content\": user_content },\n",
    "            { \"role\": \"assistant\", \"content\": assistant_content }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    \n",
    "dataset = dataset.map(convert_to_messages, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9939087e-513e-42f4-be92-5442c0d7a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filename = os.path.join(dataset_parent_path, f\"{dataset_name.replace('/', '--').replace('.', '-')}.jsonl\")\n",
    "dataset.to_json(dataset_filename, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78878e28-b0f5-43da-84d8-a6ec8a66d64d",
   "metadata": {},
   "source": [
    "#### Upload file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ede02-07c9-4078-a036-3712f12f41d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c0dbf2-8506-4366-8e89-4765cb6fdb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s3_uri = f\"s3://{sess.default_bucket()}/dataset\"\n",
    "\n",
    "uploaded_s3_uri = S3Uploader.upload(\n",
    "    local_path=dataset_filename,\n",
    "    desired_s3_uri=data_s3_uri\n",
    ")\n",
    "print(f\"Uploaded {dataset_filename} to > {uploaded_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821174fe-3d04-4cdc-b5c2-ff1127df2812",
   "metadata": {},
   "source": [
    "## Fine-Tune LLMs using SageMaker AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae98f744-b524-4696-8e27-08069d5799fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.modules.configs import (\n",
    "    CheckpointConfig,\n",
    "    Compute,\n",
    "    OutputDataConfig,\n",
    "    SourceCode,\n",
    "    StoppingCondition,\n",
    ")\n",
    "from sagemaker.modules.configs import InputData\n",
    "from sagemaker.modules.train import ModelTrainer\n",
    "from getpass import getpass\n",
    "import yaml\n",
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41c2e87-be05-4086-bb22-ce874b9c9c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"google/gemma-3-27b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37246cb4-8f16-45cb-95bb-9149ebaf2526",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e541e02e-164d-49c5-aed0-bd912eb2b0eb",
   "metadata": {},
   "source": [
    "### Training using `PyTorch` Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe79dd34-4699-484d-8d04-c22e32702a17",
   "metadata": {},
   "source": [
    "**Training Using `PyTorch` Estimator**\n",
    "Leverages the official PyTorch SageMaker container to run a custom training script using the Accelerate and DeepSpeed libraries. This option is ideal for users who want full control over the training pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2042c9a-2c46-4168-8acf-de39ab26e76c",
   "metadata": {},
   "source": [
    "---\n",
    "**Observability**: SageMaker AI has [SageMaker MLflow](https://docs.aws.amazon.com/sagemaker/latest/dg/mlflow.html) which enables you to accelerate generative AI by making it easier to track experiments and monitor performance of models and AI applications using a single tool.\n",
    "\n",
    "You can choose to include MLflow as a part of your training workflow to track your model fine-tuning metrics in realtime by simply specifying a **mlflow** tracking arn.\n",
    "\n",
    "Optionally you can also report to : **tensorboard**, **wandb**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad782151-d8c2-418d-9e09-c0e4347b5d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_TRACKING_SERVER_ARN = \"arn:aws:sagemaker:us-east-1:811828458885:mlflow-tracking-server/mlflow-demos\"\n",
    "\n",
    "if MLFLOW_TRACKING_SERVER_ARN:\n",
    "    reports_to = \"mlflow\"\n",
    "else:\n",
    "    reports_to = \"tensorboard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ee2532-5c9a-4b5b-957c-3b34ac1bfc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = MODEL_ID.replace('/', '--').replace('.', '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988671ce-b7e3-4d39-b7d0-d4575aa13282",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MLFLOW_TRACKING_SERVER_ARN:\n",
    "    training_env = {\n",
    "        # mlflow tracking metrics\n",
    "        \"MLFLOW_EXPERIMENT_NAME\": f\"{job_name}-exp\",\n",
    "        \"MLFLOW_TAGS\": json.dumps(\n",
    "            {\n",
    "                \"source.job\": \"sm-training-jobs\", \n",
    "                \"source.type\": \"sft\", \n",
    "                \"source.framework\": \"pytorch\"\n",
    "            }\n",
    "        ),\n",
    "        \"MLFLOW_TRACKING_URI\": MLFLOW_TRACKING_SERVER_ARN,\n",
    "        \"MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING\": \"true\",\n",
    "        # non tracking metrics - enabled\n",
    "        \"HF_TOKEN\": hf_token,\n",
    "        \"FI_EFA_USE_DEVICE_RDMA\": \"1\",\n",
    "        \"NCCL_DEBUG\": \"INFO\",\n",
    "        \"NCCL_SOCKET_IFNAME\": \"eth0\",\n",
    "        \"FI_PROVIDER\": \"efa\",\n",
    "        \"NCCL_PROTO\": \"simple\",\n",
    "        \"NCCL_NET_GDR_LEVEL\": \"5\"\n",
    "    }\n",
    "else:\n",
    "    training_env = {\n",
    "        # non tracking metrics\n",
    "        \"HF_TOKEN\": hf_token,\n",
    "        \"FI_EFA_USE_DEVICE_RDMA\": \"1\",\n",
    "        \"NCCL_DEBUG\": \"INFO\",\n",
    "        \"NCCL_SOCKET_IFNAME\": \"eth0\",\n",
    "        \"FI_PROVIDER\": \"efa\",\n",
    "        \"NCCL_PROTO\": \"simple\",\n",
    "        \"NCCL_NET_GDR_LEVEL\": \"5\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc11bb2-e93f-4b40-8197-1f00a83dee8c",
   "metadata": {},
   "source": [
    "#### Training strategy - Choose between: `PeFT`/`Spectrum`/`Full-Finetuning`\n",
    "\n",
    "Here we create a measured mapping of strategy to instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865011f1-4583-4bd7-9013-70d0c534f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sagemaker_code/requirements.txt\n",
    "transformers==4.55.0\n",
    "peft==0.17.0\n",
    "accelerate==1.10.0\n",
    "bitsandbytes==0.46.1\n",
    "datasets==4.0.0\n",
    "deepspeed==0.16.4\n",
    "evaluate==0.4.5\n",
    "hf-transfer==0.1.8\n",
    "hf_xet\n",
    "liger-kernel==0.6.1\n",
    "lm-eval[api]==0.4.9\n",
    "kernels>=0.9.0\n",
    "mlflow\n",
    "safetensors>=0.6.2\n",
    "sagemaker==2.251.1\n",
    "sagemaker-mlflow==0.1.0\n",
    "sentencepiece==0.2.0\n",
    "scikit-learn==1.7.1\n",
    "tokenizers>=0.21.4\n",
    "triton\n",
    "trl==0.21.0\n",
    "py7zr\n",
    "nvidia-ml-py\n",
    "wandb\n",
    "git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels\n",
    "vllm==0.10.1\n",
    "poetry\n",
    "yq\n",
    "psutil\n",
    "nvidia-ml-py\n",
    "pyrsmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7adb395-a75d-465e-8263-074a23fe61f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PeFT\n",
    "args = [\n",
    "    \"--config\",\n",
    "    \"hf_recipes/google/gemma-3-27b-it--vanilla-peft-qlora.yaml\",\n",
    "    # \"--run-eval\" # enable this for small models to run eval + tune\n",
    "]\n",
    "training_instance_type = \"ml.g6e.2xlarge\"\n",
    "training_instance_count = 1\n",
    "\n",
    "## For Full-Finetuning\n",
    "# args = [\n",
    "#     \"--config\",\n",
    "#     \"hf_recipes/google/gemma-3-26b-it--vanilla-full.yaml\",\n",
    "#     # \"--run-eval\" # enable this for small models if you're looking to bundle eval with fine-tuning\n",
    "# ]\n",
    "# training_instance_type = \"ml.g6e.12xlarge\"\n",
    "# training_instance_count = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164d84b1-def9-4dd8-9d8a-c5d947fbd3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=sess.boto_session.region_name,\n",
    "    version=\"2.7.1\",\n",
    "    instance_type=training_instance_type,\n",
    "    image_scope=\"training\",\n",
    ")\n",
    "print(f\"Using image: {pytorch_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1204e06-d551-44bc-9f60-52369790a6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_code = SourceCode(\n",
    "    source_dir=\"./sagemaker_code\",\n",
    "    command=f\"bash sm_accelerate_train.sh {' '.join(args)}\",\n",
    ")\n",
    "\n",
    "compute_configs = Compute(\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=training_instance_count,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    volume_size_in_gb=300\n",
    ")\n",
    "\n",
    "base_job_name = f\"{job_name}-finetune\"\n",
    "output_path = f\"s3://{sess.default_bucket()}/{base_job_name}\"\n",
    "\n",
    "model_trainer = ModelTrainer(\n",
    "    training_image=pytorch_image_uri,\n",
    "    source_code=source_code,\n",
    "    base_job_name=base_job_name,\n",
    "    compute=compute_configs,\n",
    "    stopping_condition=StoppingCondition(max_runtime_in_seconds=18000),\n",
    "    output_data_config=OutputDataConfig(\n",
    "        s3_output_path=output_path,\n",
    "    ),\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        s3_uri=os.path.join(\n",
    "            output_path,\n",
    "            dataset_name.replace('/', '--').replace('.', '-'), \n",
    "            job_name,\n",
    "            \"checkpoints\"\n",
    "        ), \n",
    "        local_path=\"/opt/ml/checkpoints\"\n",
    "    ),\n",
    "    role=role,\n",
    "    environment=training_env\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbbdd30-fe5f-46c6-9b56-248d5ca33ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.train(\n",
    "    input_data_config=[\n",
    "        InputData(\n",
    "            channel_name=\"training\",\n",
    "            data_source=uploaded_s3_uri,  \n",
    "        )\n",
    "    ], \n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8adc60-ed34-4a2e-90f3-46ad2d4ad426",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
